{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8070de8d",
   "metadata": {},
   "source": [
    "# *prepDyn*: Preprocessing of missing data for dynamic homology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc4d943",
   "metadata": {},
   "source": [
    "In dynamic homology, data are typically preprocessed to distinguish differences in sequence length resulting from missing data or insertion-deletion events. However, previous empirical studies using POY/PhyG manually preprocessed missing data with varying approaches. Here we demonstrate that coding missing data with dashes (â€“) or IUPAC Ns increase tree costs and are biologically inappropriate. Although inserting pound signs (#) around blocks of missing data has been a common solution, it reduces the severity of homology tests and precludes the discovery of the optimal tree. Therefore, missing data should be coded with question marks (?) to minimize tree costs, whereas pound signs should be inserted only on highly conserved regions. To balance time complexity and severity of homology test, we propose a heuristic to successively partition data. All procedures are implemented in a collection of Python scripts to facilitate the preprocessing of input sequences to PhyG. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4b3d0a",
   "metadata": {},
   "source": [
    "## Pre-amble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09021b2e",
   "metadata": {},
   "source": [
    "Mafft should be installed locally in the system PATH (e.g. in Ubuntu, `$ sudo apt install mafft`). In addition, the following Python modules should be installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8cc9efc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "from Bio import AlignIO, Entrez, SeqIO\n",
    "from Bio.Align import MultipleSeqAlignment\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import csv\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "#import pytrimal\n",
    "import re\n",
    "#import rich.console\n",
    "#import rich.panel\n",
    "#from rich_msa import RichAlignment\n",
    "import subprocess\n",
    "import tempfile\n",
    "from termcolor import colored\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1b3422",
   "metadata": {},
   "source": [
    "Additional custom functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e660b3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION 1: Define functions to visualize nucleotides in different colors\n",
    "# Define colors for each nucleotide (case insensitive)\n",
    "def color_nucleotide(nucleotide):\n",
    "    color_map = {\n",
    "        \"A\": \"red\",\n",
    "        \"T\": \"blue\",\n",
    "        \"G\": \"green\",\n",
    "        \"C\": \"yellow\",\n",
    "        \"-\": \"white\",\n",
    "        \"#\": \"black\",\n",
    "        \"?\": \"magenta\"\n",
    "    }\n",
    "    return colored(nucleotide, color_map.get(nucleotide.upper(), \"white\"))\n",
    "# Function to print the alignment\n",
    "def print_colored_alignment(alignment):\n",
    "    \"\"\"\n",
    "    Prints a DNA alignment with each nucleotide in a different color.\n",
    "\n",
    "    Parameters:\n",
    "        alignment (dict): A dictionary with sequence names as keys and DNA sequences as values.\n",
    "    \"\"\"\n",
    "    for name, sequence in alignment.items():\n",
    "        colored_seq = ''.join(color_nucleotide(n) for n in sequence)\n",
    "        print(f\"{name}: {colored_seq}\")\n",
    "\n",
    "\n",
    "# FUNCTION 2: Print alignments using trimAl\n",
    "def show_alignment(alignment):\n",
    "    console = rich.console.Console(width=len(alignment.sequences[0])+40)\n",
    "    widget = RichAlignment(names=[n.decode() for n in alignment.names], sequences=alignment.sequences, max_name_width=30)\n",
    "    panel = rich.panel.Panel(widget, title_align=\"left\", title=\"({} residues, {} sequences)\".format(len(alignment.sequences[0]), len(alignment.sequences)))\n",
    "    console.print(panel)\n",
    "\n",
    "# FUNCTION 3: Convert dictionaries to MultipleSeqAlignment\n",
    "def dict_to_multiple_seq_alignment(seq_dict):\n",
    "    \"\"\"\n",
    "    Converts a dictionary of sequences into a MultipleSeqAlignment object.\n",
    "    \n",
    "    Args:\n",
    "        seq_dict (dict): A dictionary where keys are sequence identifiers and values are sequences (strings).\n",
    "        \n",
    "    Returns:\n",
    "        MultipleSeqAlignment: A Biopython MultipleSeqAlignment object.\n",
    "    \"\"\"\n",
    "    # Create a list of SeqRecord objects from the dictionary\n",
    "    seq_records = []\n",
    "    \n",
    "    for seq_id, seq_str in seq_dict.items():\n",
    "        # Create a SeqRecord for each sequence\n",
    "        seq = Seq(seq_str)\n",
    "        seq_record = SeqRecord(seq, id=seq_id, description=\"\")\n",
    "        seq_records.append(seq_record)\n",
    "    \n",
    "    # Create and return the MultipleSeqAlignment object\n",
    "    alignment = MultipleSeqAlignment(seq_records)\n",
    "    return alignment\n",
    "\n",
    "# FUNCTION 4: List lengths of blocks of contiguous gaps in internal and terminal positions\n",
    "def list_gap_blocks_by_type(alignment, plot_distribution=False):\n",
    "    \"\"\"\n",
    "    Identify all blocks of contiguous gaps in the DNA alignment.\n",
    "    Classify gap blocks into terminal and internal blocks.\n",
    "    Optionally, plot the distributions of gap block lengths for terminal and internal blocks.\n",
    "    \n",
    "    Args:\n",
    "        alignment (dict): A dictionary with sequence IDs as keys and sequences as values.\n",
    "        plot_distribution (bool): If True, plot the distributions of terminal and internal gap block lengths.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Two lists:\n",
    "            - terminal_blocks: A list of gap block lengths at the start or end of sequences.\n",
    "            - internal_blocks: A list of gap block lengths in the middle of sequences.\n",
    "    \"\"\"\n",
    "    terminal_blocks = []\n",
    "    internal_blocks = []\n",
    "    \n",
    "    # Iterate over each sequence in the alignment\n",
    "    for seq_id, sequence in alignment.items():\n",
    "        sequence_length = len(sequence)\n",
    "        gap_count = 0\n",
    "        in_gap = False\n",
    "\n",
    "        # Identify gap blocks and separate terminal vs internal\n",
    "        for i, nucleotide in enumerate(sequence):\n",
    "            if nucleotide == '-':  # We are in a gap\n",
    "                if not in_gap:\n",
    "                    in_gap = True\n",
    "                    gap_count = 1  # Start a new gap block\n",
    "                else:\n",
    "                    gap_count += 1  # Continue counting the current gap block\n",
    "            else:  # We encountered a non-gap nucleotide\n",
    "                if in_gap:\n",
    "                    # End of a gap block\n",
    "                    # Check if this gap block is terminal (at start or end of the sequence)\n",
    "                    if i == sequence_length or (i - gap_count == 0):\n",
    "                        terminal_blocks.append(gap_count)\n",
    "                    else:\n",
    "                        internal_blocks.append(gap_count)\n",
    "                    in_gap = False\n",
    "                    gap_count = 0  # Reset gap count for the next block\n",
    "\n",
    "        # If the sequence ends with a gap block, add it to the appropriate list\n",
    "        if in_gap:\n",
    "            if sequence[-1] == '-':  # Last character is a gap\n",
    "                terminal_blocks.append(gap_count)\n",
    "            else:\n",
    "                internal_blocks.append(gap_count)\n",
    "\n",
    "    # Optionally plot the distributions of terminal and internal blocks\n",
    "    if plot_distribution:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        # Plotting terminal blocks\n",
    "        plt.hist(terminal_blocks, bins=20, alpha=0.5, label='Terminal Blocks', color='blue')\n",
    "        # Plotting internal blocks\n",
    "        plt.hist(internal_blocks, bins=20, alpha=0.5, label='Internal Blocks', color='red')\n",
    "        \n",
    "        plt.xlabel('Gap Block Length')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Distribution of Gap Block Lengths')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()\n",
    "\n",
    "    return terminal_blocks, internal_blocks\n",
    "\n",
    "# FUNCTION 5: Remove underscores in the beggining of file names\n",
    "def remove_leading_underscores(file_path):\n",
    "    \"\"\"\n",
    "    Remove leading contiguous underscores from the beginning of the file name.\n",
    "    If a directory is provided, it renames all files in that directory.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The full path of the file or directory.\n",
    "    \n",
    "    Returns:\n",
    "        str or None: The new file path with leading underscores removed if a file, \n",
    "                     or None if a directory (modifies in place).\n",
    "    \"\"\"\n",
    "    if os.path.isdir(file_path):\n",
    "        # If it's a directory, rename all files inside it\n",
    "        for root, dirs, files in os.walk(file_path):\n",
    "            for file in files:\n",
    "                old_file_path = os.path.join(root, file)\n",
    "                new_file_name = file.lstrip('_')\n",
    "                new_file_path = os.path.join(root, new_file_name)\n",
    "                \n",
    "                if old_file_path != new_file_path:\n",
    "                    os.rename(old_file_path, new_file_path)\n",
    "        return None  # No return for directories, as the renaming is in place\n",
    "    else:\n",
    "        # If it's a single file, rename it\n",
    "        dir_name, file_name = os.path.split(file_path)\n",
    "        new_file_name = file_name.lstrip('_')\n",
    "        new_file_path = os.path.join(dir_name, new_file_name)\n",
    "        if file_path != new_file_path:\n",
    "            os.rename(file_path, new_file_path)\n",
    "        return new_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9aedff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main functions\n",
    "\n",
    "We define below the main functions of *prepDyn*. To test each of the functions individually, we created a synthetic example. In such example, the step of data collection from GenBank was skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c7796cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "sequences = {\n",
    "    'sp1': \"TGCACCGTCGCCAACAGTAGTCCTCCACCGTCGCCACCGTCGCCAACAGTA\",\n",
    "    'sp2': \"TCCACCGTCGN?????GTAGTCTCCACCGTCGC???????GCCAACAGTA\",\n",
    "    'sp3': \"TCCACCGTCGCCAACAGTAGTCCC\",\n",
    "    'sp4': \"GTCCTCCACCGTCGCCACCTCGCCAAAGTA\",\n",
    "    'sp5': \"TCCACCGTCGCCAACAGTAGTCCTCCACCGTCGCCACCGTCGCCACAGA\",\n",
    "    'sp6': \"CACCGTCCCAACAGTAGTCCTCCACCGTCGCACCGTCGCAACAGTA\",\n",
    "} \n",
    "\n",
    "# Create a temporary file to save the sequences in FASTA format\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix='.fasta', mode='w') as fasta_file:\n",
    "    # Write the sequences to the file\n",
    "    for name, seq in sequences.items():\n",
    "        fasta_file.write(f\">{name}\\n{seq}\\n\")\n",
    "    fasta_file_path = fasta_file.name\n",
    "\n",
    "# Define the output file for the alignment\n",
    "aligned_fasta_file_path = fasta_file_path.replace('.fasta', '_aligned.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cd9f6133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "outputhat23=16\n",
      "treein = 0\n",
      "compacttree = 0\n",
      "stacksize: 8192 kb\n",
      "generating a scoring matrix for nucleotide (dist=200) ... done\n",
      "All-to-all alignment.\n",
      "    0 / 6\r",
      "    1 / 6\r",
      "    2 / 6\r",
      "    3 / 6\r",
      "    4 / 6\r",
      "tbfast-pair (nuc) Version 7.520\n",
      "alg=L, model=DNA200 (2), 2.00 (6.00), -0.10 (-0.30), noshift, amax=0.0\n",
      "0 thread(s)\n",
      "\n",
      "outputhat23=16\n",
      "Loading 'hat3.seed' ... \n",
      "done.\n",
      "Writing hat3 for iterative refinement\n",
      "generating a scoring matrix for nucleotide (dist=200) ... done\n",
      "Gap Penalty = -1.53, +0.00, +0.00\n",
      "tbutree = 1, compacttree = 0\n",
      "Constructing a UPGMA tree ... \n",
      "\r",
      "    0 / 6\n",
      "done.\n",
      "\n",
      "Progressive alignment ... \n",
      "\r",
      "STEP     1 /5 \r",
      "STEP     2 /5 \r",
      "STEP     3 /5 \r",
      "STEP     4 /5 \r",
      "STEP     5 /5 \n",
      "done.\n",
      "tbfast (nuc) Version 7.520\n",
      "alg=A, model=DNA200 (2), 1.53 (4.59), -0.00 (-0.00), noshift, amax=0.0\n",
      "1 thread(s)\n",
      "\n",
      "minimumweight = 0.000010\n",
      "autosubalignment = 0.000000\n",
      "nthread = 0\n",
      "randomseed = 0\n",
      "blosum 62 / kimura 200\n",
      "poffset = 0\n",
      "niter = 16\n",
      "sueff_global = 0.100000\n",
      "nadd = 16\n",
      "Loading 'hat3' ... done.\n",
      "generating a scoring matrix for nucleotide (dist=200) ... done\n",
      "\n",
      "\r",
      "    0 / 6\n",
      "Segment   1/  1    1-  52\n",
      "STEP 001-001-0  identical.   \r",
      "STEP 001-001-1  identical.   \r",
      "STEP 001-002-0  identical.   \r",
      "STEP 001-002-1  identical.   \r",
      "STEP 001-003-0  identical.   \r",
      "STEP 001-003-1  identical.   \r",
      "STEP 001-004-0  identical.   \r",
      "STEP 001-004-1  accepted.\r",
      "STEP 001-005-1  accepted.\r",
      "STEP 002-005-1  identical.   \r",
      "STEP 002-004-0  rejected.\r",
      "STEP 002-004-1  identical.   \r",
      "STEP 002-003-0  identical.   \r",
      "STEP 002-003-1  identical.   \r",
      "STEP 002-002-0  accepted.\r",
      "STEP 002-002-1  identical.   \r",
      "STEP 002-001-0  identical.   \r",
      "STEP 002-001-1  accepted.\r",
      "STEP 003-001-0  identical.   \r",
      "STEP 003-001-1  identical.   \r",
      "STEP 003-002-0  identical.   \r",
      "STEP 003-002-1  identical.   \r",
      "STEP 003-003-0  identical.   \r",
      "STEP 003-003-1  identical.   \r",
      "STEP 003-004-0  rejected.\r",
      "STEP 003-004-1  identical.   \r",
      "STEP 003-005-1  accepted.\r",
      "STEP 004-005-1  identical.   \r",
      "STEP 004-004-0  identical.   \r",
      "STEP 004-004-1  rejected.\r",
      "STEP 004-003-0  identical.   \r",
      "STEP 004-003-1  rejected.\r",
      "STEP 004-002-0  identical.   \r",
      "STEP 004-002-1  identical.   \r",
      "STEP 004-001-0  identical.   \r\n",
      "Oscillating.\n",
      "\n",
      "done\n",
      "dvtditr (nuc) Version 7.520\n",
      "alg=A, model=DNA200 (2), 1.53 (4.59), -0.00 (-0.00), noshift, amax=0.0\n",
      "0 thread(s)\n",
      "\n",
      "\n",
      "Strategy:\n",
      " L-INS-i (Probably most accurate, very slow)\n",
      " Iterative refinement method (<16) with LOCAL pairwise alignment information\n",
      "\n",
      "If unsure which option to use, try 'mafft --auto input > output'.\n",
      "For more information, see 'mafft --help', 'mafft --man' and the mafft page.\n",
      "\n",
      "The default gap scoring scheme has been changed in version 7.110 (2013 Oct).\n",
      "It tends to insert more gaps into gap-rich regions than previous versions.\n",
      "To disable this change, add the --leavegappyregion option.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run MAFFT\n",
    "mafft_command = ['mafft', '--auto', fasta_file_path]\n",
    "with open(aligned_fasta_file_path, 'w') as aligned_file:\n",
    "    subprocess.run(mafft_command, stdout=aligned_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "98b172c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0. Align sequences statically\n",
      "sp1: \u001b[34mt\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp2: \u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[32mg\u001b[0m\u001b[97mn\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp3: \u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp4: \u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp5: \u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp6: \u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\n",
      "\n",
      "\n",
      "Step 1.1 Delete columns presenting gaps in all taxa (artifacts from MAFFT)\n",
      "sp1: \u001b[34mt\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp2: \u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[32mg\u001b[0m\u001b[97mn\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp3: \u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp4: \u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp5: \u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp6: \u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\n",
      "\n",
      "\n",
      "Step 1.2. Delete non-terminal orphan nucleotides (given a threshold)\n",
      "sp1: \u001b[34mt\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp2: \u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[32mg\u001b[0m\u001b[97mn\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp3: \u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp4: \u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp5: \u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp6: \u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\n",
      "\n",
      "\n",
      "Step 2.1 Classify terminal (?) and internal gaps (-)\n",
      "sp1: \u001b[34mt\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp2: \u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[32mg\u001b[0m\u001b[97mn\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp3: \u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp4: \u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp5: \u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp6: \u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\n",
      "\n",
      "\n",
      "Step 1.3. Trim parsimony non-informative characters in terminal position\n",
      "sp1: \u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\n",
      "sp2: \u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[32mg\u001b[0m\u001b[97mn\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\n",
      "sp3: \u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp4: \u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\n",
      "sp5: \u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp6: \u001b[35m?\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\n",
      "\n",
      "\n",
      "Step 1.3. Treat internal missing data\n",
      "sp1: \u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\n",
      "sp2: \u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[32mg\u001b[0m\u001b[97mn\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\n",
      "sp3: \u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp4: \u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\n",
      "sp5: \u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp6: \u001b[35m?\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\n",
      "\n",
      "\n",
      "Step 3.1. Insert pound sign in the n-largest block of contiguous invariants\n",
      "sp1: \u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\n",
      "sp2: \u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[32mg\u001b[0m\u001b[97mn\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\n",
      "sp3: \u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[30m#\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp4: \u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\n",
      "sp5: \u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp6: \u001b[35m?\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\n",
      "\n",
      "\n",
      "Step 3.2 Replace ? flanked by # with -\n",
      "sp1: \u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\n",
      "sp2: \u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[32mg\u001b[0m\u001b[97mn\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\n",
      "sp3: \u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[30m#\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp4: \u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\n",
      "sp5: \u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp6: \u001b[35m?\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "alignment = AlignIO.read(aligned_fasta_file_path, \"fasta\")\n",
    "type(alignment) # Check the class of alignment (MultipleSeqAlignment\n",
    "# Convert MultipleSeqAlignment to dictionary\n",
    "alignment = {record.id: str(record.seq) for record in alignment}\n",
    "type(alignment)\n",
    "# Print MAFFT alignment\n",
    "print(\"Step 0. Align sequences statically\")\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "###########################\n",
    "# Step 1: Data collection #\n",
    "###########################\n",
    "\n",
    "def GB2MSA_1(input_file, output_prefix, delimiter=',', write_names=True):\n",
    "    \"\"\"\n",
    "    Downloads GenBank sequences based on accession numbers in a CSV/TSV file and aligns them by gene using MAFFT.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_file : str\n",
    "        Path to the CSV or TSV input file. The first column should contain sequence names (sample identifiers).\n",
    "        The first row should contain gene names starting from the second column.\n",
    "        Cells contain GenBank accession numbers (one or more separated by slashes). \"NA\", empty cells, or dashes are ignored.\n",
    "\n",
    "    output_prefix : str\n",
    "        Prefix used for naming intermediate FASTA files and final aligned output files.\n",
    "\n",
    "    delimiter : str, optional (default=',')\n",
    "        Delimiter used in the input file (e.g., ',' for CSV or '\\t' for TSV).\n",
    "\n",
    "    write_names : bool, optional (default=True)\n",
    "        If True, writes a TXT file listing all sequence names (from the first column).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    aligned_files : list of str\n",
    "        List of file paths to the MAFFT-aligned FASTA files for each gene.\n",
    "    \"\"\"\n",
    "    # Open the input CSV/TSV file\n",
    "    with open(input_file, newline='') as file:\n",
    "        reader = csv.reader(file, delimiter=delimiter)\n",
    "        rows = list(reader)\n",
    "\n",
    "    # Replace spaces in sequence names with underscores\n",
    "    sequence_names = [row[0].replace(\" \", \"_\") for row in rows[1:]]\n",
    "    \n",
    "    # Extract gene names from the header row (excluding first column)\n",
    "    gene_names = rows[0][1:]\n",
    "    \n",
    "    # Extract gene accession data for each sequence (excluding first column)\n",
    "    gene_columns = [row[1:] for row in rows[1:]]\n",
    "\n",
    "    # If requested, write the sequence names to a text file\n",
    "    if write_names:\n",
    "        names_file = f\"{output_prefix}_sequence_names.txt\"\n",
    "        with open(names_file, 'w') as nf:\n",
    "            for name in sequence_names:\n",
    "                nf.write(f\"{name}\\n\")\n",
    "\n",
    "    aligned_files = []  # List to store paths of aligned output files\n",
    "\n",
    "    # Iterate over each gene (i.e., each column after the first)\n",
    "    for gene_idx, gene_name in enumerate(gene_names):\n",
    "        fasta_file = f\"{output_prefix}_{gene_name}.fasta\"  # Name of temporary FASTA file\n",
    "        \n",
    "        with open(fasta_file, 'w') as fasta_out:\n",
    "            # Iterate through each row (sample/sequence)\n",
    "            for i, seq_name in enumerate(sequence_names):\n",
    "                cell = gene_columns[i][gene_idx].strip()\n",
    "                \n",
    "                # Skip cells with missing data (\"NA\", empty, or dash)\n",
    "                if cell.upper() == \"NA\" or not cell or cell == \"-\":\n",
    "                    continue\n",
    "\n",
    "                # Split accession numbers by '/' and filter out invalid entries\n",
    "                accessions = [acc for acc in cell.split('/') if acc.upper() != \"NA\" and acc != \"\" and acc != \"-\"]\n",
    "                sequences = []  # To hold the sequences retrieved from GenBank\n",
    "\n",
    "                # Fetch each sequence from GenBank\n",
    "                for acc in accessions:\n",
    "                    try:\n",
    "                        handle = Entrez.efetch(db=\"nucleotide\", id=acc, rettype=\"fasta\", retmode=\"text\")\n",
    "                        seq_record = SeqIO.read(handle, \"fasta\")\n",
    "                        handle.close()\n",
    "                        sequences.append(str(seq_record.seq))  # Store the sequence string\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error fetching {acc}: {e}\")\n",
    "\n",
    "                # Combine multiple sequences with 'W' delimiters to mark junctions (to be handled later)\n",
    "                combined_seq = \"WWWWWWWWWWWWWWW\".join(sequences)\n",
    "                \n",
    "                # Write the sequence to the FASTA file with its name as header\n",
    "                fasta_out.write(f\">{seq_name}\\n{combined_seq}\\n\")\n",
    "\n",
    "        # Define the name for the output alignment file\n",
    "        aligned_file = f\"{output_prefix}_{gene_name}_aligned.fasta\"\n",
    "\n",
    "        # Run MAFFT on the generated FASTA file and save the alignment\n",
    "        with open(aligned_file, 'w') as aligned_out:\n",
    "            subprocess.run([\"mafft\", \"--auto\", fasta_file], stdout=aligned_out)\n",
    "\n",
    "        # Append the aligned file path to the result list\n",
    "        aligned_files.append(aligned_file)\n",
    "\n",
    "    return aligned_files  # Return list of aligned output file paths\n",
    "\n",
    "def GB2MSA_2(alignment_file):\n",
    "    \"\"\"\n",
    "    For each sequence in the alignment:\n",
    "    - Replaces internal blocks of 15 'w' or spaced 'w' (e.g., w-w-w) with question marks.\n",
    "    - Removes columns with only '?' or '-' in all rows.\n",
    "    - Replaces dash blocks flanked by a nucleotide and a question mark with question marks.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    alignment_file : str\n",
    "        Path to the MAFFT-aligned FASTA file to be processed.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Path to the cleaned alignment file.\n",
    "    \"\"\"\n",
    "    alignment = list(SeqIO.parse(alignment_file, \"fasta\"))\n",
    "    updated_records = []\n",
    "\n",
    "    # Step 1: Replace exact block of 15 'w' or 'W' with 15 '?'\n",
    "    for record in alignment:\n",
    "        seq = str(record.seq)\n",
    "        seq_cleaned = seq.replace(\"w\" * 15, \"?\" * 15).replace(\"W\" * 15, \"?\" * 15)\n",
    "        record.seq = Seq(seq_cleaned)\n",
    "        updated_records.append(record)\n",
    "\n",
    "    # Step 2: Replace non-contiguous 'w' blocks (e.g., w-w-w) with '?'\n",
    "    for record in updated_records:\n",
    "        chars = list(str(record.seq))\n",
    "        i = 0\n",
    "        while i < len(chars):\n",
    "            if chars[i].lower() == 'w':\n",
    "                count = 1\n",
    "                indices = [i]\n",
    "                j = i + 1\n",
    "                while j < len(chars) and count < 15:\n",
    "                    if chars[j] == '-':\n",
    "                        indices.append(j)\n",
    "                    elif chars[j].lower() == 'w':\n",
    "                        indices.append(j)\n",
    "                        count += 1\n",
    "                    else:\n",
    "                        break\n",
    "                    j += 1\n",
    "                if count == 15:\n",
    "                    for idx in indices:\n",
    "                        chars[idx] = '?'\n",
    "                i = j\n",
    "            else:\n",
    "                i += 1\n",
    "        record.seq = Seq(''.join(chars))\n",
    "\n",
    "    # Step 3: Remove columns with only '?' or '-' in all rows\n",
    "    sequences = [list(str(record.seq)) for record in updated_records]\n",
    "    if len(set(len(seq) for seq in sequences)) > 1:\n",
    "        raise ValueError(\"Sequences are not of the same length!\")\n",
    "\n",
    "    valid_columns = []\n",
    "    for i in range(len(sequences[0])):\n",
    "        column = [seq[i] for seq in sequences]\n",
    "        if any(base not in ['?', '-'] for base in column):\n",
    "            valid_columns.append(i)\n",
    "\n",
    "    # Rebuild records with cleaned sequences\n",
    "    cleaned_records = []\n",
    "    for record in updated_records:\n",
    "        cleaned_seq = ''.join(str(record.seq)[i] for i in valid_columns)\n",
    "        record.seq = Seq(cleaned_seq)\n",
    "        cleaned_records.append(record)\n",
    "\n",
    "    # Step 4: Replace dash blocks flanked by nucleotide and '?' with '?'\n",
    "    for record in cleaned_records:\n",
    "        chars = list(str(record.seq))\n",
    "        i = 0\n",
    "        while i < len(chars):\n",
    "            if chars[i] == '-':\n",
    "                start = i\n",
    "                while i < len(chars) and chars[i] == '-':\n",
    "                    i += 1\n",
    "                end = i - 1\n",
    "\n",
    "                # Check flanking characters safely\n",
    "                left = chars[start - 1] if start > 0 else ''\n",
    "                right = chars[end + 1] if end + 1 < len(chars) else ''\n",
    "\n",
    "                if ((left in 'ACGTacgt' and right in '?Nn') or \n",
    "                    (left in '?Nn' and right in 'ACGTacgt')):\n",
    "                    for j in range(start, end + 1):\n",
    "                        chars[j] = '?'\n",
    "            else:\n",
    "                i += 1\n",
    "        record.seq = Seq(''.join(chars))\n",
    "\n",
    "    # Step 5: Write to cleaned output\n",
    "    cleaned_file = alignment_file.replace(\".fasta\", \"_GB2MSA.fasta\")\n",
    "    with open(cleaned_file, \"w\") as out_handle:\n",
    "        SeqIO.write(cleaned_records, out_handle, \"fasta\")\n",
    "\n",
    "    return cleaned_file\n",
    "\n",
    "\n",
    "\n",
    "def GB2MSA(input_file, output_prefix, delimiter=',', write_names=True, log=False):\n",
    "    \"\"\"\n",
    "    Complete GenBank-to-MSA pipeline:\n",
    "    1. Downloads sequences from GenBank and aligns them by gene using MAFFT.\n",
    "    2. Cleans the alignments by replacing internal missing data and removing empty columns.\n",
    "    3. Deletes intermediate files ending with '_aligned.fasta'.\n",
    "    4. Optionally logs wall clock and CPU time to a log file named '<output_prefix>_log.txt'.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_file : str\n",
    "        Path to the input CSV/TSV file with sample names and GenBank accessions.\n",
    "\n",
    "    output_prefix : str\n",
    "        Prefix for naming all output files.\n",
    "\n",
    "    delimiter : str, optional (default=',')\n",
    "        Delimiter used in the input file.\n",
    "\n",
    "    write_names : bool, optional (default=True)\n",
    "        Whether to write a file listing sequence names.\n",
    "\n",
    "    log : bool, optional (default=False)\n",
    "        If True, logs wall clock time and CPU time to a file named '<output_prefix>_log.txt'.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    cleaned_files : list of str\n",
    "        List of paths to the cleaned aligned FASTA files for each gene.\n",
    "    \"\"\"\n",
    "    start_wall = time.time()\n",
    "    start_cpu = time.process_time()\n",
    "\n",
    "    # Step 1: Generate aligned FASTA files\n",
    "    aligned_files = GB2MSA_1(input_file, output_prefix, delimiter=delimiter, write_names=write_names)\n",
    "\n",
    "    # Step 2: Clean each aligned FASTA file\n",
    "    cleaned_files = []\n",
    "    for aligned_file in aligned_files:\n",
    "        cleaned_file = GB2MSA_2(aligned_file)\n",
    "        cleaned_files.append(cleaned_file)\n",
    "\n",
    "    # Step 3: Delete intermediate *_aligned.fasta files\n",
    "    for aligned_file in aligned_files:\n",
    "        if aligned_file.endswith(\"_aligned.fasta\") and os.path.exists(aligned_file):\n",
    "            os.remove(aligned_file)\n",
    "\n",
    "    # Step 4: Log timing if requested\n",
    "    if log:\n",
    "        end_wall = time.time()\n",
    "        end_cpu = time.process_time()\n",
    "        wall_time = end_wall - start_wall\n",
    "        cpu_time = end_cpu - start_cpu\n",
    "\n",
    "        log_file = f\"{output_prefix}_log.txt\"\n",
    "        with open(log_file, \"a\") as lf:\n",
    "            lf.write(f\"--- GB2MSA run for '{output_prefix}' ---\\n\")\n",
    "            lf.write(f\"Wall clock time: {wall_time:.2f} seconds\\n\")\n",
    "            lf.write(f\"CPU time: {cpu_time:.2f} seconds\\n\\n\")\n",
    "\n",
    "    return cleaned_files\n",
    "\n",
    "#############################################\n",
    "# Step 1.1: Delete gap artifacts from Mafft #\n",
    "#############################################\n",
    "\n",
    "def remove_all_gap_columns(alignment):\n",
    "    \"\"\"\n",
    "    Remove columns from the alignment where all terminal sequences have a gap ('-').\n",
    "    \n",
    "    Parameters:\n",
    "        alignment (dict): A dictionary where keys are sequence names and values are sequence strings.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Updated alignment with columns removed where all terminal sequences have a gap.\n",
    "    \"\"\"\n",
    "    # Convert the alignment to a list of sequences\n",
    "    sequences = list(alignment.values())\n",
    "    num_sequences = len(sequences)\n",
    "    seq_length = len(sequences[0])  # Assuming all sequences are the same length\n",
    "\n",
    "    # Identify columns that need to be removed (where all terminal sequences have a gap)\n",
    "    columns_to_remove = []\n",
    "    for col in range(seq_length):\n",
    "        # Check if all terminal sequences (sp1, sp2, ...) have a gap ('-') in this column\n",
    "        if all(seq[col] == '-' for seq in sequences):\n",
    "            columns_to_remove.append(col)\n",
    "    \n",
    "    # Remove the identified columns from each sequence\n",
    "    for seq_name, seq in alignment.items():\n",
    "        # Create a new sequence with the columns removed\n",
    "        new_seq = ''.join(seq[col] for col in range(seq_length) if col not in columns_to_remove)\n",
    "        alignment[seq_name] = new_seq\n",
    "    \n",
    "    return alignment\n",
    "\n",
    "# Print\n",
    "remove_all_gap_columns(alignment)\n",
    "print(\"\\n\") # Jump a line\n",
    "print(\"Step 1.1 Delete columns presenting gaps in all taxa (artifacts from MAFFT)\")\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "###################################################\n",
    "# Step 1.2: Delete orphan nucleotides iteratively #\n",
    "###################################################\n",
    "\n",
    "def calculate_orphan_threshold_from_percentile(alignment, percentile=25, log=False, terminal_only=True):\n",
    "    \"\"\"\n",
    "    Calculate orphan threshold based on a specific percentile of gap lengths in the alignment.\n",
    "    \n",
    "    Args:\n",
    "        alignment (dict): A dictionary with sequence IDs as keys and sequences as values.\n",
    "        percentile (float): The percentile to use for setting the orphan threshold (e.g., 75 for the 75th percentile).\n",
    "        log (bool): If True, print the list of gap lengths and the computed orphan threshold.\n",
    "        terminal_only (bool): If True, only consider gap blocks at the terminal positions (start and end of sequences).\n",
    "        \n",
    "    Returns:\n",
    "        int: Calculated orphan threshold based on the specified percentile.\n",
    "    \"\"\"\n",
    "    gap_lengths = []\n",
    "\n",
    "    # Loop through each sequence to calculate gap lengths\n",
    "    for sequence in alignment.values():\n",
    "        sequence_length = len(sequence)\n",
    "        gap_count = 0\n",
    "        in_gap = False\n",
    "\n",
    "        # If terminal_only is True, we'll check only the first and last blocks of gaps\n",
    "        if terminal_only:\n",
    "            # Check for gaps starting at the beginning of the sequence\n",
    "            if sequence[0] == '-':\n",
    "                gap_count = 1\n",
    "                in_gap = True\n",
    "                for nucleotide in sequence[1:]:\n",
    "                    if nucleotide == '-':\n",
    "                        gap_count += 1\n",
    "                    else:\n",
    "                        break  # End of the first terminal gap block\n",
    "                gap_lengths.append(gap_count)  # Add the terminal gap block at the start\n",
    "                \n",
    "            # Check for gaps starting at the end of the sequence\n",
    "            gap_count = 0\n",
    "            in_gap = False\n",
    "            if sequence[-1] == '-':\n",
    "                gap_count = 1\n",
    "                in_gap = True\n",
    "                for nucleotide in reversed(sequence[:-1]):\n",
    "                    if nucleotide == '-':\n",
    "                        gap_count += 1\n",
    "                    else:\n",
    "                        break  # End of the last terminal gap block\n",
    "                gap_lengths.append(gap_count)  # Add the terminal gap block at the end\n",
    "        else:\n",
    "            # Loop through the sequence to find contiguous blocks of gaps (not restricted to terminals)\n",
    "            for nucleotide in sequence:\n",
    "                if nucleotide == '-':\n",
    "                    if not in_gap:\n",
    "                        in_gap = True  # Start of a new gap block\n",
    "                        gap_count = 1  # Start counting the length of the new gap block\n",
    "                    else:\n",
    "                        gap_count += 1  # Continue counting the gap block length\n",
    "                else:\n",
    "                    if in_gap:\n",
    "                        gap_lengths.append(gap_count)  # End of a gap block, save its length\n",
    "                        in_gap = False\n",
    "                        gap_count = 0  # Reset the gap count for the next block\n",
    "\n",
    "            # If the sequence ends with a gap block, make sure to add the last block's length\n",
    "            if in_gap:\n",
    "                gap_lengths.append(gap_count)\n",
    "        \n",
    "    # Calculate the specified percentile of gap lengths\n",
    "    orphan_threshold = int(np.percentile(gap_lengths, percentile))  # Use the given percentile (e.g., 75th percentile)\n",
    "    \n",
    "    # Print the list of gap lengths\n",
    "    if log:\n",
    "        print(\"List of gap lengths:\", gap_lengths)\n",
    "        print(\"Orphan threshold:\", orphan_threshold)\n",
    "        \n",
    "    return orphan_threshold\n",
    "    \n",
    "def delete_orphan_nucleotides2(alignment, orphan_threshold, log_changes=False):\n",
    "    \"\"\"\n",
    "    Iteratively eliminates orphan nucleotides from the first and last block of contiguous nucleotides \n",
    "    for each sequence in the alignment until no changes occur. Optionally generates a log to track changes.\n",
    "    \n",
    "    Args:\n",
    "        alignment (dict): A dictionary with sequence IDs as keys and sequences as values.\n",
    "        orphan_threshold (int): The threshold length for blocks of nucleotides and the maximum number of gaps allowed between blocks.\n",
    "        log_changes (bool): Flag to determine whether to generate a log of the changes made.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the same keys, but orphan nucleotides replaced with dashes in the first and last blocks.\n",
    "        str (optional): A log string capturing all the changes made during the process (if `log_changes=True`).\n",
    "    \"\"\"\n",
    "    alignment_changed = True  # Flag to track if any changes were made\n",
    "    change_log = []  # List to store log messages\n",
    "\n",
    "    while alignment_changed:\n",
    "        alignment_changed = False  # Reset flag to False before starting a new iteration\n",
    "\n",
    "        # Loop through the alignment and check each sequence\n",
    "        for seq_id, sequence in alignment.items():\n",
    "            new_sequence = list(sequence)  # Convert sequence to list for easier modification\n",
    "            sequence_length = len(sequence)\n",
    "\n",
    "            # Identify all blocks of contiguous nucleotides (blocks of non-gap characters)\n",
    "            blocks = []\n",
    "            i = 0\n",
    "            while i < sequence_length:\n",
    "                if new_sequence[i] != '-':\n",
    "                    block_start = i\n",
    "                    while i < sequence_length and new_sequence[i] != '-':\n",
    "                        i += 1\n",
    "                    block_end = i\n",
    "                    blocks.append((block_start, block_end))\n",
    "                else:\n",
    "                    i += 1\n",
    "\n",
    "            # If there's only one block of contiguous nucleotides, skip the sequence\n",
    "            if len(blocks) <= 1:\n",
    "                continue\n",
    "\n",
    "            # Check the first block (if orphaned)\n",
    "            first_block_start, first_block_end = blocks[0]\n",
    "            if first_block_end - first_block_start < orphan_threshold:  # Check if block length is smaller than orphan_threshold\n",
    "                if len(blocks) > 1:  # Check if there's a next block\n",
    "                    # Check the number of gaps between the first block and the second block\n",
    "                    gap_count = 0\n",
    "                    for i in range(first_block_end, blocks[1][0]):\n",
    "                        if new_sequence[i] == '-':\n",
    "                            gap_count += 1\n",
    "                    if gap_count > orphan_threshold:  # If more than orphan_threshold gaps, replace the first block with dashes\n",
    "                        new_sequence[first_block_start:first_block_end] = ['-'] * (first_block_end - first_block_start)\n",
    "                        alignment_changed = True  # Mark that the alignment has been changed\n",
    "                        if log_changes:\n",
    "                            change_log.append(f\"Modified {seq_id}: First block from position {first_block_start} to {first_block_end} replaced with dashes\")\n",
    "\n",
    "            # Check the last block (if orphaned)\n",
    "            last_block_start, last_block_end = blocks[-1]\n",
    "            if last_block_end - last_block_start < orphan_threshold:  # Check if block length is smaller than orphan_threshold\n",
    "                if len(blocks) > 1:  # Check if there's a previous block\n",
    "                    # Check the number of gaps between the last block and the previous block\n",
    "                    gap_count = 0\n",
    "                    for i in range(blocks[-2][1], last_block_start):\n",
    "                        if new_sequence[i] == '-':\n",
    "                            gap_count += 1\n",
    "                    if gap_count > orphan_threshold:  # If more than orphan_threshold gaps, replace the last block with dashes\n",
    "                        new_sequence[last_block_start:last_block_end] = ['-'] * (last_block_end - last_block_start)\n",
    "                        alignment_changed = True  # Mark that the alignment has been changed\n",
    "                        if log_changes:\n",
    "                            change_log.append(f\"Modified {seq_id}: Last block from position {last_block_start} to {last_block_end} replaced with dashes\")\n",
    "\n",
    "            # Convert list back to string and update the alignment if it changed\n",
    "            alignment[seq_id] = ''.join(new_sequence)\n",
    "\n",
    "    # If log_changes is enabled, return the change log along with the alignment\n",
    "    if log_changes:\n",
    "        change_log_text = \"\\n\".join(change_log)\n",
    "        return alignment, change_log_text\n",
    "    else:\n",
    "        return alignment\n",
    "\n",
    "print('\\n')\n",
    "print(\"Step 1.2. Delete non-terminal orphan nucleotides (given a threshold)\")\n",
    "print_colored_alignment(delete_orphan_nucleotides2(alignment, orphan_threshold=5))\n",
    "\n",
    "#########################################################\n",
    "# Step 2.1: Classify terminal (?) and internal gaps (-) #\n",
    "#########################################################\n",
    "\n",
    "def replace_terminal_gaps_dict(alignment):\n",
    "    \"\"\"\n",
    "    Replaces terminal gaps ('-') with '?' in a sequence alignment stored as a dictionary,\n",
    "    while keeping internal gaps as '-'.\n",
    "\n",
    "    Parameters:\n",
    "        alignment (dict): Dictionary with sequence names as keys and sequences as values.\n",
    "\n",
    "    Returns:\n",
    "        dict: Modified alignment with terminal gaps replaced by '?'.\n",
    "    \"\"\"\n",
    "    # Rename 'modified_alignment' to 'alignment'\n",
    "    for name, sequence in alignment.items():\n",
    "        # Find the first and last non-gap characters\n",
    "        first_non_gap = next((i for i, char in enumerate(sequence) if char != \"-\"), None)\n",
    "        last_non_gap = next((i for i, char in enumerate(reversed(sequence), 1) if char != \"-\"), None)\n",
    "        \n",
    "        if first_non_gap is not None and last_non_gap is not None:\n",
    "            last_non_gap = len(sequence) - last_non_gap  # Adjust reversed index\n",
    "            \n",
    "            # Replace terminal gaps with '?' and keep internal gaps as '-'\n",
    "            new_sequence = (\n",
    "                \"?\" * first_non_gap +\n",
    "                sequence[first_non_gap:last_non_gap + 1] +\n",
    "                \"?\" * (len(sequence) - last_non_gap - 1)\n",
    "            )\n",
    "            alignment[name] = new_sequence\n",
    "        else:\n",
    "            # Handle sequences with only gaps\n",
    "            alignment[name] = \"?\" * len(sequence)\n",
    "    \n",
    "    return alignment\n",
    "\n",
    "replace_terminal_gaps_dict(alignment)\n",
    "print(\"\\n\") # Jump a line\n",
    "print(\"Step 2.1 Classify terminal (?) and internal gaps (-)\")\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "##################################################\n",
    "# Step 1.3: Trim terminal invariants iteratively #\n",
    "##################################################\n",
    "def is_parsimony_non_informative(column):\n",
    "    \"\"\"\n",
    "    Check if a column is invariant.\n",
    "    A column is non-informative if:\n",
    "    - All characters are the same.\n",
    "    - The characters include '?' and only one other character (e.g., 'A' and '?').\n",
    "\n",
    "    Parameters:\n",
    "        column (list): A list of characters in a column (e.g., ['A', 'A', 'A', '?']).\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the column is non-informative, False if informative.\n",
    "    \"\"\"\n",
    "    unique_characters = set(column)\n",
    "        \n",
    "    # Check if the column has only one unique character (parsimony non-informative)\n",
    "    if len(unique_characters) == 1:\n",
    "        return True\n",
    "    \n",
    "    # Check if the column contains '?' and exactly one other unique character\n",
    "    if '?' in unique_characters and len(unique_characters) == 2:\n",
    "        return True\n",
    "    \n",
    "    # Otherwise, the column is informative (multiple unique characters without '?')\n",
    "    return False\n",
    "\n",
    "\n",
    "def remove_non_informative_positions(alignment):\n",
    "    \"\"\"\n",
    "    Remove parsimony non-informative positions from both the start and the end of the alignment.\n",
    "    \n",
    "    Parameters:\n",
    "        alignment (dict): Dictionary where keys are sequence names and values are sequences (strings).\n",
    "        \n",
    "    Returns:\n",
    "        dict: Updated alignment with non-informative positions removed.\n",
    "    \"\"\"\n",
    "    # Convert the alignment to a list of sequences\n",
    "    sequences = list(alignment.values())\n",
    "    seq_length = len(sequences[0])\n",
    "   \n",
    "    # Remove non-informative positions from the start\n",
    "    first_position = 0\n",
    "    while first_position < seq_length and is_parsimony_non_informative([seq[first_position] for seq in sequences]):\n",
    "        first_position += 1\n",
    "    \n",
    "    # Remove non-informative positions from the end\n",
    "    last_position = seq_length - 1\n",
    "    while last_position >= first_position and is_parsimony_non_informative([seq[last_position] for seq in sequences]):\n",
    "        last_position -= 1\n",
    "    \n",
    "    # Build the updated alignment and rename the variable to 'alignment'\n",
    "    for seq_name, seq in alignment.items():\n",
    "        alignment[seq_name] = seq[first_position:last_position + 1]\n",
    "    \n",
    "    return alignment\n",
    "\n",
    "# Print\n",
    "remove_non_informative_positions(alignment)\n",
    "print(\"\\n\") # Jump a line\n",
    "print(\"Step 1.3. Trim parsimony non-informative characters in terminal position\")\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "##################################\n",
    "# Step 2.2 Internal missing data #\n",
    "##################################\n",
    "\n",
    "def replace_dashes_with_question_marks(alignment, internal_column_ranges=None, internal_leaves=\"all\", internal_method=\"manual\", internal_threshold=None):\n",
    "    \"\"\"\n",
    "    Replace dashes with question marks in the specified column ranges for each sequence in the alignment.\n",
    "    \n",
    "    Args:\n",
    "        alignment (dict): A dictionary with sequence IDs as keys and sequences as values.\n",
    "        internal_column_ranges (list of tuples, optional): A list of tuples where each tuple defines a range of columns \n",
    "                                                          (inclusive) to check for dashes. E.g., [(5, 10), (50, 60)].\n",
    "        internal_leaves (str or list, optional): If \"all\", replace dashes in all sequences. If a list of sequence IDs \n",
    "                                                   is provided, replace dashes in those sequences only.\n",
    "        internal_method (str, optional): Defines how to replace dashes.\n",
    "            - \"manual\": Specify column ranges and terminal sequences to replace dashes.\n",
    "            - \"semi\": Replace internal blocks of contiguous dashes larger than the threshold with question marks.\n",
    "        internal_threshold (int, optional): The threshold for \"semi\" method. Only internal blocks of contiguous gaps \n",
    "                                             larger than this threshold are replaced with question marks.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Updated alignment with dashes replaced by question marks in the specified columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If internal_leaves is a list, only consider those sequences\n",
    "    if internal_leaves != \"all\":\n",
    "        sequences_to_process = set(internal_leaves)\n",
    "    else:\n",
    "        sequences_to_process = set(alignment.keys())\n",
    "    \n",
    "    # Convert the alignment into a list of sequences for easier indexing\n",
    "    alignment = {seq_id: list(seq) for seq_id, seq in alignment.items()}  # Convert sequences to lists for mutability\n",
    "\n",
    "    if internal_method == \"manual\":\n",
    "        # Replace dashes with question marks in the specified column ranges\n",
    "        for seq_id, seq in alignment.items():\n",
    "            if seq_id not in sequences_to_process:\n",
    "                continue  # Skip sequences not in the internal_leavesinternal_terminals list\n",
    "            \n",
    "            for start, end in internal_column_ranges:\n",
    "                # Ensure the range is within the bounds of the sequence length\n",
    "                start = max(0, start)\n",
    "                end = min(len(seq), end)\n",
    "\n",
    "                # Replace dashes with question marks within the specified range\n",
    "                for i in range(start, end + 1):  # +1 because the end is inclusive\n",
    "                    if seq[i] == '-':\n",
    "                        seq[i] = '?'\n",
    "        \n",
    "    elif internal_method == \"semi\" and internal_threshold is not None:\n",
    "        # Replace internal blocks of contiguous dashes larger than the internal_threshold with question marks\n",
    "        for seq_id, seq in alignment.items():\n",
    "            if seq_id not in sequences_to_process:\n",
    "                continue  # Skip sequences not in the internal_leavesinternal_terminals list\n",
    "\n",
    "            # Identify contiguous blocks of gaps (internal and terminal)\n",
    "            gap_block_start = None\n",
    "            for i in range(len(seq)):\n",
    "                if seq[i] == '-':\n",
    "                    if gap_block_start is None:\n",
    "                        gap_block_start = i  # Start of a new gap block\n",
    "                else:\n",
    "                    if gap_block_start is not None:\n",
    "                        # End of a gap block\n",
    "                        gap_length = i - gap_block_start\n",
    "                        if gap_length > internal_threshold and gap_block_start != 0 and gap_block_start != len(seq) - gap_length:\n",
    "                            # It's an internal block larger than threshold, replace with '?'\n",
    "                            for j in range(gap_block_start, i):\n",
    "                                seq[j] = '?'\n",
    "                        gap_block_start = None  # Reset for the next block\n",
    "            # Check for a gap block at the end of the sequence\n",
    "            if gap_block_start is not None:\n",
    "                gap_length = len(seq) - gap_block_start\n",
    "                if gap_length > internal_threshold and gap_block_start != 0:\n",
    "                    for j in range(gap_block_start, len(seq)):\n",
    "                        seq[j] = '?'\n",
    "        \n",
    "    # Convert the list back to a string\n",
    "    alignment = {seq_id: ''.join(seq) for seq_id, seq in alignment.items()}\n",
    "\n",
    "    return alignment\n",
    "\n",
    "# Print\n",
    "#alignment = replace_dashes_with_question_marks(alignment, internal_column_ranges=[(1,30),(33,40)], internal_leaves=[\"sp2\"], internal_method=\"manual\")\n",
    "alignment = replace_dashes_with_question_marks(alignment, internal_leaves=[\"sp2\"], internal_method=\"semi\", internal_threshold=2)\n",
    "print(\"\\n\") # Jump a line\n",
    "print(\"Step 1.3. Treat internal missing data\")\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "#########################\n",
    "# Step 3.1 Partitioning #\n",
    "#########################\n",
    "\n",
    "def classify_and_insert_hashtags(alignment, \n",
    "                                 partitioning_round=1, \n",
    "                                 log_csv_output=False, \n",
    "                                 csv_file_path=\"contiguous_invariant_blocks.csv\"):\n",
    "    # Step 1: Classify columns as invariant or variant\n",
    "    num_sequences = len(alignment)\n",
    "    num_columns = len(next(iter(alignment.values())))  # Get the number of columns from one sequence\n",
    "\n",
    "    column_types = []  # To store the type of each column (invariant/variant)\n",
    "    contiguous_invariant_blocks = []  # To store lengths and positions of invariant blocks\n",
    "\n",
    "    # Classify each column\n",
    "    for col_idx in range(num_columns):\n",
    "        column = [seq[col_idx] for seq in alignment.values()]  # Get the column from all sequences\n",
    "        unique_values = set(column)\n",
    "        \n",
    "        # A column is invariant if all values are the same (or have only '?' and one nucleotide)\n",
    "        if len(unique_values - {'?'} ) == 1:  # Exclude '?' when checking for unique values\n",
    "            column_types.append('invariant')\n",
    "        else:\n",
    "            column_types.append('variant')\n",
    "\n",
    "    # Step 2: Identify contiguous invariant columns and their lengths\n",
    "    current_invariant_block = None\n",
    "    for col_idx in range(num_columns):\n",
    "        if column_types[col_idx] == 'invariant':\n",
    "            if current_invariant_block is None:\n",
    "                current_invariant_block = {'start': col_idx, 'length': 1}\n",
    "            else:\n",
    "                current_invariant_block['length'] += 1\n",
    "        else:\n",
    "            if current_invariant_block:\n",
    "                contiguous_invariant_blocks.append(current_invariant_block)\n",
    "                current_invariant_block = None\n",
    "    \n",
    "    if current_invariant_block:\n",
    "        contiguous_invariant_blocks.append(current_invariant_block)  # Append the last block if any\n",
    "    \n",
    "    # Step 3: Sort blocks by length (descending order)\n",
    "    contiguous_invariant_blocks.sort(key=lambda x: x['length'], reverse=True)\n",
    "    \n",
    "    # Group the blocks by length\n",
    "    block_lengths = {}\n",
    "    for block in contiguous_invariant_blocks:\n",
    "        block_lengths.setdefault(block['length'], []).append(block)\n",
    "    \n",
    "    # Step 4: Track the positions for inserting hashtags\n",
    "    hashtag_positions = []\n",
    "\n",
    "    # Get the largest block sizes we need to insert hashtags into\n",
    "    block_lengths_sorted = sorted(block_lengths.keys(), reverse=True)\n",
    "    for block_length in block_lengths_sorted[:partitioning_round]:\n",
    "        blocks = block_lengths[block_length]\n",
    "        for block in blocks:\n",
    "            start_idx = block['start']\n",
    "            end_idx = start_idx + block['length'] - 1\n",
    "            \n",
    "            # Calculate the middle index (for blocks of length 4, the middle is between positions 1 and 2)\n",
    "            middle_idx = (start_idx + end_idx) // 2\n",
    "            \n",
    "            # Store the position to insert the hashtag in the block\n",
    "            hashtag_positions.append(middle_idx)\n",
    "\n",
    "    # Step 5: Insert hashtag at the collected positions in all sequences\n",
    "    for seq_id, seq in alignment.items():\n",
    "        # Sort positions to insert hashtags from left to right\n",
    "        sorted_positions = sorted(hashtag_positions)\n",
    "        # Keep track of shifts caused by hashtag insertions\n",
    "        shift = 0\n",
    "        for middle_idx in sorted_positions:  \n",
    "            # Adjust the middle index due to shifts from previous hashtag insertions\n",
    "            adjusted_idx = middle_idx + shift\n",
    "            # Insert the hashtag at the middle of the block\n",
    "            seq = seq[:adjusted_idx + 1] + '#' + seq[adjusted_idx + 1:]\n",
    "            # Update the shift value (increased by 1 for each insertion)\n",
    "            shift += 1\n",
    "        alignment[seq_id] = seq\n",
    "\n",
    "    # Step 6: Optionally, write the contiguous invariant blocks to a CSV file\n",
    "    if log_csv_output:\n",
    "        # Ensure the file path is valid, adjust it if necessary (to save locally)\n",
    "        file_dir = os.path.dirname(csv_file_path)\n",
    "        if not os.path.exists(file_dir) and file_dir:\n",
    "            os.makedirs(file_dir)  # Create the directory if it doesn't exist\n",
    "\n",
    "        # Write the contiguous invariant blocks log to CSV\n",
    "        with open(csv_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Start', 'End', 'Length'])  # Write the header row\n",
    "            for block in contiguous_invariant_blocks:\n",
    "                start_idx = block['start']\n",
    "                end_idx = start_idx + block['length'] - 1  # Calculate the end index\n",
    "                writer.writerow([start_idx, end_idx, block['length']])\n",
    "\n",
    "        print(f\"Log of contiguous invariant blocks written to {csv_file_path}\")\n",
    "\n",
    "    # Return the modified alignment and information about invariant blocks\n",
    "    return alignment, contiguous_invariant_blocks\n",
    "\n",
    "\n",
    "# Print\n",
    "classify_and_insert_hashtags(alignment,partitioning_round=1, log_csv_output=False)\n",
    "print(\"\\n\") # Jump a line\n",
    "print(\"Step 3.1. Insert pound sign in the n-largest block of contiguous invariants\")\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "########################\n",
    "# Step 3.2: Refinement #\n",
    "########################\n",
    "\n",
    "def refinement_question2hyphen(alignment):\n",
    "    \"\"\"\n",
    "    Replaces contiguous '?' characters flanked by '#' with '-' in the sequence alignment.\n",
    "    This includes blocks surrounded by '#' as well as the first and last blocks that are flanked only on one side.\n",
    "\n",
    "    Parameters:\n",
    "        alignment (dict): Dictionary with sequence names as keys and sequences as values.\n",
    "\n",
    "    Returns:\n",
    "        dict: Modified alignment with '?' replaced by '-' where flanked by '#'.\n",
    "    \"\"\"\n",
    "    # Iterate through each sequence in the alignment\n",
    "    for name, sequence in alignment.items():\n",
    "        # Replace '?' surrounded by '#' on both sides (internal blocks)\n",
    "        modified_sequence = re.sub(r'(?<=#)\\?+(?=#)', lambda m: '-' * len(m.group(0)), sequence)\n",
    "        \n",
    "        # Replace '?' in the first block (flanked by # on the right)\n",
    "        modified_sequence = re.sub(r'^(\\?+)(?=#)', lambda m: '-' * len(m.group(0)), modified_sequence)\n",
    "        \n",
    "        # Replace '?' in the last block (flanked by # on the left)\n",
    "        modified_sequence = re.sub(r'(?<=#)(\\?+)$', lambda m: '-' * len(m.group(0)), modified_sequence)\n",
    "\n",
    "        # Update the alignment dictionary with the modified sequence\n",
    "        alignment[name] = modified_sequence\n",
    "\n",
    "    return alignment\n",
    "\n",
    "# Print\n",
    "refinement_question2hyphen(alignment) # Replaces blocks of missing data (consecutive '?' characters flanked by '#') with '-'.\n",
    "print(\"\\n\") # Jump a line\n",
    "print(\"Step 3.2 Replace ? flanked by # with -\")\n",
    "print_colored_alignment(alignment)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8906480e",
   "metadata": {},
   "source": [
    "## prepDyn: Integrated function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295dee1b",
   "metadata": {},
   "source": [
    "Integrating all functions into a straight-forward workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "de857e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GB2MSA on GenBank input...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/labanfibios/anaconda3/lib/python3.10/site-packages/Bio/Entrez/__init__.py:694: UserWarning: \n",
      "            Email address is not specified.\n",
      "\n",
      "            To make use of NCBI's E-utilities, NCBI requires you to specify your\n",
      "            email address with each request.  As an example, if your email address\n",
      "            is A.N.Other@example.com, you can specify it as follows:\n",
      "               from Bio import Entrez\n",
      "               Entrez.email = 'A.N.Other@example.com'\n",
      "            In case of excessive usage of the E-utilities, NCBI will attempt to contact\n",
      "            a user at the email address provided before blocking access to the\n",
      "            E-utilities.\n",
      "  warnings.warn(\n",
      "outputhat23=16\n",
      "treein = 0\n",
      "compacttree = 0\n",
      "stacksize: 8192 kb\n",
      "generating a scoring matrix for nucleotide (dist=200) ... done\n",
      "All-to-all alignment.\n",
      "tbfast-pair (nuc) Version 7.520\n",
      "alg=L, model=DNA200 (2), 2.00 (6.00), -0.10 (-0.30), noshift, amax=0.0\n",
      "0 thread(s)\n",
      "\n",
      "outputhat23=16\n",
      "Loading 'hat3.seed' ... \n",
      "done.\n",
      "Writing hat3 for iterative refinement\n",
      "generating a scoring matrix for nucleotide (dist=200) ... done\n",
      "Gap Penalty = -1.53, +0.00, +0.00\n",
      "tbutree = 1, compacttree = 0\n",
      "Constructing a UPGMA tree ... \n",
      "   20 / 27\n",
      "done.\n",
      "\n",
      "Progressive alignment ... \n",
      "STEP    25 /26 \n",
      "Reallocating..done. *alloclen = 6651\n",
      "STEP    26 /26 \n",
      "done.\n",
      "tbfast (nuc) Version 7.520\n",
      "alg=A, model=DNA200 (2), 1.53 (4.59), -0.00 (-0.00), noshift, amax=0.0\n",
      "1 thread(s)\n",
      "\n",
      "minimumweight = 0.000010\n",
      "autosubalignment = 0.000000\n",
      "nthread = 0\n",
      "randomseed = 0\n",
      "blosum 62 / kimura 200\n",
      "poffset = 0\n",
      "niter = 16\n",
      "sueff_global = 0.100000\n",
      "nadd = 16\n",
      "Loading 'hat3' ... done.\n",
      "generating a scoring matrix for nucleotide (dist=200) ... done\n",
      "\n",
      "   20 / 27\n",
      "Segment   1/  1    1-3346\n",
      "STEP 003-002-0  identical.    identical.    identical.    identical.    identical.    identical.    identical.    identical.    accepted. rejected. identical.    identical.    identical.    identical.    identical.    identical.    identical.   \n",
      "Converged.\n",
      "\n",
      "done\n",
      "dvtditr (nuc) Version 7.520\n",
      "alg=A, model=DNA200 (2), 1.53 (4.59), -0.00 (-0.00), noshift, amax=0.0\n",
      "0 thread(s)\n",
      "\n",
      "\n",
      "Strategy:\n",
      " L-INS-i (Probably most accurate, very slow)\n",
      " Iterative refinement method (<16) with LOCAL pairwise alignment information\n",
      "\n",
      "If unsure which option to use, try 'mafft --auto input > output'.\n",
      "For more information, see 'mafft --help', 'mafft --man' and the mafft page.\n",
      "\n",
      "The default gap scoring scheme has been changed in version 7.110 (2013 Oct).\n",
      "It tends to insert more gaps into gap-rich regions than previous versions.\n",
      "To disable this change, add the --leavegappyregion option.\n",
      "\n",
      "/Users/labanfibios/anaconda3/lib/python3.10/site-packages/Bio/Entrez/__init__.py:694: UserWarning: \n",
      "            Email address is not specified.\n",
      "\n",
      "            To make use of NCBI's E-utilities, NCBI requires you to specify your\n",
      "            email address with each request.  As an example, if your email address\n",
      "            is A.N.Other@example.com, you can specify it as follows:\n",
      "               from Bio import Entrez\n",
      "               Entrez.email = 'A.N.Other@example.com'\n",
      "            In case of excessive usage of the E-utilities, NCBI will attempt to contact\n",
      "            a user at the email address provided before blocking access to the\n",
      "            E-utilities.\n",
      "  warnings.warn(\n",
      "outputhat23=16\n",
      "treein = 0\n",
      "compacttree = 0\n",
      "stacksize: 8192 kb\n",
      "generating a scoring matrix for nucleotide (dist=200) ... done\n",
      "All-to-all alignment.\n",
      "tbfast-pair (nuc) Version 7.520\n",
      "alg=L, model=DNA200 (2), 2.00 (6.00), -0.10 (-0.30), noshift, amax=0.0\n",
      "0 thread(s)\n",
      "\n",
      "outputhat23=16\n",
      "Loading 'hat3.seed' ... \n",
      "done.\n",
      "Writing hat3 for iterative refinement\n",
      "generating a scoring matrix for nucleotide (dist=200) ... done\n",
      "Gap Penalty = -1.53, +0.00, +0.00\n",
      "tbutree = 1, compacttree = 0\n",
      "Constructing a UPGMA tree ... \n",
      "    0 / 9\n",
      "done.\n",
      "\n",
      "Progressive alignment ... \n",
      "STEP     8 /8 \n",
      "done.\n",
      "tbfast (nuc) Version 7.520\n",
      "alg=A, model=DNA200 (2), 1.53 (4.59), -0.00 (-0.00), noshift, amax=0.0\n",
      "1 thread(s)\n",
      "\n",
      "minimumweight = 0.000010\n",
      "autosubalignment = 0.000000\n",
      "nthread = 0\n",
      "randomseed = 0\n",
      "blosum 62 / kimura 200\n",
      "poffset = 0\n",
      "niter = 16\n",
      "sueff_global = 0.100000\n",
      "nadd = 16\n",
      "Loading 'hat3' ... done.\n",
      "generating a scoring matrix for nucleotide (dist=200) ... done\n",
      "\n",
      "    0 / 9\n",
      "Segment   1/  1    1- 386\n",
      "STEP 002-004-1  identical.   \n",
      "Converged.\n",
      "\n",
      "done\n",
      "dvtditr (nuc) Version 7.520\n",
      "alg=A, model=DNA200 (2), 1.53 (4.59), -0.00 (-0.00), noshift, amax=0.0\n",
      "0 thread(s)\n",
      "\n",
      "\n",
      "Strategy:\n",
      " L-INS-i (Probably most accurate, very slow)\n",
      " Iterative refinement method (<16) with LOCAL pairwise alignment information\n",
      "\n",
      "If unsure which option to use, try 'mafft --auto input > output'.\n",
      "For more information, see 'mafft --help', 'mafft --man' and the mafft page.\n",
      "\n",
      "The default gap scoring scheme has been changed in version 7.110 (2013 Oct).\n",
      "It tends to insert more gaps into gap-rich regions than previous versions.\n",
      "To disable this change, add the --leavegappyregion option.\n",
      "\n",
      "/Users/labanfibios/anaconda3/lib/python3.10/site-packages/Bio/Entrez/__init__.py:694: UserWarning: \n",
      "            Email address is not specified.\n",
      "\n",
      "            To make use of NCBI's E-utilities, NCBI requires you to specify your\n",
      "            email address with each request.  As an example, if your email address\n",
      "            is A.N.Other@example.com, you can specify it as follows:\n",
      "               from Bio import Entrez\n",
      "               Entrez.email = 'A.N.Other@example.com'\n",
      "            In case of excessive usage of the E-utilities, NCBI will attempt to contact\n",
      "            a user at the email address provided before blocking access to the\n",
      "            E-utilities.\n",
      "  warnings.warn(\n",
      "outputhat23=16\n",
      "treein = 0\n",
      "compacttree = 0\n",
      "stacksize: 8192 kb\n",
      "generating a scoring matrix for nucleotide (dist=200) ... done\n",
      "All-to-all alignment.\n",
      "tbfast-pair (nuc) Version 7.520\n",
      "alg=L, model=DNA200 (2), 2.00 (6.00), -0.10 (-0.30), noshift, amax=0.0\n",
      "0 thread(s)\n",
      "\n",
      "outputhat23=16\n",
      "Loading 'hat3.seed' ... \n",
      "done.\n",
      "Writing hat3 for iterative refinement\n",
      "generating a scoring matrix for nucleotide (dist=200) ... done\n",
      "Gap Penalty = -1.53, +0.00, +0.00\n",
      "tbutree = 1, compacttree = 0\n",
      "Constructing a UPGMA tree ... \n",
      "   10 / 19\n",
      "done.\n",
      "\n",
      "Progressive alignment ... \n",
      "STEP    18 /18 \n",
      "done.\n",
      "tbfast (nuc) Version 7.520\n",
      "alg=A, model=DNA200 (2), 1.53 (4.59), -0.00 (-0.00), noshift, amax=0.0\n",
      "1 thread(s)\n",
      "\n",
      "minimumweight = 0.000010\n",
      "autosubalignment = 0.000000\n",
      "nthread = 0\n",
      "randomseed = 0\n",
      "blosum 62 / kimura 200\n",
      "poffset = 0\n",
      "niter = 16\n",
      "sueff_global = 0.100000\n",
      "nadd = 16\n",
      "Loading 'hat3' ... done.\n",
      "generating a scoring matrix for nucleotide (dist=200) ... done\n",
      "\n",
      "   10 / 19\n",
      "Segment   1/  1    1- 647\n",
      "STEP 002-017-1  identical.    identical.   \n",
      "Converged.\n",
      "\n",
      "done\n",
      "dvtditr (nuc) Version 7.520\n",
      "alg=A, model=DNA200 (2), 1.53 (4.59), -0.00 (-0.00), noshift, amax=0.0\n",
      "0 thread(s)\n",
      "\n",
      "\n",
      "Strategy:\n",
      " L-INS-i (Probably most accurate, very slow)\n",
      " Iterative refinement method (<16) with LOCAL pairwise alignment information\n",
      "\n",
      "If unsure which option to use, try 'mafft --auto input > output'.\n",
      "For more information, see 'mafft --help', 'mafft --man' and the mafft page.\n",
      "\n",
      "The default gap scoring scheme has been changed in version 7.110 (2013 Oct).\n",
      "It tends to insert more gaps into gap-rich regions than previous versions.\n",
      "To disable this change, add the --leavegappyregion option.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prepDyn(input_file=None, \n",
    "             GB_input=None,\n",
    "             input_format=\"fasta\",\n",
    "             orphan_method=None,\n",
    "             orphan_threshold=5,\n",
    "             percentile=25,\n",
    "             del_inv=True,\n",
    "             internal_method=None,\n",
    "             internal_column_ranges=None,\n",
    "             internal_leaves=\"all\",\n",
    "             internal_threshold=None,\n",
    "             partitioning_round=0,\n",
    "             output_file=None,\n",
    "             output_format=\"fasta\",\n",
    "             log=False):\n",
    "    \"\"\"\n",
    "    Preprocess missing data for dynamic homology in PhyG. First, columns containing \n",
    "    only gaps, orphan nucleotides, and invariant columns can be trimmed. Second, \n",
    "    missing data is coded with question marks. Third, partitions are delimited in \n",
    "    highly conserved regions.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to the input alignment file or directory. Ignored if GB_input is provided.\n",
    "        GB_input (str): Path to a CSV/TSV file containing GenBank accession numbers. If provided,\n",
    "                        sequences will be downloaded from GenBank and aligned before preprocessing.\n",
    "        input_format (str): Format of the input alignment. Options: 'fasta' (default), \n",
    "                            'clustal', 'phylip', or any format accepted by Biopython. \n",
    "        orphan_method (str): The trimming method. By default, trimming orphan nucleotides\n",
    "                             is not performed. Options:\n",
    "                            - 'auto': trim using the 25th percentile;\n",
    "                            - 'semi': trim with a manual threshold.\n",
    "        orphan_threshold (int): Threshold used to trim orphan nucleotides if orphan_method = 'semi'.\n",
    "        percentile (float): Used with orphan_method = 'auto' to define trimming threshold.\n",
    "        del_inv (bool): Whether to trim invariant terminal columns. Default is True.\n",
    "        internal_method (str): Defines how to identify internal missing data. Automatic identificaton\n",
    "                               of missing data is made if GB_input is provided. Otherwise, naive \n",
    "                               options to identify internal missing data are:\n",
    "                               - \"manual\": Use column ranges;\n",
    "                               - \"semi\": Use a threshold for gaps.\n",
    "        internal_column_ranges (list): Column ranges (inclusive) if internal_method = 'manual'.\n",
    "        internal_leaves (str or list): Sequences to apply internal missing data replacement \n",
    "                                       if internal_method is not \"None\".\n",
    "        internal_threshold (int): Used with internal_method = 'semi' to define gap threshold.\n",
    "                                  Contiguous '-' larger than the threshold are replaced with '?'.\n",
    "        partitioning_round (int): Number of partitioning round. Invariant regions are sorted by length\n",
    "                                  in descendant order and the n-largest blocks partitioned using '#'.\n",
    "        output_file (str): Custom prefix for output files. If None, base_name from input_file is used.\n",
    "        output_format (str): Output format. Default is 'fasta'.\n",
    "        log (bool): Whether to write a log with wall-clock time. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        dict: The preprocessed unaligned sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    # Start timers if logging is enabled\n",
    "    if log:\n",
    "        start_wall_time = time.time()\n",
    "        start_cpu_time = time.process_time()\n",
    "\n",
    "    # Step 1: Run GB2MSA if GenBank input is provided\n",
    "    if GB_input is not None:\n",
    "        print(\"Running GB2MSA on GenBank input...\")\n",
    "        gb_output_prefix = output_file if output_file else \"output\"\n",
    "        cleaned_files = GB2MSA(GB_input, output_prefix=gb_output_prefix, log=False)\n",
    "\n",
    "        for file in cleaned_files:\n",
    "            # Extract gene name from filename (e.g., \"output_geneX.fasta\" -> \"geneX\")\n",
    "            gene_name = os.path.splitext(os.path.basename(file))[0].replace(\"output_\", \"\")\n",
    "            # Construct gene-specific output prefix\n",
    "            if output_file:\n",
    "                specific_output_prefix = f\"{output_file}_{gene_name}\"\n",
    "            else:\n",
    "                specific_output_prefix = f\"{gene_name}\"\n",
    "            prepDyn(input_file=file,\n",
    "                     input_format=\"fasta\",\n",
    "                     orphan_method=orphan_method,\n",
    "                     orphan_threshold=orphan_threshold,\n",
    "                     percentile=percentile,\n",
    "                     del_inv=del_inv,\n",
    "                     internal_method=internal_method,\n",
    "                     internal_column_ranges=internal_column_ranges,\n",
    "                     internal_leaves=internal_leaves,\n",
    "                     internal_threshold=internal_threshold,\n",
    "                     partitioning_round=partitioning_round,\n",
    "                     output_format=output_format,\n",
    "                     log=log,\n",
    "                     output_file=specific_output_prefix)\n",
    "        return\n",
    "\n",
    "    # Step 2: If a folder is provided, process each alignment inside\n",
    "    if os.path.isdir(input_file):\n",
    "        for file_name in os.listdir(input_file):\n",
    "            if file_name.endswith(f\".{input_format}\"):\n",
    "                file_path = os.path.join(input_file, file_name)\n",
    "                lumen(file_path,\n",
    "                      input_format=input_format,\n",
    "                      orphan_method=orphan_method,\n",
    "                      orphan_threshold=orphan_threshold,\n",
    "                      percentile=percentile,\n",
    "                      del_inv=del_inv,\n",
    "                      internal_method=internal_method,\n",
    "                      internal_column_ranges=internal_column_ranges,\n",
    "                      internal_leaves=internal_leaves,\n",
    "                      internal_threshold=internal_threshold,\n",
    "                      output_format=output_format)\n",
    "        return\n",
    "\n",
    "    # Step 3: Read and process alignment\n",
    "    alignment = AlignIO.read(input_file, input_format)\n",
    "    alignment = {record.id: str(record.seq) for record in alignment}\n",
    "\n",
    "    remove_all_gap_columns(alignment)\n",
    "\n",
    "    if orphan_method == \"auto\":\n",
    "        orphan_threshold = calculate_orphan_threshold_from_percentile(alignment, percentile, terminal_only=True)\n",
    "        alignment = delete_orphan_nucleotides2(alignment, orphan_threshold)\n",
    "    elif orphan_method == \"semi\":\n",
    "        alignment = delete_orphan_nucleotides2(alignment, orphan_threshold)\n",
    "\n",
    "    alignment = replace_terminal_gaps_dict(alignment)\n",
    "\n",
    "    if del_inv:\n",
    "        alignment = remove_non_informative_positions(alignment)\n",
    "    alignment = replace_terminal_gaps_dict(alignment)\n",
    "\n",
    "    if internal_method == \"manual\":\n",
    "        alignment = replace_dashes_with_question_marks(alignment=alignment, \n",
    "                                                       internal_column_ranges=internal_column_ranges, \n",
    "                                                       internal_leaves=internal_leaves, \n",
    "                                                       internal_method=\"manual\")\n",
    "    elif internal_method == \"semi\":\n",
    "        alignment = replace_dashes_with_question_marks(alignment=alignment, \n",
    "                                                       internal_leaves=internal_leaves, \n",
    "                                                       internal_method=\"semi\", \n",
    "                                                       internal_threshold=internal_threshold)\n",
    "\n",
    "    classify_and_insert_hashtags(alignment, partitioning_round=partitioning_round)\n",
    "    refinement_question2hyphen(alignment)\n",
    "\n",
    "    # Step 4: Write output file\n",
    "    records = [SeqRecord(Seq(seq), id=key, description=\"\") for key, seq in alignment.items()]\n",
    "    base_name = os.path.splitext(os.path.basename(input_file))[0]\n",
    "\n",
    "    if output_file:\n",
    "        output_prefix = f\"{output_file}\"\n",
    "    else:\n",
    "        output_prefix = base_name\n",
    "\n",
    "    output_path = f\"{output_prefix}_preprocessed.{output_format}\"\n",
    "    with open(output_path, \"w\") as output_handle:\n",
    "        SeqIO.write(records, output_handle, output_format)\n",
    "\n",
    "    # Step 5: Write log\n",
    "    if log:\n",
    "        end_wall_time = time.time()\n",
    "        end_cpu_time = time.process_time()\n",
    "        wall_time = end_wall_time - start_wall_time\n",
    "        cpu_time = end_cpu_time - start_cpu_time\n",
    "        log_path = f\"{output_prefix}_log.txt\"\n",
    "        with open(log_path, \"w\") as log_file:\n",
    "            log_file.write(f\"Wall-clock time: {wall_time:.8f} seconds\\n\")\n",
    "            log_file.write(f\"CPU time: {cpu_time:.8f} seconds\\n\")\n",
    "\n",
    "    return alignment\n",
    "    \n",
    "###########\n",
    "# Example #\n",
    "###########\n",
    "prepDyn(GB_input = \"Data/prepDyn_Test1_GB2MSA/teste.csv\", output_file=\"Data/prepDyn_Test1_GB2MSA/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_round=1,\n",
    "         log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331357c5",
   "metadata": {},
   "source": [
    "## Time complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc271c21",
   "metadata": {},
   "source": [
    "To understand time complexity, the following combination of parameters were used to simulate XX MSAs: \n",
    "\n",
    "- Number of leaves = [50, 100, 150, 200, 250, 300, 350, 450, 500]\n",
    "- Number of characters = [500, 1000, 1500, 2000, 2500, 5000, 10000, 50000]\n",
    "- Percentage of terminal missing data = [0.05, 0.1, 0.15, 0.2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac13aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98543a87",
   "metadata": {},
   "source": [
    "## Empirical example 1: Sanger sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feb32a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "161f2c96",
   "metadata": {},
   "source": [
    "## Empirical example 2: Phylogenomic sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e65d13",
   "metadata": {},
   "source": [
    "Empirical example using 352 loci sequeced with AHE from XX species of the Old World treefrogs Rhacophoridae. Note that only files with the extension \".fasta\" in the specified directory will be considered as input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "89ea26a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an alignment (FASTA, Clustal, Phylip, etc.)\n",
    "lumen(input_file = \"/Users/labanfibios/Desktop/Doutorado/Project/B3_PreprocessingPHYG/Example2_Rhacophoridae/\", \n",
    "      input_format=\"fasta\",\n",
    "      orphan_method=\"semi\",\n",
    "      orphan_threshold=10, \n",
    "      internal_method=\"semi\",\n",
    "      internal_threshold=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944dde12",
   "metadata": {},
   "source": [
    "## Empirical example 3: Ancient DNA sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e32c565",
   "metadata": {},
   "source": [
    "## TRASH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7643a37",
   "metadata": {},
   "source": [
    "Other functions used in previous versions of this code (now discarded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ab3c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Step 1.2: Delete orphan nucleotides in the first and last positions #\n",
    "#######################################################################\n",
    "\n",
    "def delete_orphan_nucleotides(alignment, orphan_threshold=10):\n",
    "    \"\"\"\n",
    "    Orphan nucleotides are artifacts from static alignment, in which one or a few nucleotides\n",
    "    are accidentally not aligned in the beginning or end of the sequences. Manual deletion or\n",
    "    local alignment should be performed, but here I provide a heuristic, optional function to\n",
    "    tentatively identify and delete orphan nucleotides in terminal positions.\n",
    "    \n",
    "    For each sequence in the alignment, replaces orphan nucleotides at the \n",
    "    beginning or end of the sequence with a hyphen if they are followed or preceded \n",
    "    by a number of gaps equal to or greater than the orphan_threshold, but only if \n",
    "    the number of orphan nucleotides are fewer than or equal to the orphan_threshold in length.\n",
    "    \n",
    "    Parameters:\n",
    "        alignment (dict): Dictionary with sequence names as keys and sequences as values.\n",
    "        orphan_threshold (int): The number of consecutive gaps after the orphan nucleotide\n",
    "                                 required for it to be considered an orphan.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Modified alignment with orphan nucleotides replaced by '-'.\n",
    "    \"\"\"\n",
    "    \n",
    "    for seq_name, seq in alignment.items():\n",
    "        # Check the first position for orphan nucleotides (beginning of sequence)\n",
    "        if seq[0] != \"-\":\n",
    "            i = 0\n",
    "            # Find contiguous orphan nucleotides at the beginning of the sequence\n",
    "            while i < len(seq) and seq[i] != \"-\" and seq[i] != \"#\":\n",
    "                i += 1\n",
    "            \n",
    "            # If the contiguous orphan nucleotides are followed by enough gaps\n",
    "            if i > 0 and seq[i:i+orphan_threshold] == \"-\" * orphan_threshold:\n",
    "                # Only replace if the length of contiguous orphan nucleotides is <= orphan_threshold\n",
    "                if i <= orphan_threshold:\n",
    "                    seq = \"-\" * i + seq[i:]\n",
    "        \n",
    "        # Check the last position for orphan nucleotides (end of sequence)\n",
    "        if seq[-1] != \"-\":\n",
    "            i = len(seq) - 1\n",
    "            # Find contiguous orphan nucleotides at the end of the sequence\n",
    "            while i >= 0 and seq[i] != \"-\" and seq[i] != \"#\":\n",
    "                i -= 1\n",
    "            \n",
    "            # If the contiguous orphan nucleotides are preceded by enough gaps\n",
    "            if i < len(seq) - 1 and seq[i - orphan_threshold:i] == \"-\" * orphan_threshold:\n",
    "                # Only replace if the length of contiguous orphan nucleotides is <= orphan_threshold\n",
    "                if len(seq) - i - 1 <= orphan_threshold:\n",
    "                    seq = seq[:i+1] + \"-\" * (len(seq) - i - 1)\n",
    "        \n",
    "        # Update the sequence in the alignment dictionary\n",
    "        alignment[seq_name] = seq\n",
    "    \n",
    "    return alignment\n",
    "\n",
    "print('\\n')\n",
    "print(\"Step 1.2. Delete terminal orphan nucleotides (given a threshold)\")\n",
    "print_colored_alignment(delete_orphan_nucleotides(alignment, orphan_threshold=5))\n",
    "\n",
    "###########################################################\n",
    "# Step 2.2: Insert breaks in terminal gap opening/closure #\n",
    "###########################################################\n",
    "def add_breaks_terminal(alignment):\n",
    "    \"\"\"\n",
    "    Add # in all instances of terminal gap opening/closure.\n",
    "       \n",
    "    Parameters:\n",
    "        alignment (dict): Dictionary where keys are sequence names and values are sequences.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Updated alignment with '#' before terminal gap opening and after terminal gap closure.\n",
    "    \"\"\"\n",
    "    # Determine the length of the sequences\n",
    "    seq_length = len(next(iter(alignment.values())))\n",
    "\n",
    "    # Initialize a list to keep track of positions that need '#' in all sequences\n",
    "    hash_positions = [False] * seq_length\n",
    "\n",
    "    # Iterate through each sequence to find gap regions\n",
    "    for seq in alignment.values():\n",
    "        i = 0\n",
    "        while i < seq_length:\n",
    "            if seq[i] == '?':\n",
    "                # Found the start of a gap region\n",
    "                start = i\n",
    "                while i < seq_length and seq[i] == '?':\n",
    "                    i += 1\n",
    "                end = i\n",
    "                # Mark the positions for this gap region (avoid marking the first column)\n",
    "                if start > 0:\n",
    "                    hash_positions[start] = True\n",
    "                if end < seq_length:\n",
    "                    hash_positions[end] = True\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    # Update each sequence with '#' at the identified positions\n",
    "    for key in alignment:\n",
    "        new_seq = []\n",
    "        for i in range(seq_length):\n",
    "            if hash_positions[i]:\n",
    "                new_seq.append('#')\n",
    "            new_seq.append(alignment[key][i])\n",
    "        # Handle the case where the last position is a gap\n",
    "        if hash_positions[-1]:\n",
    "            new_seq.append('#')\n",
    "        alignment[key] = ''.join(new_seq)\n",
    "        \n",
    "# Print\n",
    "add_breaks_terminal(alignment)\n",
    "print(\"\\n\") # Jump a line\n",
    "print(\"Step 2.2 Insert pound signs in all cases of TERMINAL gap opening/closure\")\n",
    "print_colored_alignment(alignment)\n",
    "        \n",
    "###############################################################################\n",
    "# Step 2.3: Insert breaks in internal gap opening/closure if block length > X #\n",
    "###############################################################################\n",
    "\n",
    "def add_breaks_internal(alignment, X=1, Y=None):\n",
    "    \"\"\"\n",
    "    Add '#' flanking internal gap blocks if the gap block size exceeds X.\n",
    "    \n",
    "    Parameters:\n",
    "        alignment (dict): Dictionary where keys are sequence names and values are sequences.\n",
    "        X (int): Threshold for the minimum gap size to trigger '#' insertion (default: X = 1).\n",
    "        Y (list of ranges): Optional list of ranges (start, end) defining blocks of gaps that must be flanked by '#'. Indexing of columns in the matrix starts with zero (not one). In internal gap blocks, Y should be specified if the minimum size of known missing data blocks (e.g. incomplete sequences due to primers) is not larger than the maximum size of gap blocks.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated alignment with '#' flanking internal gap blocks.\n",
    "    \"\"\"\n",
    "    # Determine the length of the sequences\n",
    "    seq_length = len(next(iter(alignment.values())))\n",
    "\n",
    "    # Initialize a list to keep track of positions that need '#' in all sequences\n",
    "    hash_positions = [False] * seq_length\n",
    "\n",
    "    # Iterate through each sequence to find internal gap block\n",
    "    for seq in alignment.values():\n",
    "        i = 0  # Start from position 0\n",
    "        while i < seq_length:\n",
    "            if seq[i] == '-':\n",
    "                # Find the start of a gap block\n",
    "                start = i\n",
    "                while i < seq_length and seq[i] == '-':\n",
    "                    i += 1\n",
    "                end = i\n",
    "                # Mark the position for this gap opening/closure ONLY if the gap block is larger than X\n",
    "                if (end - start) > X:\n",
    "                    hash_positions[start] = True  # Mark the start of gap block\n",
    "                    if end < seq_length:\n",
    "                        hash_positions[end] = True  # Mark the end of gap block\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    # Add '#' around the ranges specified in Y\n",
    "    if Y:\n",
    "        for range_start, range_end in Y:\n",
    "            if range_start >= 0 and range_end < seq_length:\n",
    "                hash_positions[range_start] = True\n",
    "                if range_end + 1 < seq_length:\n",
    "                    hash_positions[range_end + 1] = True\n",
    "                    \n",
    "    # Add '#' at the identified positions in each sequence\n",
    "    for key in alignment:\n",
    "        new_seq = []\n",
    "        for i in range(seq_length):\n",
    "            if i > 0 and hash_positions[i]:  # Avoid position 1 and single-gap blocks\n",
    "                new_seq.append('#')\n",
    "            new_seq.append(alignment[key][i])\n",
    "        if hash_positions[-1]:\n",
    "            new_seq.append('#')  # Ensure we append '#' at the end if necessary\n",
    "        \n",
    "        # After modifying the sequence, remove consecutive '#' characters\n",
    "        modified_seq = ''.join(new_seq)\n",
    "        modified_seq = re.sub(r'#+', '#', modified_seq)  # Replace multiple '#' with a single '#'\n",
    "        \n",
    "        alignment[key] = modified_seq\n",
    "    \n",
    "    return alignment  # Return the modified alignment\n",
    "\n",
    "\n",
    "# Print\n",
    "add_breaks_internal(alignment, X=2) # insert '#' flanking internal gap blocks larger than 2\n",
    "#add_breaks_internal(alignment, X=2, Y=[(37, 42)]) # insert '#' in columns 37 and 42\n",
    "print(\"\\n\") # Jump a line\n",
    "print(\"Step 2.3 Insert pound signs if INTERNAL block length > X (e.g. X = 1)\")\n",
    "print_colored_alignment(alignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57ea93e",
   "metadata": {},
   "source": [
    "Some functions to use the trimal method. However, even when we use the \"terminal_only\" option, internal columns are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "9975b90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0. Align sequences statically\n",
      "sp1: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp2: \u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[32mg\u001b[0m\u001b[97mn\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp3: \u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp4: \u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp5: \u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp6: \u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "\n",
      "\n",
      "Step 1.2. Trim problematic positions using trimai\n",
      "sp1: \u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp2: \u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[32mg\u001b[0m\u001b[97mn\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp3: \u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp4: \u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp5: \u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp6: \u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Input data\n",
    "alignment = {\n",
    "    'sp1': \"tt-------------agtagt-ctccaccg-cgccaccgtcgc-------a\",\n",
    "    'sp2': \"-ccaccgtc------gngtag-ctccaccg-cgc-------gccaacagta\",\n",
    "    'sp3': \"-ccaccgtcgccaacagtagt------ccg--------tt-----------\",\n",
    "    'sp4': \"------ct-----------gt-ctccaccg-cgccacc-tcgcca-aagta\",\n",
    "    'sp5': \"--caccgtcgccaacagtagt-ctccaccg-cgccaccgtcgcca-cag-a\",\n",
    "    'sp6': \"--caccgtc-ccaacagtagt-ctccaccgtcgcaccg-tcgcaa-c----\"\n",
    "} \n",
    "\n",
    "print(\"Step 0. Align sequences statically\")\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "#print('\\n')\n",
    "#print(\"Step 1.1 Delete columns presenting gaps in all taxa\")\n",
    "#print_colored_alignment(remove_all_gap_columns(alignment))\n",
    "\n",
    "print('\\n')\n",
    "print(\"Step 1.2. Trim problematic positions using trimai\")\n",
    "\n",
    "# Convert dictionary to Biopython objects\n",
    "alignment = dict_to_multiple_seq_alignment(alignment)\n",
    "# Convert Biopython to pytrimal objects\n",
    "alignment = Alignment.from_biopython(alignment)\n",
    "# Trim using trimal\n",
    "trimmer = AutomaticTrimmer(method=\"gappyout\") \n",
    "#trimmer = AutomaticTrimmer(method=\"automated1\")\n",
    "alignment = trimmer.trim(alignment)\n",
    "alignment = pytrimal.TrimmedAlignment.terminal_only(alignment)\n",
    "\n",
    "\n",
    "# Convert the trimal to Biopython\n",
    "alignment = Alignment.to_biopython(alignment)\n",
    "# Convert MultipleSeqAlignment to dictionary\n",
    "alignment = {record.id: str(record.seq) for record in alignment}\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "# Print\n",
    "#show_alignment(alignment)\n",
    "\n",
    "# Write in fasta format\n",
    "#Alignment.dump(alignment, file=\"Example_trimai.fasta\", format=\"fasta\")\n",
    "\n",
    "def calculate_statistics(values):\n",
    "    mean = sum(values) / len(values)  # Mean = Sum of values / Number of values\n",
    "    min_value = min(values)           # Minimum value\n",
    "    max_value = max(values)           # Maximum value\n",
    "    \n",
    "    return mean, min_value, max_value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
