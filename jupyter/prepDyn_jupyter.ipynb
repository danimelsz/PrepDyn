{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8070de8d",
   "metadata": {},
   "source": [
    "# *prepDyn*: Preprocessing of missing data for dynamic homology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc4d943",
   "metadata": {},
   "source": [
    "In dynamic homology, data are typically preprocessed to distinguish differences in sequence length resulting from missing data or insertion-deletion events. However, previous empirical studies using POY/PhyG manually preprocessed missing data with varying approaches. Here we demonstrate that coding missing data with dashes (â€“) or IUPAC Ns increase tree costs and are biologically inappropriate. Although inserting pound signs (#) around blocks of missing data has been a common solution, it reduces the severity of homology tests and precludes the discovery of the optimal tree. Therefore, missing data should be coded with question marks (?) to minimize tree costs, whereas pound signs should be inserted only on highly conserved regions. To balance time complexity and severity of homology test, we propose a heuristic to successively partition data. All procedures are implemented in a collection of Python scripts to facilitate the preprocessing of input sequences to PhyG. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4b3d0a",
   "metadata": {},
   "source": [
    "## Pre-amble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09021b2e",
   "metadata": {},
   "source": [
    "Mafft should be installed locally in the system PATH (e.g. in Ubuntu, `$ sudo apt install mafft`). In addition, the following Python modules should be installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cc9efc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "from Bio import AlignIO, Entrez, SeqIO\n",
    "from Bio.Align import MultipleSeqAlignment\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import csv\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "#import pytrimal\n",
    "import re\n",
    "#import rich.console\n",
    "#import rich.panel\n",
    "#from rich_msa import RichAlignment\n",
    "import subprocess\n",
    "import tempfile\n",
    "from termcolor import colored\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1b3422",
   "metadata": {},
   "source": [
    "Additional custom functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e660b3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION 1: Define functions to visualize nucleotides in different colors\n",
    "# Define colors for each nucleotide (case insensitive)\n",
    "def color_nucleotide(nucleotide):\n",
    "    color_map = {\n",
    "        \"A\": \"red\",\n",
    "        \"T\": \"blue\",\n",
    "        \"G\": \"green\",\n",
    "        \"C\": \"yellow\",\n",
    "        \"-\": \"white\",\n",
    "        \"#\": \"black\",\n",
    "        \"?\": \"magenta\"\n",
    "    }\n",
    "    return colored(nucleotide, color_map.get(nucleotide.upper(), \"white\"))\n",
    "# Function to print the alignment\n",
    "def print_colored_alignment(alignment):\n",
    "    \"\"\"\n",
    "    Prints a DNA alignment with each nucleotide in a different color.\n",
    "\n",
    "    Parameters:\n",
    "        alignment (dict): A dictionary with sequence names as keys and DNA sequences as values.\n",
    "    \"\"\"\n",
    "    for name, sequence in alignment.items():\n",
    "        colored_seq = ''.join(color_nucleotide(n) for n in sequence)\n",
    "        print(f\"{name}: {colored_seq}\")\n",
    "\n",
    "\n",
    "# FUNCTION 2: Print alignments using trimAl\n",
    "def show_alignment(alignment):\n",
    "    console = rich.console.Console(width=len(alignment.sequences[0])+40)\n",
    "    widget = RichAlignment(names=[n.decode() for n in alignment.names], sequences=alignment.sequences, max_name_width=30)\n",
    "    panel = rich.panel.Panel(widget, title_align=\"left\", title=\"({} residues, {} sequences)\".format(len(alignment.sequences[0]), len(alignment.sequences)))\n",
    "    console.print(panel)\n",
    "\n",
    "# FUNCTION 3: Convert dictionaries to MultipleSeqAlignment\n",
    "def dict_to_multiple_seq_alignment(seq_dict):\n",
    "    \"\"\"\n",
    "    Converts a dictionary of sequences into a MultipleSeqAlignment object.\n",
    "    \n",
    "    Args:\n",
    "        seq_dict (dict): A dictionary where keys are sequence identifiers and values are sequences (strings).\n",
    "        \n",
    "    Returns:\n",
    "        MultipleSeqAlignment: A Biopython MultipleSeqAlignment object.\n",
    "    \"\"\"\n",
    "    # Create a list of SeqRecord objects from the dictionary\n",
    "    seq_records = []\n",
    "    \n",
    "    for seq_id, seq_str in seq_dict.items():\n",
    "        # Create a SeqRecord for each sequence\n",
    "        seq = Seq(seq_str)\n",
    "        seq_record = SeqRecord(seq, id=seq_id, description=\"\")\n",
    "        seq_records.append(seq_record)\n",
    "    \n",
    "    # Create and return the MultipleSeqAlignment object\n",
    "    alignment = MultipleSeqAlignment(seq_records)\n",
    "    return alignment\n",
    "\n",
    "# FUNCTION 4: List lengths of blocks of contiguous gaps in internal and terminal positions\n",
    "def list_gap_blocks_by_type(alignment, plot_distribution=False):\n",
    "    \"\"\"\n",
    "    Identify all blocks of contiguous gaps in the DNA alignment.\n",
    "    Classify gap blocks into terminal and internal blocks.\n",
    "    Optionally, plot the distributions of gap block lengths for terminal and internal blocks.\n",
    "    \n",
    "    Args:\n",
    "        alignment (dict): A dictionary with sequence IDs as keys and sequences as values.\n",
    "        plot_distribution (bool): If True, plot the distributions of terminal and internal gap block lengths.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Two lists:\n",
    "            - terminal_blocks: A list of gap block lengths at the start or end of sequences.\n",
    "            - internal_blocks: A list of gap block lengths in the middle of sequences.\n",
    "    \"\"\"\n",
    "    terminal_blocks = []\n",
    "    internal_blocks = []\n",
    "    \n",
    "    # Iterate over each sequence in the alignment\n",
    "    for seq_id, sequence in alignment.items():\n",
    "        sequence_length = len(sequence)\n",
    "        gap_count = 0\n",
    "        in_gap = False\n",
    "\n",
    "        # Identify gap blocks and separate terminal vs internal\n",
    "        for i, nucleotide in enumerate(sequence):\n",
    "            if nucleotide == '-':  # We are in a gap\n",
    "                if not in_gap:\n",
    "                    in_gap = True\n",
    "                    gap_count = 1  # Start a new gap block\n",
    "                else:\n",
    "                    gap_count += 1  # Continue counting the current gap block\n",
    "            else:  # We encountered a non-gap nucleotide\n",
    "                if in_gap:\n",
    "                    # End of a gap block\n",
    "                    # Check if this gap block is terminal (at start or end of the sequence)\n",
    "                    if i == sequence_length or (i - gap_count == 0):\n",
    "                        terminal_blocks.append(gap_count)\n",
    "                    else:\n",
    "                        internal_blocks.append(gap_count)\n",
    "                    in_gap = False\n",
    "                    gap_count = 0  # Reset gap count for the next block\n",
    "\n",
    "        # If the sequence ends with a gap block, add it to the appropriate list\n",
    "        if in_gap:\n",
    "            if sequence[-1] == '-':  # Last character is a gap\n",
    "                terminal_blocks.append(gap_count)\n",
    "            else:\n",
    "                internal_blocks.append(gap_count)\n",
    "\n",
    "    # Optionally plot the distributions of terminal and internal blocks\n",
    "    if plot_distribution:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        # Plotting terminal blocks\n",
    "        plt.hist(terminal_blocks, bins=20, alpha=0.5, label='Terminal Blocks', color='blue')\n",
    "        # Plotting internal blocks\n",
    "        plt.hist(internal_blocks, bins=20, alpha=0.5, label='Internal Blocks', color='red')\n",
    "        \n",
    "        plt.xlabel('Gap Block Length')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Distribution of Gap Block Lengths')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()\n",
    "\n",
    "    return terminal_blocks, internal_blocks\n",
    "\n",
    "# FUNCTION 5: Remove underscores in the beggining of file names\n",
    "def remove_leading_underscores(file_path):\n",
    "    \"\"\"\n",
    "    Remove leading contiguous underscores from the beginning of the file name.\n",
    "    If a directory is provided, it renames all files in that directory.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The full path of the file or directory.\n",
    "    \n",
    "    Returns:\n",
    "        str or None: The new file path with leading underscores removed if a file, \n",
    "                     or None if a directory (modifies in place).\n",
    "    \"\"\"\n",
    "    if os.path.isdir(file_path):\n",
    "        # If it's a directory, rename all files inside it\n",
    "        for root, dirs, files in os.walk(file_path):\n",
    "            for file in files:\n",
    "                old_file_path = os.path.join(root, file)\n",
    "                new_file_name = file.lstrip('_')\n",
    "                new_file_path = os.path.join(root, new_file_name)\n",
    "                \n",
    "                if old_file_path != new_file_path:\n",
    "                    os.rename(old_file_path, new_file_path)\n",
    "        return None  # No return for directories, as the renaming is in place\n",
    "    else:\n",
    "        # If it's a single file, rename it\n",
    "        dir_name, file_name = os.path.split(file_path)\n",
    "        new_file_name = file_name.lstrip('_')\n",
    "        new_file_path = os.path.join(dir_name, new_file_name)\n",
    "        if file_path != new_file_path:\n",
    "            os.rename(file_path, new_file_path)\n",
    "        return new_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14de974c",
   "metadata": {},
   "source": [
    "## Main functions\n",
    "\n",
    "We define below the main functions of *prepDyn*. To test each of the functions individually, we created a synthetic example. In such example, the step of data collection from GenBank was skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd9f6133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "outputhat23=16\n",
      "treein = 0\n",
      "compacttree = 0\n",
      "stacksize: 8192 kb\n",
      "generating a scoring matrix for nucleotide (dist=200) ... done\n",
      "All-to-all alignment.\n",
      "tbfast-pair (nuc) Version 7.520\n",
      "alg=L, model=DNA200 (2), 2.00 (6.00), -0.10 (-0.30), noshift, amax=0.0\n",
      "0 thread(s)\n",
      "\n",
      "outputhat23=16\n",
      "Loading 'hat3.seed' ... \n",
      "done.\n",
      "Writing hat3 for iterative refinement\n",
      "generating a scoring matrix for nucleotide (dist=200) ... done\n",
      "Gap Penalty = -1.53, +0.00, +0.00\n",
      "tbutree = 1, compacttree = 0\n",
      "Constructing a UPGMA tree ... \n",
      "    0 / 6\n",
      "done.\n",
      "\n",
      "Progressive alignment ... \n",
      "STEP     5 /5 \n",
      "done.\n",
      "tbfast (nuc) Version 7.520\n",
      "alg=A, model=DNA200 (2), 1.53 (4.59), -0.00 (-0.00), noshift, amax=0.0\n",
      "1 thread(s)\n",
      "\n",
      "minimumweight = 0.000010\n",
      "autosubalignment = 0.000000\n",
      "nthread = 0\n",
      "randomseed = 0\n",
      "blosum 62 / kimura 200\n",
      "poffset = 0\n",
      "niter = 16\n",
      "sueff_global = 0.100000\n",
      "nadd = 16\n",
      "Loading 'hat3' ... done.\n",
      "generating a scoring matrix for nucleotide (dist=200) ... done\n",
      "\n",
      "    0 / 6\n",
      "Segment   1/  1    1-  52\n",
      "STEP 002-002-0  identical.   \n",
      "Converged.\n",
      "\n",
      "done\n",
      "dvtditr (nuc) Version 7.520\n",
      "alg=A, model=DNA200 (2), 1.53 (4.59), -0.00 (-0.00), noshift, amax=0.0\n",
      "0 thread(s)\n",
      "\n",
      "\n",
      "Strategy:\n",
      " L-INS-i (Probably most accurate, very slow)\n",
      " Iterative refinement method (<16) with LOCAL pairwise alignment information\n",
      "\n",
      "If unsure which option to use, try 'mafft --auto input > output'.\n",
      "For more information, see 'mafft --help', 'mafft --man' and the mafft page.\n",
      "\n",
      "The default gap scoring scheme has been changed in version 7.110 (2013 Oct).\n",
      "It tends to insert more gaps into gap-rich regions than previous versions.\n",
      "To disable this change, add the --leavegappyregion option.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input data\n",
    "alignment = {\n",
    "    'sp1': \"TGCACCGTCGCCAACAGTAGTCCTCCACCGTCGCCACCGTCNCCAACAG??\",\n",
    "    'sp2': \"T-CACCGTCGCCAACAGTAGTCCTCCACCGTCGCCNNNNNNTCCAACA???\",\n",
    "    'sp3': \"TG-ACCGTCGCCAACAGTAGTCCTCCACCGTCGCCACCGTCGCCAA-----\",\n",
    "    'sp4': \"TGC-CCGTCGCCNNNNNNNNNNNNNNNNNNTCGCNACCGTCGCCAACATTT\",\n",
    "    'sp5': \"TGCA-CGTCGCCAACAGTAGTCCTCCACCGTCGCCACCGTCGCCAACAGTG\",\n",
    "    'sp6': \"TGCAC-GTCGCCAACAGTAGTCCTCCACCGTCGCCACCGTCGCCAACAGTA\",\n",
    "} \n",
    "\n",
    "# Create a temporary file to save the sequences in FASTA format\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix='.fasta', mode='w') as fasta_file:\n",
    "    # Write the sequences to the file\n",
    "    for name, seq in alignment.items():\n",
    "        fasta_file.write(f\">{name}\\n{seq}\\n\")\n",
    "    fasta_file_path = fasta_file.name\n",
    "\n",
    "# Define the output file for the alignment\n",
    "aligned_fasta_file_path = fasta_file_path.replace('.fasta', '_aligned.fasta')\n",
    "# Run MAFFT\n",
    "mafft_command = ['mafft', '--auto', fasta_file_path]\n",
    "with open(aligned_fasta_file_path, 'w') as aligned_file:\n",
    "    subprocess.run(mafft_command, stdout=aligned_file)\n",
    "\n",
    "# Input data\n",
    "alignment = AlignIO.read(aligned_fasta_file_path, \"fasta\")\n",
    "type(alignment) # Check the class of alignment (MultipleSeqAlignment\n",
    "# Convert MultipleSeqAlignment to dictionary\n",
    "alignment = {record.id: str(record.seq) for record in alignment}\n",
    "type(alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98b172c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0. Align sequences statically\n",
      "sp1: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp2: \u001b[34mT\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp3: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[97m-\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp4: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp5: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp6: \u001b[34mT\u001b[0m\u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "\n",
      "\n",
      "Step 1.1 Delete columns presenting gaps in all taxa (artifacts from MAFFT)\n",
      "sp1: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp2: \u001b[34mT\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp3: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[97m-\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp4: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp5: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp6: \u001b[34mT\u001b[0m\u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "\n",
      "\n",
      "Step 1.2. Delete terminal orphan nucleotides (given a threshold)\n",
      "sp1: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp2: \u001b[34mT\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp3: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[97m-\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp4: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp5: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp6: \u001b[34mT\u001b[0m\u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "\n",
      "\n",
      "Step 2.1 Classify terminal (?) and internal gaps (-)\n",
      "sp1: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp2: \u001b[34mT\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp3: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[97m-\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp4: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp5: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp6: \u001b[34mT\u001b[0m\u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "\n",
      "\n",
      "Step 1.3. Trim parsimony non-informative characters in terminal position\n",
      "sp1: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\n",
      "sp2: \u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\n",
      "sp3: \u001b[32mG\u001b[0m\u001b[97m-\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp4: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp5: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp6: \u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\n",
      "\n",
      "\n",
      "Step 1.3. Treat internal missing data\n",
      "sp1: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\n",
      "sp2: \u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\n",
      "sp3: \u001b[32mG\u001b[0m\u001b[97m-\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp4: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp5: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp6: \u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\n",
      "Threshold = size of the 1st largest block: 18 columns (columns 27 to 44)\n",
      "Merged blocks at columns 0-0 and 2-9 (deleted '#' at col 1) â†’ size=9\n",
      "Merged blocks at columns 0-8 and 10-12 (deleted '#' at col 9) â†’ size=12\n",
      "Merged blocks at columns 0-11 and 13-13 (deleted '#' at col 12) â†’ size=13\n",
      "Merged blocks at columns 14-19 and 21-22 (deleted '#' at col 20) â†’ size=8\n",
      "\n",
      "\n",
      "Step 3.1. Insert pound signs\n",
      "sp1: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\n",
      "sp2: \u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[30m#\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\n",
      "sp3: \u001b[32mG\u001b[0m\u001b[97m-\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[30m#\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp4: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[30m#\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[30m#\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp5: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp6: \u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\n",
      "\n",
      "\n",
      "Step 3.2.1 Replace ? flanked by # with -\n",
      "sp1: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\n",
      "sp2: \u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[30m#\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\n",
      "sp3: \u001b[32mG\u001b[0m\u001b[97m-\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[30m#\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp4: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[30m#\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[30m#\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp5: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp6: \u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\n",
      "\n",
      "\n",
      "Step 3.2.2 Delete columns of W (artifacts from internal missing data identification by GB2MSA)\n",
      "sp1: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\n",
      "sp2: \u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[30m#\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\n",
      "sp3: \u001b[32mG\u001b[0m\u001b[97m-\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[30m#\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp4: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[30m#\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[30m#\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp5: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp6: \u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\n",
      "\n",
      "\n",
      "Step 3.2.3 Replace IUPAC N with question marks)\n",
      "sp1: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\n",
      "sp2: \u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[30m#\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\n",
      "sp3: \u001b[32mG\u001b[0m\u001b[97m-\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[30m#\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp4: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[30m#\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[30m#\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[35m?\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp5: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp6: \u001b[35m?\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[30m#\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Comment the alignment below if you want to use the MAFFT alignment above\n",
    "alignment = {\n",
    "    'sp1': \"TGCACCGTCGCCAACAGTAGTC??????????????????CNCCAACA---\",\n",
    "    'sp2': \"T-CACCGTCG????CAGTAGTCCTCCACCGTCGCCNNNNNNTCCAACA?--\",\n",
    "    'sp3': \"TG-ACCGTCGCCA???????CACTCCACCGTCGCCACCGTCGCCAA-----\",\n",
    "    'sp4': \"TGC-CCGTCGCCNNNNNNNNNNNNNNNNNNTCGCNACCGTCGCCAACA?--\",\n",
    "    'sp5': \"TGCA-CGTCGCCAACAGTAGTCCTCCACCGTCGCCACCGTCGCCAACAGTG\",\n",
    "    'sp6': \"T?CAC-GTCGCCAACAGTAGTCCTCCACCGTCGCCACCGTCGCCAACA---\",\n",
    "} \n",
    "\n",
    "# Print MAFFT alignment\n",
    "print(\"Step 0. Align sequences statically\")\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "###########################\n",
    "# Step 1: Data collection #\n",
    "###########################\n",
    "\n",
    "def GB2MSA_1(input_file, output_prefix, delimiter=',', write_names=True):\n",
    "    \"\"\"\n",
    "    Downloads GenBank sequences based on accession numbers in a CSV/TSV file and aligns them by gene using \n",
    "    MAFFT. If two fragments of the same locus are concatenated with no overlap between them, the space\n",
    "    between them will be treaed as missing data (15 Ws will flank these blocks of missing data, which will\n",
    "    be used to track these regions and be replaced with question marks in GB2MSA_2).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_file : str\n",
    "        Path to the CSV or TSV input file. The first column should contain sequence names (sample identifiers).\n",
    "        The first row should contain gene names starting from the second column. Cells contain GenBank \n",
    "        accession numbers (one or more separated by slashes). \"NA\", empty cells, or dashes are ignored.\n",
    "\n",
    "    output_prefix : str\n",
    "        Prefix used for naming intermediate FASTA files and final aligned output files.\n",
    "\n",
    "    delimiter : str, optional (default=',')\n",
    "        Delimiter used in the input file (e.g., ',' for CSV or '\\t' for TSV).\n",
    "\n",
    "    write_names : bool, optional (default=True)\n",
    "        If True, writes a TXT file listing all sequence names (from the first column).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    aligned_files : list of str\n",
    "        List of file paths to the MAFFT-aligned FASTA files for each gene.\n",
    "    \"\"\"\n",
    "    # Open the input CSV/TSV file\n",
    "    with open(input_file, newline='') as file:\n",
    "        reader = csv.reader(file, delimiter=delimiter)\n",
    "        rows = list(reader)\n",
    "\n",
    "    # Replace spaces in sequence names with underscores\n",
    "    sequence_names = [row[0].replace(\" \", \"_\") for row in rows[1:]]\n",
    "    \n",
    "    # Extract gene names from the header row (excluding first column)\n",
    "    gene_names = rows[0][1:]\n",
    "    \n",
    "    # Extract gene accession data for each sequence (excluding first column)\n",
    "    gene_columns = [row[1:] for row in rows[1:]]\n",
    "\n",
    "    # If requested, write the sequence names to a text file\n",
    "    if write_names:\n",
    "        names_file = f\"{output_prefix}_sequence_names.txt\"\n",
    "        with open(names_file, 'w') as nf:\n",
    "            for name in sequence_names:\n",
    "                nf.write(f\"{name}\\n\")\n",
    "\n",
    "    aligned_files = []  # List to store paths of aligned output files\n",
    "\n",
    "    # Iterate over each gene (i.e., each column after the first)\n",
    "    for gene_idx, gene_name in enumerate(gene_names):\n",
    "        fasta_file = f\"{output_prefix}_{gene_name}.fasta\"  # Name of temporary FASTA file\n",
    "        \n",
    "        with open(fasta_file, 'w') as fasta_out:\n",
    "            # Iterate through each row (sample/sequence)\n",
    "            for i, seq_name in enumerate(sequence_names):\n",
    "                cell = gene_columns[i][gene_idx].strip()\n",
    "                \n",
    "                # Skip cells with missing data (\"NA\", empty, or dash)\n",
    "                if cell.upper() == \"NA\" or not cell or cell == \"-\":\n",
    "                    continue\n",
    "\n",
    "                # Split accession numbers by '/' and filter out invalid entries\n",
    "                accessions = [acc for acc in cell.split('/') if acc.upper() != \"NA\" and acc != \"\" and acc != \"-\"]\n",
    "                sequences = []  # To hold the sequences retrieved from GenBank\n",
    "\n",
    "                # Fetch each sequence from GenBank\n",
    "                for acc in accessions:\n",
    "                    try:\n",
    "                        handle = Entrez.efetch(db=\"nucleotide\", id=acc, rettype=\"fasta\", retmode=\"text\")\n",
    "                        seq_record = SeqIO.read(handle, \"fasta\")\n",
    "                        handle.close()\n",
    "                        sequences.append(str(seq_record.seq))  # Store the sequence string\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error fetching {acc}: {e}\")\n",
    "\n",
    "                # Combine multiple sequences with 'W' delimiters to mark junctions (to be handled later)\n",
    "                combined_seq = \"WWWWWWWWWWWWWWW\".join(sequences)\n",
    "                \n",
    "                # Write the sequence to the FASTA file with its name as header\n",
    "                fasta_out.write(f\">{seq_name}\\n{combined_seq}\\n\")\n",
    "\n",
    "        # Define the name for the output alignment file\n",
    "        aligned_file = f\"{output_prefix}_{gene_name}_aligned.fasta\"\n",
    "\n",
    "        # Run MAFFT on the generated FASTA file and save the alignment\n",
    "        with open(aligned_file, 'w') as aligned_out:\n",
    "            subprocess.run([\"mafft\", \"--auto\", fasta_file], stdout=aligned_out)\n",
    "\n",
    "        # Append the aligned file path to the result list\n",
    "        aligned_files.append(aligned_file)\n",
    "\n",
    "    return aligned_files  # Return list of aligned output file paths\n",
    "\n",
    "def GB2MSA_2(alignment_file):\n",
    "    \"\"\"\n",
    "    If internal missing data were identified by GB2MSA_1, 15 Ws flank the blocks of missing data. \n",
    "    For each sequence in the alignment, GB2MSA_2:\n",
    "    - Replaces internal blocks of 15 'w' or spaced 'w' (e.g., w-w-w) with question marks.\n",
    "    - Removes columns with only '?' or '-' in all rows.\n",
    "    - Replaces dash blocks flanked by a nucleotide and a question mark (e.g., A??--AAC) with \n",
    "    question marks. These dashes can be artifacts from spaced 'w' and are missing data.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    alignment_file : str\n",
    "        Path to the MAFFT-aligned FASTA file to be processed.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Path to the cleaned alignment file.\n",
    "    \"\"\"\n",
    "    alignment = list(SeqIO.parse(alignment_file, \"fasta\"))\n",
    "    updated_records = []\n",
    "\n",
    "    # Step 1: Replace exact block of 15 'w' or 'W' with 15 '?'\n",
    "    for record in alignment:\n",
    "        seq = str(record.seq)\n",
    "        seq_cleaned = seq.replace(\"w\" * 15, \"?\" * 15).replace(\"W\" * 15, \"?\" * 15)\n",
    "        record.seq = Seq(seq_cleaned)\n",
    "        updated_records.append(record)\n",
    "\n",
    "    # Step 2: Replace non-contiguous 'w' blocks (e.g., w-w-w) with '?'\n",
    "    for record in updated_records:\n",
    "        chars = list(str(record.seq))\n",
    "        i = 0\n",
    "        while i < len(chars):\n",
    "            if chars[i].lower() == 'w':\n",
    "                count = 1\n",
    "                indices = [i]\n",
    "                j = i + 1\n",
    "                while j < len(chars) and count < 15:\n",
    "                    if chars[j] == '-':\n",
    "                        indices.append(j)\n",
    "                    elif chars[j].lower() == 'w':\n",
    "                        indices.append(j)\n",
    "                        count += 1\n",
    "                    else:\n",
    "                        break\n",
    "                    j += 1\n",
    "                if count == 15:\n",
    "                    for idx in indices:\n",
    "                        chars[idx] = '?'\n",
    "                i = j\n",
    "            else:\n",
    "                i += 1\n",
    "        record.seq = Seq(''.join(chars))\n",
    "\n",
    "    # Step 3: Remove columns with only '?' or '-' in all rows\n",
    "    sequences = [list(str(record.seq)) for record in updated_records]\n",
    "    if len(set(len(seq) for seq in sequences)) > 1:\n",
    "        raise ValueError(\"Sequences are not of the same length!\")\n",
    "\n",
    "    valid_columns = []\n",
    "    for i in range(len(sequences[0])):\n",
    "        column = [seq[i] for seq in sequences]\n",
    "        if any(base not in ['?', '-'] for base in column):\n",
    "            valid_columns.append(i)\n",
    "\n",
    "    # Rebuild records with cleaned sequences\n",
    "    cleaned_records = []\n",
    "    for record in updated_records:\n",
    "        cleaned_seq = ''.join(str(record.seq)[i] for i in valid_columns)\n",
    "        record.seq = Seq(cleaned_seq)\n",
    "        cleaned_records.append(record)\n",
    "\n",
    "    # Step 4: Replace dash blocks flanked by nucleotide and '?' with '?'\n",
    "    for record in cleaned_records:\n",
    "        chars = list(str(record.seq))\n",
    "        i = 0\n",
    "        while i < len(chars):\n",
    "            if chars[i] == '-':\n",
    "                start = i\n",
    "                while i < len(chars) and chars[i] == '-':\n",
    "                    i += 1\n",
    "                end = i - 1\n",
    "\n",
    "                # Check flanking characters safely\n",
    "                left = chars[start - 1] if start > 0 else ''\n",
    "                right = chars[end + 1] if end + 1 < len(chars) else ''\n",
    "\n",
    "                if ((left in 'ACGTacgt' and right in '?Nn') or \n",
    "                    (left in '?Nn' and right in 'ACGTacgt')):\n",
    "                    for j in range(start, end + 1):\n",
    "                        chars[j] = '?'\n",
    "            else:\n",
    "                i += 1\n",
    "        record.seq = Seq(''.join(chars))\n",
    "\n",
    "    # Step 5: Write to cleaned output\n",
    "    cleaned_file = alignment_file.replace(\".fasta\", \"_GB2MSA.fasta\")\n",
    "    with open(cleaned_file, \"w\") as out_handle:\n",
    "        SeqIO.write(cleaned_records, out_handle, \"fasta\")\n",
    "\n",
    "    return cleaned_file\n",
    "\n",
    "def GB2MSA_3(alignment_dict, orphan_threshold=6, log=False):\n",
    "    \"\"\"\n",
    "    Replaces specific blocks in a DNA alignment with '?':\n",
    "    - Contiguous gap blocks (length >= orphan_threshold) that are adjacent\n",
    "      to contiguous nucleotide blocks (length < orphan_threshold), where the\n",
    "      other side of that nucleotide block touches a '?'\n",
    "\n",
    "    Parameters:\n",
    "    alignment_dict (dict): {sequence_name: aligned_sequence}\n",
    "    orphan_threshold (int): Minimum size to define a valid gap block\n",
    "    log (bool): If True, print the start and end positions of replaced blocks\n",
    "\n",
    "    Returns:\n",
    "    dict: Cleaned alignment with selective replacements\n",
    "    \"\"\"\n",
    "    cleaned_alignment = {}\n",
    "\n",
    "    for seq_name, original_seq in alignment_dict.items():\n",
    "        sequence = list(original_seq)\n",
    "        seq_len = len(sequence)\n",
    "        updated_seq = ''.join(sequence)\n",
    "\n",
    "        gap_matches = list(re.finditer(r'-+', updated_seq))\n",
    "        valid_gap_blocks = [(m.start(), m.end() - 1) for m in gap_matches\n",
    "                            if (m.end() - m.start()) >= orphan_threshold]\n",
    "\n",
    "        to_replace = set()\n",
    "\n",
    "        if log:\n",
    "            print(f\"Sequence: {seq_name}\")\n",
    "\n",
    "        for gap_start, gap_end in valid_gap_blocks:\n",
    "            replaced = False\n",
    "\n",
    "            # Check left nucleotide orphan\n",
    "            left_end = gap_start - 1\n",
    "            i = left_end\n",
    "            while i >= 0 and updated_seq[i] not in '-?':\n",
    "                i -= 1\n",
    "            left_start = i + 1\n",
    "            left_len = left_end - left_start + 1\n",
    "\n",
    "            if left_len > 0 and left_len < orphan_threshold and i >= 0 and updated_seq[i] == '?':\n",
    "                to_replace.update(range(left_start, left_end + 1))\n",
    "                to_replace.update(range(gap_start, gap_end + 1))\n",
    "                replaced = True\n",
    "                if log:\n",
    "                    print(f\"  Gap block: {gap_start}-{gap_end}\")\n",
    "                    print(f\"  Left orphan nucleotide block: {left_start}-{left_end}\")\n",
    "\n",
    "            if not replaced:\n",
    "                # Check right nucleotide orphan\n",
    "                right_start = gap_end + 1\n",
    "                i = right_start\n",
    "                while i < seq_len and updated_seq[i] not in '-?':\n",
    "                    i += 1\n",
    "                right_end = i - 1\n",
    "                right_len = right_end - right_start + 1\n",
    "\n",
    "                if right_len > 0 and right_len < orphan_threshold and i < seq_len and updated_seq[i] == '?':\n",
    "                    to_replace.update(range(right_start, right_end + 1))\n",
    "                    to_replace.update(range(gap_start, gap_end + 1))\n",
    "                    if log:\n",
    "                        print(f\"  Gap block: {gap_start}-{gap_end}\")\n",
    "                        print(f\"  Right orphan nucleotide block: {right_start}-{right_end}\")\n",
    "\n",
    "        # Replace in sequence\n",
    "        for i in to_replace:\n",
    "            sequence[i] = '?'\n",
    "\n",
    "        cleaned_alignment[seq_name] = ''.join(sequence)\n",
    "\n",
    "    return cleaned_alignment\n",
    "\n",
    "def GB2MSA_4(alignment_dict):\n",
    "    cleaned_alignment = {}\n",
    "\n",
    "    for seq_name, seq in alignment_dict.items():\n",
    "        sequence = list(seq)\n",
    "\n",
    "        # Use regex to find blocks with at least 15 characters of w/W/- combined\n",
    "        # but must contain at least 15 w or W\n",
    "        pattern = re.finditer(r'([wW\\-]{15,})', ''.join(sequence))\n",
    "\n",
    "        for match in pattern:\n",
    "            block = match.group()\n",
    "            start, end = match.start(), match.end()\n",
    "\n",
    "            # Count number of w/W in the block\n",
    "            w_count = sum(1 for c in block if c in 'wW')\n",
    "            if w_count >= 15:\n",
    "                for i in range(start, end):\n",
    "                    sequence[i] = '?'\n",
    "\n",
    "        cleaned_alignment[seq_name] = ''.join(sequence)\n",
    "\n",
    "    return cleaned_alignment\n",
    "\n",
    "def GB2MSA(input_file, \n",
    "           output_prefix, \n",
    "           delimiter=',', \n",
    "           write_names=False, \n",
    "           log=False, \n",
    "           orphan_threshold=6):\n",
    "    \"\"\"\n",
    "    Complete GenBank-to-MSA pipeline:\n",
    "    1. Downloads sequences from GenBank and aligns them by gene using MAFFT.\n",
    "    2. Cleans the alignments by replacing internal missing data and removing empty columns.\n",
    "    3. Applies GB2MSA_3 to replace selected gap and orphan nucleotide blocks with '?'.\n",
    "    4. Applies GB2MSA_4 to replace blocks of 15 or more w/W (with or without interspersed gaps) with '?'.\n",
    "    5. Replaces terminal '?' in sequences with '-'.\n",
    "    6. Deletes intermediate files ending with '_aligned.fasta'.\n",
    "    7. Optionally logs wall clock and CPU time to a log file named '<output_prefix>_log.txt'.\n",
    "    \"\"\"\n",
    "    start_wall = time.time()\n",
    "    start_cpu = time.process_time()\n",
    "\n",
    "    # Step 1: Generate aligned FASTA files\n",
    "    aligned_files = GB2MSA_1(input_file, output_prefix, delimiter=delimiter, write_names=write_names)\n",
    "\n",
    "    # Step 2: Clean each aligned FASTA file\n",
    "    cleaned_files = []\n",
    "    for aligned_file in aligned_files:\n",
    "        cleaned_file = GB2MSA_2(aligned_file)\n",
    "        cleaned_files.append(cleaned_file)\n",
    "\n",
    "    # Step 3: Apply GB2MSA_3 to clean orphan gap/nucleotide blocks\n",
    "    for cleaned_file in cleaned_files:\n",
    "        records = list(SeqIO.parse(cleaned_file, \"fasta\"))\n",
    "        alignment_dict = {record.id: str(record.seq) for record in records}\n",
    "        updated_dict = GB2MSA_3(alignment_dict, orphan_threshold=orphan_threshold, log=log)\n",
    "        \n",
    "        # Step 4: Apply GB2MSA_4 to handle w/W blocks\n",
    "        updated_dict = GB2MSA_4(updated_dict)\n",
    "\n",
    "        updated_records = []\n",
    "        for record in records:\n",
    "            record.seq = Seq(updated_dict[record.id])\n",
    "            updated_records.append(record)\n",
    "        with open(cleaned_file, \"w\") as out_handle:\n",
    "            SeqIO.write(updated_records, out_handle, \"fasta\")\n",
    "\n",
    "    # Step 5: Replace terminal '?' with '-' in each sequence\n",
    "    for cleaned_file in cleaned_files:\n",
    "        records = list(SeqIO.parse(cleaned_file, \"fasta\"))\n",
    "        updated_records = []\n",
    "        for record in records:\n",
    "            seq = str(record.seq)\n",
    "            left = len(seq) - len(seq.lstrip('?'))\n",
    "            right = len(seq) - len(seq.rstrip('?'))\n",
    "            new_seq = '-' * left + seq[left:len(seq)-right] + '-' * right if right > 0 else '-' * left + seq[left:]\n",
    "            record.seq = Seq(new_seq)\n",
    "            updated_records.append(record)\n",
    "        with open(cleaned_file, \"w\") as out_handle:\n",
    "            SeqIO.write(updated_records, out_handle, \"fasta\")\n",
    "\n",
    "    # Step 6: Delete intermediate *_aligned.fasta files\n",
    "    for aligned_file in aligned_files:\n",
    "        if aligned_file.endswith(\"_aligned.fasta\") and os.path.exists(aligned_file):\n",
    "            os.remove(aligned_file)\n",
    "\n",
    "    # Step 7: Log timing if requested\n",
    "    if log:\n",
    "        end_wall = time.time()\n",
    "        end_cpu = time.process_time()\n",
    "        wall_time = end_wall - start_wall\n",
    "        cpu_time = end_cpu - start_cpu\n",
    "\n",
    "        log_file = f\"{output_prefix}_log.txt\"\n",
    "        with open(log_file, \"a\") as lf:\n",
    "            lf.write(f\"--- GB2MSA run for '{output_prefix}' ---\\n\")\n",
    "            lf.write(f\"Wall clock time: {wall_time:.2f} seconds\\n\")\n",
    "            lf.write(f\"CPU time: {cpu_time:.2f} seconds\\n\\n\")\n",
    "\n",
    "    return cleaned_files\n",
    "\n",
    "def addSeq(\n",
    "    alignment,\n",
    "    new_seqs,\n",
    "    output,\n",
    "    write_names=True,\n",
    "    orphan_threshold=0,\n",
    "    log=False,\n",
    "    n2question=None,\n",
    "    gaps2question=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Add new sequences to an existing alignment using MAFFT, clean and standardize the result.\n",
    "\n",
    "    Steps performed:\n",
    "    1. Remove '#' columns from original alignment.\n",
    "    2. Align new sequences with MAFFT using --add.\n",
    "    3. Trim orphan nucleotide blocks from new sequences.\n",
    "    4. Remove short DNA blocks near # in first and last partitions.\n",
    "    5. Replace terminal '-' with '?'.\n",
    "    6. Optionally replace all N/n with '?' in selected sequences.\n",
    "    7. Optionally replace long gap blocks with '?'.\n",
    "    8. Reinsert '#' columns.\n",
    "    9. Replace all-'?' blocks between '#' with '-'.\n",
    "    10. Write the result and an optional log file.\n",
    "\n",
    "    Parameters:\n",
    "        alignment (str or dict): Existing alignment file path (FASTA) or dictionary {id: sequence}.\n",
    "        new_seqs (str or dict): New sequences to add (FASTA path or dict {id: sequence}).\n",
    "        output (str): Path to write the updated alignment in FASTA format.\n",
    "        write_names (bool): Whether to write a _terminal_names.txt file listing sequence IDs.\n",
    "        orphan_threshold (int): Threshold to detect and remove orphan DNA blocks.\n",
    "        log (bool): If True, writes a log file with trimming and runtime information.\n",
    "        n2question (str, list or None): Replace 'N/n' with '?' in specific sequences:\n",
    "            - 'all': apply to all sequences\n",
    "            - str: apply to a single sequence ID\n",
    "            - list: apply to listed sequence IDs\n",
    "        gaps2question (int or None): Replace contiguous gap blocks larger than this threshold with '?'. Only applied to added sequences.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Example usage:\n",
    "        addSeq(\"alignment.fasta\", \"new.fasta\", \"updated.fasta\", n2question=\"seq123\", log=True)\n",
    "        addSeq(alignment_dict, new_dict, \"out.fas\", n2question='all')\n",
    "    \"\"\"\n",
    "\n",
    "    # Start timing the execution\n",
    "    start_time = time.time()\n",
    "    temp_files_to_remove = []  # Temporary files to be removed after execution\n",
    "    log_lines = [] \n",
    "\n",
    "    # Log the function call and parameters for reproducibility\n",
    "    if log:\n",
    "        cmd_used = f\"addSeq(alignment=..., new_seqs=..., output='{output}', write_names={write_names}, orphan_threshold={orphan_threshold}, log={log}, n2question={n2question}, gaps2question={gaps2question})\"\n",
    "        log_lines.append(f\"Command used: {cmd_used}\")\n",
    "        log_lines.append(\"\")\n",
    "\n",
    "    # === Step 1: Load and clean the alignment ===\n",
    "    def write_dict_to_temp_fasta(seq_dict):\n",
    "        records = [SeqRecord(Seq(seq), id=str(seq_id), description=\"\") for seq_id, seq in seq_dict.items()]\n",
    "        tmp = tempfile.NamedTemporaryFile(\"w+\", delete=False)\n",
    "        SeqIO.write(records, tmp, \"fasta\")\n",
    "        tmp.close()\n",
    "        return tmp.name\n",
    "\n",
    "    if isinstance(alignment, dict):\n",
    "        alignment_path = write_dict_to_temp_fasta(alignment)\n",
    "        temp_files_to_remove.append(alignment_path)\n",
    "    elif isinstance(alignment, str):\n",
    "        alignment_path = alignment\n",
    "    else:\n",
    "        raise ValueError(\"alignment must be a FASTA file path or a dictionary\")\n",
    "\n",
    "    records = list(SeqIO.parse(alignment_path, \"fasta\"))\n",
    "    if not records:\n",
    "        raise ValueError(\"Input alignment is empty or not found\")\n",
    "\n",
    "    aln_len = len(records[0].seq)\n",
    "    # Identify columns that are '#' characters to temporarily remove them for alignment\n",
    "    pound_cols = [i for i in range(aln_len) if any(rec.seq[i] == '#' for rec in records)]\n",
    "\n",
    "    # Log input alignment info\n",
    "    if log:\n",
    "        log_lines.append(f\"Input alignment: {len(records)} sequences\")\n",
    "        log_lines.append(f\"Input alignment: {len(pound_cols)} # columns\")\n",
    "\n",
    "    def remove_cols(seq, cols):\n",
    "        return ''.join(seq[i] for i in range(len(seq)) if i not in cols)\n",
    "\n",
    "    # Remove '#' columns\n",
    "    aln_no_pound = [\n",
    "        SeqRecord(Seq(remove_cols(str(rec.seq), pound_cols)), id=rec.id, description=\"\")\n",
    "        for rec in records\n",
    "    ]\n",
    "    # Write cleaned alignment to a temporary file\n",
    "    with tempfile.NamedTemporaryFile(\"w+\", delete=False) as aln_tmp:\n",
    "        SeqIO.write(aln_no_pound, aln_tmp, \"fasta\")\n",
    "        aln_path = aln_tmp.name\n",
    "        temp_files_to_remove.append(aln_path)\n",
    "\n",
    "    # === Step 2: Load new sequences ===\n",
    "    if isinstance(new_seqs, dict):\n",
    "        new_seqs_path = write_dict_to_temp_fasta(new_seqs)\n",
    "        new_seq_count = len(new_seqs)\n",
    "        temp_files_to_remove.append(new_seqs_path)\n",
    "    elif isinstance(new_seqs, str):\n",
    "        new_seq_count = sum(1 for _ in SeqIO.parse(new_seqs, \"fasta\"))\n",
    "        new_seqs_path = new_seqs\n",
    "    else:\n",
    "        raise ValueError(\"new_seqs must be a FASTA file path or a dictionary\")\n",
    "    # Log new sequence info\n",
    "    if log:\n",
    "        log_lines.append(f\"Input new sequences: {new_seq_count} sequences\")\n",
    "\n",
    "    # === Step 3: Align new sequences with MAFFT ===\n",
    "    with tempfile.NamedTemporaryFile(\"w+\", delete=False) as out_tmp:\n",
    "        out_path = out_tmp.name\n",
    "        temp_files_to_remove.append(out_path)\n",
    "\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            ['mafft', '--add', new_seqs_path, '--keeplength', '--preservecase', aln_path],\n",
    "            check=True,\n",
    "            stdout=open(out_path, 'w'),\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "        )\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        for file in temp_files_to_remove:\n",
    "            try:\n",
    "                os.remove(file)\n",
    "            except Exception:\n",
    "                pass\n",
    "        raise RuntimeError(f\"MAFFT failed!\\nCommand: {e.cmd}\\nExit status: {e.returncode}\\nMAFFT error output:\\n{e.stderr}\")\n",
    "\n",
    "    mafft_aligned_records = list(SeqIO.parse(out_path, \"fasta\"))\n",
    "    original_ids = {rec.id for rec in records}\n",
    "    new_records = [rec for rec in mafft_aligned_records if rec.id not in original_ids]\n",
    "\n",
    "\n",
    "    def replace_gap_blocks(seq, threshold, seq_id=None):\n",
    "        seq_list = list(seq)\n",
    "        replaced_log = []\n",
    "        i = 0\n",
    "        while i < len(seq_list):\n",
    "            if seq_list[i] == '-':\n",
    "                start = i\n",
    "                while i < len(seq_list) and seq_list[i] == '-':\n",
    "                    i += 1\n",
    "                if (i - start) > threshold:\n",
    "                    for j in range(start, i):\n",
    "                        seq_list[j] = '?'\n",
    "                    if seq_id:\n",
    "                        replaced_log.append(f\"{seq_id}: {i - start} contiguous '-' replaced with '?' at {start}â€“{i}\")\n",
    "            else:\n",
    "                i += 1\n",
    "        return ''.join(seq_list), replaced_log\n",
    "\n",
    "    def find_dna_blocks(seq_list, start, end):\n",
    "        blocks = []\n",
    "        i = start\n",
    "        while i < end:\n",
    "            if seq_list[i] not in \"-?#\":\n",
    "                s = i\n",
    "                while i < end and seq_list[i] not in \"-?#\":\n",
    "                    i += 1\n",
    "                e = i\n",
    "                blocks.append((s, e))\n",
    "            else:\n",
    "                i += 1\n",
    "        return blocks\n",
    "\n",
    "    def trim_orphan_blocks(seq, threshold, seq_id=None):\n",
    "        seq_list = list(seq)\n",
    "        trimmed_log = []\n",
    "\n",
    "        def find_blocks(seq_list):\n",
    "            blocks = []\n",
    "            i = 0\n",
    "            while i < len(seq_list):\n",
    "                if seq_list[i] not in \"-?#\":\n",
    "                    start = i\n",
    "                    while i < len(seq_list) and seq_list[i] not in \"-?#\":\n",
    "                        i += 1\n",
    "                    end = i\n",
    "                    blocks.append((start, end))\n",
    "                else:\n",
    "                    i += 1\n",
    "            return blocks\n",
    "\n",
    "        changed = True\n",
    "        while changed:\n",
    "            changed = False\n",
    "\n",
    "            while True:\n",
    "                blocks = find_blocks(seq_list)\n",
    "                if len(blocks) < 2:\n",
    "                    break\n",
    "                first_start, first_end = blocks[0]\n",
    "                next_start = blocks[1][0]\n",
    "                gap_count = seq_list[first_end:next_start].count('-') + seq_list[first_end:next_start].count('?')\n",
    "                size = first_end - first_start\n",
    "                if size < threshold and gap_count > threshold:\n",
    "                    deleted = ''.join(seq_list[first_start:first_end])\n",
    "                    seq_list[first_start:first_end] = ['-'] * size\n",
    "                    if seq_id:\n",
    "                        trimmed_log.append(f\"{seq_id}: Left {first_start}â€“{first_end} (size={size}, '{deleted}')\")\n",
    "                    changed = True\n",
    "                    continue\n",
    "                break\n",
    "\n",
    "            while True:\n",
    "                blocks = find_blocks(seq_list)\n",
    "                if len(blocks) < 2:\n",
    "                    break\n",
    "                last_start, last_end = blocks[-1]\n",
    "                prev_end = blocks[-2][1]\n",
    "                gap_count = seq_list[prev_end:last_start].count('-') + seq_list[prev_end:last_start].count('?')\n",
    "                size = last_end - last_start\n",
    "                if size < threshold and gap_count > threshold:\n",
    "                    deleted = ''.join(seq_list[last_start:last_end])\n",
    "                    seq_list[last_start:last_end] = ['-'] * size\n",
    "                    if seq_id:\n",
    "                        trimmed_log.append(f\"{seq_id}: Right {last_start}â€“{last_end} (size={size}, '{deleted}')\")\n",
    "                    changed = True\n",
    "                    continue\n",
    "                break\n",
    "\n",
    "        if pound_cols:\n",
    "            first_hash = min(pound_cols)\n",
    "            last_hash = max(pound_cols)\n",
    "            blocks = find_dna_blocks(seq_list, 0, first_hash)\n",
    "            if len(blocks) == 1:\n",
    "                s, e = blocks[0]\n",
    "                if e == first_hash and (e - s) < threshold:\n",
    "                    deleted = ''.join(seq_list[s:e])\n",
    "                    seq_list[s:e] = ['-'] * (e - s)\n",
    "                    if seq_id:\n",
    "                        trimmed_log.append(f\"{seq_id}: Left {s}â€“{e} (size={e - s}, '{deleted}')\")\n",
    "            blocks = find_dna_blocks(seq_list, last_hash + 1, len(seq_list))\n",
    "            if len(blocks) == 1:\n",
    "                s, e = blocks[0]\n",
    "                if s == last_hash + 1 and (e - s) < threshold:\n",
    "                    deleted = ''.join(seq_list[s:e])\n",
    "                    seq_list[s:e] = ['-'] * (e - s)\n",
    "                    if seq_id:\n",
    "                        trimmed_log.append(f\"{seq_id}: Right {s}â€“{e} (size={e - s}, '{deleted}')\")\n",
    "\n",
    "        return ''.join(seq_list), trimmed_log\n",
    "\n",
    "    trimmed_new_records = []\n",
    "    all_trim_logs = []\n",
    "    gaps2q_log = []  # This will no longer be used for logging replaced gaps\n",
    "\n",
    "    for rec in new_records:\n",
    "        trimmed_seq, seq_log = trim_orphan_blocks(str(rec.seq), orphan_threshold, seq_id=rec.id)\n",
    "        # Remove gaps2question here to not log replaced gaps multiple times\n",
    "        trimmed_new_records.append(SeqRecord(Seq(trimmed_seq), id=rec.id, description=\"\"))\n",
    "        all_trim_logs.extend(seq_log)\n",
    "\n",
    "    processed_records = [rec for rec in mafft_aligned_records if rec.id in original_ids] + trimmed_new_records\n",
    "\n",
    "    updated_records = []\n",
    "    for rec in processed_records:\n",
    "        seq_chars = list(str(rec.seq))\n",
    "        for i in range(len(seq_chars)):\n",
    "            if seq_chars[i] == '-':\n",
    "                seq_chars[i] = '?'\n",
    "            else:\n",
    "                break\n",
    "        for i in range(len(seq_chars) - 1, -1, -1):\n",
    "            if seq_chars[i] == '-':\n",
    "                seq_chars[i] = '?'\n",
    "            else:\n",
    "                break\n",
    "        updated_records.append(SeqRecord(Seq(''.join(seq_chars)), id=rec.id, description=\"\"))\n",
    "\n",
    "    n2q_log = []\n",
    "    if n2question:\n",
    "        if isinstance(n2question, str) and n2question != 'all':\n",
    "            target_ids = {n2question}\n",
    "        elif isinstance(n2question, list):\n",
    "            target_ids = set(n2question)\n",
    "        elif n2question == 'all':\n",
    "            target_ids = {rec.id for rec in updated_records}\n",
    "        else:\n",
    "            target_ids = set()\n",
    "\n",
    "        for rec in updated_records:\n",
    "            if rec.id in target_ids:\n",
    "                seq_str = str(rec.seq)\n",
    "                count_n = seq_str.count('N') + seq_str.count('n')\n",
    "                if count_n > 0:\n",
    "                    rec.seq = Seq(seq_str.replace('N', '?').replace('n', '?'))\n",
    "                    n2q_log.append(f\"{rec.id}: {count_n} N/n replaced with ?\")\n",
    "\n",
    "    # === Now apply gaps2question as the very last step on added sequences ===\n",
    "    def replace_gap_blocks(seq, threshold, seq_id=None):\n",
    "        seq_list = list(seq)\n",
    "        replaced_log = []\n",
    "        i = 0\n",
    "        while i < len(seq_list):\n",
    "            if seq_list[i] == '-':\n",
    "                start = i\n",
    "                while i < len(seq_list) and seq_list[i] == '-':\n",
    "                    i += 1\n",
    "                if (i - start) > threshold:\n",
    "                    for j in range(start, i):\n",
    "                        seq_list[j] = '?'\n",
    "                    if seq_id:\n",
    "                        replaced_log.append(f\"{seq_id}: {i - start} contiguous '-' replaced with '?' at {start}â€“{i}\")\n",
    "            else:\n",
    "                i += 1\n",
    "        return ''.join(seq_list), replaced_log\n",
    "\n",
    "    # Apply gaps2question only if specified\n",
    "    if gaps2question:\n",
    "        updated_records_dict = {rec.id: rec for rec in updated_records}\n",
    "        for rec in trimmed_new_records:\n",
    "            seq = str(updated_records_dict[rec.id].seq)\n",
    "            new_seq, _ = replace_gap_blocks(seq, gaps2question, seq_id=rec.id)\n",
    "            updated_records_dict[rec.id].seq = Seq(new_seq)\n",
    "        updated_records = list(updated_records_dict.values())\n",
    "\n",
    "    final_records = []\n",
    "    for rec in updated_records:\n",
    "        seq_list = list(str(rec.seq))\n",
    "        for col in sorted(pound_cols):\n",
    "            seq_list.insert(col, '#')\n",
    "        final_records.append(SeqRecord(Seq(''.join(seq_list)), id=rec.id, description=\"\"))\n",
    "\n",
    "    def process_blocks(seq):\n",
    "        seq_chars = list(seq)\n",
    "        blocks = []\n",
    "        start = 0\n",
    "        for i, c in enumerate(seq_chars):\n",
    "            if c == '#':\n",
    "                blocks.append((start, i))\n",
    "                start = i + 1\n",
    "        blocks.append((start, len(seq_chars)))\n",
    "        for (start, end) in blocks:\n",
    "            if all(seq_chars[i] == '?' for i in range(start, end)):\n",
    "                for i in range(start, end):\n",
    "                    seq_chars[i] = '-'\n",
    "        return ''.join(seq_chars)\n",
    "\n",
    "    final_output = [\n",
    "        SeqRecord(Seq(process_blocks(str(rec.seq))), id=rec.id, description=\"\")\n",
    "        for rec in final_records\n",
    "    ]\n",
    "\n",
    "    # --- New function to find contiguous '?' blocks ---\n",
    "    def find_question_blocks(seq):\n",
    "        blocks = []\n",
    "        seq_len = len(seq)\n",
    "        i = 0\n",
    "        while i < seq_len:\n",
    "            if seq[i] == '?':\n",
    "                start = i\n",
    "                while i < seq_len and seq[i] == '?':\n",
    "                    i += 1\n",
    "                end = i\n",
    "                blocks.append((start, end))\n",
    "            else:\n",
    "                i += 1\n",
    "        return blocks\n",
    "\n",
    "    # Identify gap ('?') blocks in added sequences for final logging\n",
    "    gap_question_blocks_log = []\n",
    "    added_ids = {rec.id for rec in trimmed_new_records}  # IDs of added sequences\n",
    "\n",
    "    for rec in final_output:\n",
    "        if rec.id in added_ids:\n",
    "            q_blocks = find_question_blocks(str(rec.seq))\n",
    "            for (start, end) in q_blocks:\n",
    "                length = end - start\n",
    "                gap_question_blocks_log.append(f\"{rec.id}: ? block at positions {start}-{end} (length={length})\")\n",
    "\n",
    "    SeqIO.write(final_output, output, \"fasta\")\n",
    "    if write_names:\n",
    "        with open(output + \"_terminal_names.txt\", \"w\") as f:\n",
    "            for rec in final_output:\n",
    "                f.write(rec.id + \"\\n\")\n",
    "\n",
    "    if log:\n",
    "        elapsed = time.time() - start_time\n",
    "        log_lines.append(f\"Final alignment: {len(final_output)} sequences\")\n",
    "        log_lines.append(f\"Final alignment: {len(final_output[0].seq)} columns\")\n",
    "        log_lines.append(f\"Final alignment: {sum(1 for i in range(len(final_output[0].seq)) if any(rec.seq[i] == '#' for rec in final_output))} # columns\")\n",
    "        log_lines.append(\"\")\n",
    "        log_lines.append(\"Trimmed orphan blocks from new sequences:\")\n",
    "        log_lines.extend(all_trim_logs or [\"None\"])\n",
    "        if gap_question_blocks_log:\n",
    "            log_lines.append(\"\")\n",
    "            log_lines.append(\"Gap block replacements:\")\n",
    "            log_lines.extend(gap_question_blocks_log)\n",
    "        if n2q_log:\n",
    "            log_lines.append(\"\")\n",
    "            log_lines.append(\"N/n to ? replacements:\")\n",
    "            log_lines.extend(n2q_log)\n",
    "        log_lines.append(\"\")\n",
    "        log_lines.append(f\"Runtime: {elapsed:.2f} seconds\")\n",
    "        with open(output + \".log\", \"w\") as log_file:\n",
    "            log_file.write(\"\\n\".join(log_lines))\n",
    "\n",
    "    for file in temp_files_to_remove:\n",
    "        try:\n",
    "            os.remove(file)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Step 1.1: Delete gap artifacts from Mafft #\n",
    "#############################################\n",
    "\n",
    "def remove_all_gap_columns(alignment):\n",
    "    \"\"\"\n",
    "    Remove columns from the alignment where all terminal sequences have a gap ('-').\n",
    "    \n",
    "    Parameters:\n",
    "        alignment (dict): A dictionary where keys are sequence names and values are sequence strings.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Updated alignment with columns removed where all terminal sequences have a gap.\n",
    "    \"\"\"\n",
    "    # Convert the alignment to a list of sequences\n",
    "    sequences = list(alignment.values())\n",
    "    num_sequences = len(sequences)\n",
    "    seq_length = len(sequences[0])  # Assuming all sequences are the same length\n",
    "\n",
    "    # Identify columns that need to be removed (where all terminal sequences have a gap)\n",
    "    columns_to_remove = []\n",
    "    for col in range(seq_length):\n",
    "        # Check if all terminal sequences (sp1, sp2, ...) have a gap ('-') in this column\n",
    "        if all(seq[col] == '-' for seq in sequences):\n",
    "            columns_to_remove.append(col)\n",
    "    \n",
    "    # Remove the identified columns from each sequence\n",
    "    for seq_name, seq in alignment.items():\n",
    "        # Create a new sequence with the columns removed\n",
    "        new_seq = ''.join(seq[col] for col in range(seq_length) if col not in columns_to_remove)\n",
    "        alignment[seq_name] = new_seq\n",
    "    \n",
    "    return alignment\n",
    "\n",
    "# Print\n",
    "remove_all_gap_columns(alignment)\n",
    "print(\"\\n\") # Jump a line\n",
    "print(\"Step 1.1 Delete columns presenting gaps in all taxa (artifacts from MAFFT)\")\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "###################################################\n",
    "# Step 1.2: Delete orphan nucleotides iteratively #\n",
    "###################################################\n",
    "\n",
    "def calculate_orphan_threshold_from_percentile(alignment, percentile=25, log=False, terminal_only=True):\n",
    "    \"\"\"\n",
    "    Calculate orphan threshold based on a specific percentile of gap lengths in the alignment.\n",
    "    \n",
    "    Args:\n",
    "        alignment (dict): A dictionary with sequence IDs as keys and sequences as values.\n",
    "        percentile (float): The percentile to use for setting the orphan threshold (e.g., 75 for the 75th percentile).\n",
    "        log (bool): If True, print the list of gap lengths and the computed orphan threshold.\n",
    "        terminal_only (bool): If True, only consider gap blocks at the terminal positions (start and end of sequences).\n",
    "        \n",
    "    Returns:\n",
    "        int: Calculated orphan threshold based on the specified percentile.\n",
    "    \"\"\"\n",
    "    gap_lengths = []\n",
    "\n",
    "    # Loop through each sequence to calculate gap lengths\n",
    "    for sequence in alignment.values():\n",
    "        sequence_length = len(sequence)\n",
    "        gap_count = 0\n",
    "        in_gap = False\n",
    "\n",
    "        # If terminal_only is True, we'll check only the first and last blocks of gaps\n",
    "        if terminal_only:\n",
    "            # Check for gaps starting at the beginning of the sequence\n",
    "            if sequence[0] == '-':\n",
    "                gap_count = 1\n",
    "                in_gap = True\n",
    "                for nucleotide in sequence[1:]:\n",
    "                    if nucleotide == '-':\n",
    "                        gap_count += 1\n",
    "                    else:\n",
    "                        break  # End of the first terminal gap block\n",
    "                gap_lengths.append(gap_count)  # Add the terminal gap block at the start\n",
    "                \n",
    "            # Check for gaps starting at the end of the sequence\n",
    "            gap_count = 0\n",
    "            in_gap = False\n",
    "            if sequence[-1] == '-':\n",
    "                gap_count = 1\n",
    "                in_gap = True\n",
    "                for nucleotide in reversed(sequence[:-1]):\n",
    "                    if nucleotide == '-':\n",
    "                        gap_count += 1\n",
    "                    else:\n",
    "                        break  # End of the last terminal gap block\n",
    "                gap_lengths.append(gap_count)  # Add the terminal gap block at the end\n",
    "        else:\n",
    "            # Loop through the sequence to find contiguous blocks of gaps (not restricted to terminals)\n",
    "            for nucleotide in sequence:\n",
    "                if nucleotide == '-':\n",
    "                    if not in_gap:\n",
    "                        in_gap = True  # Start of a new gap block\n",
    "                        gap_count = 1  # Start counting the length of the new gap block\n",
    "                    else:\n",
    "                        gap_count += 1  # Continue counting the gap block length\n",
    "                else:\n",
    "                    if in_gap:\n",
    "                        gap_lengths.append(gap_count)  # End of a gap block, save its length\n",
    "                        in_gap = False\n",
    "                        gap_count = 0  # Reset the gap count for the next block\n",
    "\n",
    "            # If the sequence ends with a gap block, make sure to add the last block's length\n",
    "            if in_gap:\n",
    "                gap_lengths.append(gap_count)\n",
    "        \n",
    "    # Calculate the specified percentile of gap lengths\n",
    "    orphan_threshold = int(np.percentile(gap_lengths, percentile))  # Use the given percentile (e.g., 75th percentile)\n",
    "    \n",
    "    # Print the list of gap lengths\n",
    "    if log:\n",
    "        print(\"List of gap lengths:\", gap_lengths)\n",
    "        print(\"Orphan threshold:\", orphan_threshold)\n",
    "        \n",
    "    return orphan_threshold\n",
    "    \n",
    "def delete_orphan_nucleotides2(alignment, orphan_threshold, log_changes=False):\n",
    "    \"\"\"\n",
    "    Iteratively eliminates orphan nucleotide blocks from the start and end of each sequence.\n",
    "    An orphan block is a short contiguous run of nucleotides near the terminal ends separated by many gaps.\n",
    "\n",
    "    Args:\n",
    "        alignment (dict): {sequence_id: sequence_string}\n",
    "        orphan_threshold (int): Max block length and max gap tolerance.\n",
    "        log_changes (bool): Whether to return a log of changes made.\n",
    "\n",
    "    Returns:\n",
    "        dict: Cleaned alignment.\n",
    "        str (optional): Log of changes made.\n",
    "    \"\"\"\n",
    "    change_log = []\n",
    "\n",
    "    def find_blocks(seq):\n",
    "        \"\"\"Find contiguous non-gap blocks as (start, end) tuples.\"\"\"\n",
    "        blocks = []\n",
    "        i = 0\n",
    "        while i < len(seq):\n",
    "            if seq[i] != '-':\n",
    "                start = i\n",
    "                while i < len(seq) and seq[i] != '-':\n",
    "                    i += 1\n",
    "                end = i\n",
    "                blocks.append((start, end))\n",
    "            else:\n",
    "                i += 1\n",
    "        return blocks\n",
    "\n",
    "    alignment_changed = True\n",
    "    while alignment_changed:\n",
    "        alignment_changed = False\n",
    "\n",
    "        for seq_id, sequence in alignment.items():\n",
    "            seq_list = list(sequence)\n",
    "            changed = False\n",
    "\n",
    "            # Iteratively check from left side\n",
    "            while True:\n",
    "                blocks = find_blocks(seq_list)\n",
    "                if len(blocks) < 2:\n",
    "                    break\n",
    "                first_start, first_end = blocks[0]\n",
    "                next_start = blocks[1][0]\n",
    "                gap_count = seq_list[first_end:next_start].count('-')\n",
    "\n",
    "                if (first_end - first_start < orphan_threshold) and (gap_count > orphan_threshold):\n",
    "                    deleted = ''.join(seq_list[first_start:first_end])\n",
    "                    seq_list[first_start:first_end] = ['-'] * (first_end - first_start)\n",
    "                    changed = True\n",
    "                    if log_changes:\n",
    "                        change_log.append(\n",
    "                            f\"{seq_id}: Left block {first_start}-{first_end} deleted ('{deleted}')\"\n",
    "                        )\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            # Iteratively check from right side\n",
    "            while True:\n",
    "                blocks = find_blocks(seq_list)\n",
    "                if len(blocks) < 2:\n",
    "                    break\n",
    "                last_start, last_end = blocks[-1]\n",
    "                prev_end = blocks[-2][1]\n",
    "                gap_count = seq_list[prev_end:last_start].count('-')\n",
    "\n",
    "                if (last_end - last_start < orphan_threshold) and (gap_count > orphan_threshold):\n",
    "                    deleted = ''.join(seq_list[last_start:last_end])\n",
    "                    seq_list[last_start:last_end] = ['-'] * (last_end - last_start)\n",
    "                    changed = True\n",
    "                    if log_changes:\n",
    "                        change_log.append(\n",
    "                            f\"{seq_id}: Right block {last_start}-{last_end} deleted ('{deleted}')\"\n",
    "                        )\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            if changed:\n",
    "                alignment_changed = True\n",
    "                alignment[seq_id] = ''.join(seq_list)\n",
    "\n",
    "    if log_changes:\n",
    "        return alignment, \"\\n\".join(change_log)\n",
    "    else:\n",
    "        return alignment\n",
    "\n",
    "print('\\n')\n",
    "print(\"Step 1.2. Delete terminal orphan nucleotides (given a threshold)\")\n",
    "print_colored_alignment(delete_orphan_nucleotides2(alignment, orphan_threshold=5))\n",
    "\n",
    "#########################################################\n",
    "# Step 2.1: Classify terminal (?) and internal gaps (-) #\n",
    "#########################################################\n",
    "\n",
    "def replace_terminal_gaps_dict(alignment):\n",
    "    \"\"\"\n",
    "    Replaces terminal gaps ('-') with '?' in a sequence alignment stored as a dictionary,\n",
    "    while keeping internal gaps as '-'.\n",
    "\n",
    "    Parameters:\n",
    "        alignment (dict): Dictionary with sequence names as keys and sequences as values.\n",
    "\n",
    "    Returns:\n",
    "        dict: Modified alignment with terminal gaps replaced by '?'.\n",
    "    \"\"\"\n",
    "    # Rename 'modified_alignment' to 'alignment'\n",
    "    for name, sequence in alignment.items():\n",
    "        # Find the first and last non-gap characters\n",
    "        first_non_gap = next((i for i, char in enumerate(sequence) if char != \"-\"), None)\n",
    "        last_non_gap = next((i for i, char in enumerate(reversed(sequence), 1) if char != \"-\"), None)\n",
    "        \n",
    "        if first_non_gap is not None and last_non_gap is not None:\n",
    "            last_non_gap = len(sequence) - last_non_gap  # Adjust reversed index\n",
    "            \n",
    "            # Replace terminal gaps with '?' and keep internal gaps as '-'\n",
    "            new_sequence = (\n",
    "                \"?\" * first_non_gap +\n",
    "                sequence[first_non_gap:last_non_gap + 1] +\n",
    "                \"?\" * (len(sequence) - last_non_gap - 1)\n",
    "            )\n",
    "            alignment[name] = new_sequence\n",
    "        else:\n",
    "            # Handle sequences with only gaps\n",
    "            alignment[name] = \"?\" * len(sequence)\n",
    "    \n",
    "    return alignment\n",
    "\n",
    "replace_terminal_gaps_dict(alignment)\n",
    "print(\"\\n\") # Jump a line\n",
    "print(\"Step 2.1 Classify terminal (?) and internal gaps (-)\")\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "##################################################\n",
    "# Step 1.3: Trim terminal invariants iteratively #\n",
    "##################################################\n",
    "def is_parsimony_non_informative(column):\n",
    "    \"\"\"\n",
    "    Check if a column is invariant.\n",
    "    A column is non-informative if:\n",
    "    - All characters are the same.\n",
    "    - The characters include '?' and only one other character (e.g., 'A' and '?').\n",
    "    \n",
    "    Parameters:\n",
    "        column (list): A list of characters in a column (e.g., ['A', 'A', 'A', '?']).\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the column is non-informative, False if informative.\n",
    "    \"\"\"\n",
    "    unique_characters = set(column)\n",
    "        \n",
    "    # Case 1: Check if the column has only one unique character (parsimony non-informative)\n",
    "    if len(unique_characters) == 1:\n",
    "        return True\n",
    "    \n",
    "    # Case 2: Check if the column contains '?' and exactly one other unique character\n",
    "    if '?' in unique_characters and len(unique_characters) == 2:\n",
    "        return True\n",
    "\n",
    "    # Otherwise, the column is informative (multiple unique characters without '?')\n",
    "    return False\n",
    "\n",
    "def remove_non_informative_positions(alignment, removed_indices=None):\n",
    "    \"\"\"\n",
    "    Remove parsimony non-informative positions from both the start and the end of the alignment.\n",
    "    \n",
    "    Parameters:\n",
    "        alignment (dict): Dictionary where keys are sequence names and values are sequences (strings).\n",
    "        removed_indices (list, optional): If provided, the function will append the indices of removed columns.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Updated alignment with non-informative positions removed.\n",
    "    \"\"\"\n",
    "    # Convert the alignment to a list of sequences\n",
    "    sequences = list(alignment.values())\n",
    "    seq_length = len(sequences[0])\n",
    "\n",
    "    # Remove non-informative positions from the start\n",
    "    first_position = 0\n",
    "    while first_position < seq_length and is_parsimony_non_informative([seq[first_position] for seq in sequences]):\n",
    "        first_position += 1\n",
    "\n",
    "    # Remove non-informative positions from the end\n",
    "    last_position = seq_length - 1\n",
    "    while last_position >= first_position and is_parsimony_non_informative([seq[last_position] for seq in sequences]):\n",
    "        last_position -= 1\n",
    "\n",
    "    # If logging, record the indices of removed columns\n",
    "    if removed_indices is not None:\n",
    "        removed_start = list(range(0, first_position))\n",
    "        removed_end = list(range(last_position + 1, seq_length))\n",
    "        removed_indices.extend(removed_start + removed_end)\n",
    "\n",
    "    # Build the updated alignment\n",
    "    for seq_name, seq in alignment.items():\n",
    "        alignment[seq_name] = seq[first_position:last_position + 1]\n",
    "\n",
    "    return alignment\n",
    "\n",
    "# Print\n",
    "remove_non_informative_positions(alignment)\n",
    "print(\"\\n\") # Jump a line\n",
    "print(\"Step 1.3. Trim parsimony non-informative characters in terminal position\")\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "##################################\n",
    "# Step 2.2 Internal missing data #\n",
    "##################################\n",
    "\n",
    "def replace_dashes_with_question_marks(alignment, internal_column_ranges=None, internal_leaves=\"all\", internal_method=\"manual\", internal_threshold=None):\n",
    "    \"\"\"\n",
    "    Replace dashes with question marks in the specified column ranges for each sequence in the alignment.\n",
    "    \n",
    "    Args:\n",
    "        alignment (dict): A dictionary with sequence IDs as keys and sequences as values.\n",
    "        internal_column_ranges (list of tuples, optional): A list of tuples where each tuple defines a range of columns \n",
    "                                                          (inclusive) to check for dashes. E.g., [(5, 10), (50, 60)].\n",
    "        internal_leaves (str or list, optional): If \"all\", replace dashes in all sequences. If a list of sequence IDs \n",
    "                                                   is provided, replace dashes in those sequences only.\n",
    "        internal_method (str, optional): Defines how to replace dashes.\n",
    "            - \"manual\": Specify column ranges and terminal sequences to replace dashes.\n",
    "            - \"semi\": Replace internal blocks of contiguous dashes larger than the threshold with question marks.\n",
    "        internal_threshold (int, optional): The threshold for \"semi\" method. Only internal blocks of contiguous gaps \n",
    "                                             larger than this threshold are replaced with question marks.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Updated alignment with dashes replaced by question marks in the specified columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If internal_leaves is a list, only consider those sequences\n",
    "    if internal_leaves != \"all\":\n",
    "        sequences_to_process = set(internal_leaves)\n",
    "    else:\n",
    "        sequences_to_process = set(alignment.keys())\n",
    "    \n",
    "    # Convert the alignment into a list of sequences for easier indexing\n",
    "    alignment = {seq_id: list(seq) for seq_id, seq in alignment.items()}  # Convert sequences to lists for mutability\n",
    "\n",
    "    if internal_method == \"manual\":\n",
    "        # Replace dashes with question marks in the specified column ranges\n",
    "        for seq_id, seq in alignment.items():\n",
    "            if seq_id not in sequences_to_process:\n",
    "                continue  # Skip sequences not in the internal_leavesinternal_terminals list\n",
    "            \n",
    "            for start, end in internal_column_ranges:\n",
    "                # Ensure the range is within the bounds of the sequence length\n",
    "                start = max(0, start)\n",
    "                end = min(len(seq), end)\n",
    "\n",
    "                # Replace dashes with question marks within the specified range\n",
    "                for i in range(start, end + 1):  # +1 because the end is inclusive\n",
    "                    if seq[i] == '-':\n",
    "                        seq[i] = '?'\n",
    "        \n",
    "    elif internal_method == \"semi\" and internal_threshold is not None:\n",
    "        # Replace internal blocks of contiguous dashes larger than the internal_threshold with question marks\n",
    "        for seq_id, seq in alignment.items():\n",
    "            if seq_id not in sequences_to_process:\n",
    "                continue  # Skip sequences not in the internal_leavesinternal_terminals list\n",
    "\n",
    "            # Identify contiguous blocks of gaps (internal and terminal)\n",
    "            gap_block_start = None\n",
    "            for i in range(len(seq)):\n",
    "                if seq[i] == '-':\n",
    "                    if gap_block_start is None:\n",
    "                        gap_block_start = i  # Start of a new gap block\n",
    "                else:\n",
    "                    if gap_block_start is not None:\n",
    "                        # End of a gap block\n",
    "                        gap_length = i - gap_block_start\n",
    "                        if gap_length > internal_threshold and gap_block_start != 0 and gap_block_start != len(seq) - gap_length:\n",
    "                            # It's an internal block larger than threshold, replace with '?'\n",
    "                            for j in range(gap_block_start, i):\n",
    "                                seq[j] = '?'\n",
    "                        gap_block_start = None  # Reset for the next block\n",
    "            # Check for a gap block at the end of the sequence\n",
    "            if gap_block_start is not None:\n",
    "                gap_length = len(seq) - gap_block_start\n",
    "                if gap_length > internal_threshold and gap_block_start != 0:\n",
    "                    for j in range(gap_block_start, len(seq)):\n",
    "                        seq[j] = '?'\n",
    "        \n",
    "    # Convert the list back to a string\n",
    "    alignment = {seq_id: ''.join(seq) for seq_id, seq in alignment.items()}\n",
    "\n",
    "    return alignment\n",
    "\n",
    "# Print\n",
    "#alignment = replace_dashes_with_question_marks(alignment, internal_column_ranges=[(1,30),(33,40)], internal_leaves=[\"sp2\"], internal_method=\"manual\")\n",
    "alignment = replace_dashes_with_question_marks(alignment, internal_leaves=[\"sp2\"], internal_method=\"semi\", internal_threshold=2)\n",
    "print(\"\\n\") # Jump a line\n",
    "print(\"Step 1.3. Treat internal missing data\")\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "#########################\n",
    "# Step 3.1 Partitioning #\n",
    "#########################\n",
    "\n",
    "def add_breaks_terminal(alignment):\n",
    "    \"\"\"\n",
    "    Add # in all instances of terminal gap opening/closure (indicated by ?).\n",
    "       \n",
    "    Parameters:\n",
    "        alignment (dict): Dictionary where keys are sequence names and values are sequences.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Updated alignment with '#' before terminal gap opening and after terminal gap closure.\n",
    "    \"\"\"\n",
    "    # Determine the length of the sequences\n",
    "    seq_length = len(next(iter(alignment.values())))\n",
    "\n",
    "    # Initialize a list to keep track of positions that need '#' in all sequences\n",
    "    hash_positions = [False] * seq_length\n",
    "\n",
    "    # Iterate through each sequence to find gap regions\n",
    "    for seq in alignment.values():\n",
    "        i = 0\n",
    "        while i < seq_length:\n",
    "            if seq[i] == '?':\n",
    "                # Found the start of a gap region\n",
    "                start = i\n",
    "                while i < seq_length and seq[i] == '?':\n",
    "                    i += 1\n",
    "                end = i\n",
    "                # Mark the positions for this gap region (avoid marking the first column)\n",
    "                if start > 0:\n",
    "                    hash_positions[start] = True\n",
    "                if end < seq_length:\n",
    "                    hash_positions[end] = True\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    # Update each sequence with '#' at the identified positions\n",
    "    for key in alignment:\n",
    "        new_seq = []\n",
    "        for i in range(seq_length):\n",
    "            if hash_positions[i]:\n",
    "                new_seq.append('#')\n",
    "            new_seq.append(alignment[key][i])\n",
    "        # Handle the case where the last position is a gap\n",
    "        if hash_positions[-1]:\n",
    "            new_seq.append('#')\n",
    "        alignment[key] = ''.join(new_seq)\n",
    "\n",
    "# Method: conservative        \n",
    "def classify_and_insert_hashtags(alignment, \n",
    "                                 partitioning_round=1, \n",
    "                                 log_csv_output=False, \n",
    "                                 csv_file_path=\"contiguous_invariant_blocks.csv\"):\n",
    "    # Step 1: Classify columns as invariant or variant\n",
    "    num_sequences = len(alignment)\n",
    "    num_columns = len(next(iter(alignment.values())))  # Get the number of columns from one sequence\n",
    "\n",
    "    column_types = []  # To store the type of each column (invariant/variant)\n",
    "    contiguous_invariant_blocks = []  # To store lengths and positions of invariant blocks\n",
    "\n",
    "    for col_idx in range(num_columns):\n",
    "        column = [seq[col_idx] for seq in alignment.values()]\n",
    "        unique_values = set(column)\n",
    "        if len(unique_values - {'?'}) == 1:\n",
    "            column_types.append('invariant')\n",
    "        else:\n",
    "            column_types.append('variant')\n",
    "\n",
    "    # Step 2: Identify contiguous invariant columns and their lengths\n",
    "    current_invariant_block = None\n",
    "    for col_idx in range(num_columns):\n",
    "        if column_types[col_idx] == 'invariant':\n",
    "            if current_invariant_block is None:\n",
    "                current_invariant_block = {'start': col_idx, 'length': 1}\n",
    "            else:\n",
    "                current_invariant_block['length'] += 1\n",
    "        else:\n",
    "            if current_invariant_block:\n",
    "                contiguous_invariant_blocks.append(current_invariant_block)\n",
    "                current_invariant_block = None\n",
    "    if current_invariant_block:\n",
    "        contiguous_invariant_blocks.append(current_invariant_block)\n",
    "\n",
    "    # Step 3: Log and optionally process blocks\n",
    "    contiguous_invariant_blocks.sort(key=lambda x: x['length'], reverse=True)\n",
    "    block_lengths = {}\n",
    "    for block in contiguous_invariant_blocks:\n",
    "        block_lengths.setdefault(block['length'], []).append(block)\n",
    "\n",
    "    if partitioning_round == \"max\":\n",
    "        add_breaks_terminal(alignment)  # Call the other function instead of inserting hashtags\n",
    "    else:\n",
    "        # Step 4: Track the positions for inserting hashtags\n",
    "        hashtag_positions = []\n",
    "        block_lengths_sorted = sorted(block_lengths.keys(), reverse=True)\n",
    "        for block_length in block_lengths_sorted[:partitioning_round]:\n",
    "            blocks = block_lengths[block_length]\n",
    "            for block in blocks:\n",
    "                start_idx = block['start']\n",
    "                end_idx = start_idx + block['length'] - 1\n",
    "                middle_idx = (start_idx + end_idx) // 2\n",
    "                hashtag_positions.append(middle_idx)\n",
    "\n",
    "        # Step 5: Insert hashtags\n",
    "        for seq_id, seq in alignment.items():\n",
    "            sorted_positions = sorted(hashtag_positions)\n",
    "            shift = 0\n",
    "            for middle_idx in sorted_positions:\n",
    "                adjusted_idx = middle_idx + shift\n",
    "                seq = seq[:adjusted_idx + 1] + '#' + seq[adjusted_idx + 1:]\n",
    "                shift += 1\n",
    "            alignment[seq_id] = seq\n",
    "\n",
    "    # Step 6: Optionally log to CSV\n",
    "    if log_csv_output:\n",
    "        file_dir = os.path.dirname(csv_file_path)\n",
    "        if not os.path.exists(file_dir) and file_dir:\n",
    "            os.makedirs(file_dir)\n",
    "\n",
    "        with open(csv_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Start', 'End', 'Length'])\n",
    "            for block in contiguous_invariant_blocks:\n",
    "                start_idx = block['start']\n",
    "                end_idx = start_idx + block['length'] - 1\n",
    "                writer.writerow([start_idx, end_idx, block['length']])\n",
    "        print(f\"Log of contiguous invariant blocks written to {csv_file_path}\")\n",
    "\n",
    "    return alignment, contiguous_invariant_blocks\n",
    "\n",
    "def remove_adjacent_pound_columns(alignment):\n",
    "    \"\"\"\n",
    "    Remove adjacent columns of '#' in an alignment dictionary.\n",
    "    Keeps only one '#' per contiguous block of # columns.\n",
    "\n",
    "    Args:\n",
    "        alignment (dict): DNA alignment {sequence_id: sequence_string}\n",
    "\n",
    "    Returns:\n",
    "        dict: Cleaned alignment with single '#' per contiguous block\n",
    "    \"\"\"\n",
    "    # Transpose alignment to column-wise format\n",
    "    columns = list(zip(*alignment.values()))\n",
    "\n",
    "    # Identify positions to keep (i.e., not redundant '#')\n",
    "    keep_indices = []\n",
    "    prev_pound = False\n",
    "    for i, col in enumerate(columns):\n",
    "        if all(c == '#' for c in col):\n",
    "            if not prev_pound:\n",
    "                keep_indices.append(i)\n",
    "                prev_pound = True\n",
    "            # else skip this redundant pound column\n",
    "        else:\n",
    "            keep_indices.append(i)\n",
    "            prev_pound = False\n",
    "\n",
    "    # Rebuild alignment from kept columns\n",
    "    cleaned_alignment = {}\n",
    "    for seq_id in alignment:\n",
    "        new_seq = ''.join([alignment[seq_id][i] for i in keep_indices])\n",
    "        cleaned_alignment[seq_id] = new_seq\n",
    "\n",
    "    return cleaned_alignment\n",
    "\n",
    "# Method: equal\n",
    "def equal_length_partitioning(alignment, partitioning_size=None, partitioning_round=None, log=False):\n",
    "    \"\"\"\n",
    "    Insert pound signs '#' into a DNA alignment at regular intervals, either:\n",
    "      - every `partitioning_size` bp (e.g. every 100 bp), or\n",
    "      - using `partitioning_round` to divide into equal-length partitions.\n",
    "\n",
    "    Args:\n",
    "        alignment (dict): Dictionary of sequences {seq_id: sequence_str}\n",
    "        partitioning_size (int): Length of partitions (in base pairs)\n",
    "        partitioning_round (int): Number of pound signs to insert (creates N+1 partitions)\n",
    "        log (bool): Whether to print insertion positions and runtime\n",
    "\n",
    "    Returns:\n",
    "        dict: Modified alignment with pound signs inserted\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if not alignment:\n",
    "        raise ValueError(\"Alignment is empty\")\n",
    "\n",
    "    aln_length = len(next(iter(alignment.values())))\n",
    "    positions = []\n",
    "\n",
    "    # Mode 1: divide by fixed size\n",
    "    if partitioning_size:\n",
    "        positions = list(range(partitioning_size, aln_length, partitioning_size))\n",
    "\n",
    "    # Mode 2: divide by number of equal partitions\n",
    "    elif partitioning_round:\n",
    "        if partitioning_round >= aln_length:\n",
    "            raise ValueError(\"Too many partitions requested for alignment length.\")\n",
    "        interval = aln_length // (partitioning_round + 1)\n",
    "        positions = [(i + 1) * interval for i in range(partitioning_round)]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"You must provide either 'partitioning_size' or 'partitioning_round'\")\n",
    "\n",
    "    # Insert '#' at the selected positions\n",
    "    updated_alignment = {}\n",
    "    for seq_id, seq in alignment.items():\n",
    "        seq_list = list(seq)\n",
    "        for offset, pos in enumerate(positions):\n",
    "            seq_list.insert(pos + offset, \"#\")  # account for shifting due to insertions\n",
    "        updated_alignment[seq_id] = ''.join(seq_list)\n",
    "\n",
    "    runtime = time.time() - start_time\n",
    "\n",
    "    if log:\n",
    "        print(\"--- equal_length_partitioning ---\")\n",
    "        print(f\"Total time: {runtime:.4f} seconds\")\n",
    "        print(f\"Pound signs inserted at columns: {positions}\\n\")\n",
    "\n",
    "    return updated_alignment\n",
    "\n",
    "# Method: max\n",
    "def insert_pound_around_questions(alignment):\n",
    "    \"\"\"\n",
    "    Insert '#' columns around the opening and closure of '?' blocks.\n",
    "    \n",
    "    - If a '?' block starts in the first column, insert '#' in the second column.\n",
    "    - If a '?' block ends in the last column, insert '#' in the penultimate column.\n",
    "    - Elsewhere, insert '#' before or after transition points.\n",
    "    \n",
    "    Parameters:\n",
    "        alignment (dict): {sequence_name: aligned_sequence}\n",
    "    \n",
    "    Returns:\n",
    "        dict: Updated alignment with '#' inserted as new columns.\n",
    "    \"\"\"\n",
    "    if not alignment:\n",
    "        return alignment\n",
    "\n",
    "    names = list(alignment.keys())\n",
    "    matrix = [list(alignment[name]) for name in names]\n",
    "    num_rows = len(matrix)\n",
    "    num_cols = len(matrix[0])\n",
    "\n",
    "    # Check consistency\n",
    "    if any(len(row) != num_cols for row in matrix):\n",
    "        raise ValueError(\"All sequences must have the same length.\")\n",
    "\n",
    "    insert_indices = set()\n",
    "\n",
    "    for col in range(num_cols):\n",
    "        column = [matrix[r][col] for r in range(num_rows)]\n",
    "\n",
    "        if '?' not in column:\n",
    "            continue\n",
    "\n",
    "        # Check for start of '?' block\n",
    "        if col == 0:\n",
    "            next_col = [matrix[r][col + 1] for r in range(num_rows)]\n",
    "            if any(c == '?' and nc != '?' for c, nc in zip(column, next_col)) or \\\n",
    "               any(c != '?' and nc == '?' for c, nc in zip(column, next_col)):\n",
    "                insert_indices.add(1)\n",
    "        elif col == num_cols - 1:\n",
    "            prev_col = [matrix[r][col - 1] for r in range(num_rows)]\n",
    "            if any(pc != '?' and c == '?' for pc, c in zip(prev_col, column)) or \\\n",
    "               any(pc == '?' and c != '?' for pc, c in zip(prev_col, column)):\n",
    "                insert_indices.add(num_cols - 1)\n",
    "        else:\n",
    "            prev_col = [matrix[r][col - 1] for r in range(num_rows)]\n",
    "            next_col = [matrix[r][col + 1] for r in range(num_rows)]\n",
    "\n",
    "            # Transition into '?' block â†’ insert '#' before\n",
    "            if any(pc != '?' and c == '?' for pc, c in zip(prev_col, column)):\n",
    "                insert_indices.add(col)\n",
    "\n",
    "            # Transition out of '?' block â†’ insert '#' after\n",
    "            if any(c == '?' and nc != '?' for c, nc in zip(column, next_col)):\n",
    "                insert_indices.add(col + 1)\n",
    "\n",
    "    # Insert pound signs (from right to left to maintain positions)\n",
    "    for idx in sorted(insert_indices, reverse=True):\n",
    "        for row in matrix:\n",
    "            row.insert(idx, '#')\n",
    "\n",
    "    # Return as dict\n",
    "    return {names[i]: ''.join(row) for i, row in enumerate(matrix)}\n",
    "\n",
    "# Method: balanced\n",
    "def balanced_partitioning(alignment, log=False, partitioning_round=1):\n",
    "    \"\"\"\n",
    "    Partition alignment by merging blocks flanked by '#' based on a length threshold.\n",
    "\n",
    "    Steps:\n",
    "    1. Call insert_pound_around_questions to place '#' around blocks of '?'.\n",
    "    2. Identify blocks flanked by '#'.\n",
    "    3. Use the N-th longest block (based on partitioning_round) as the merging threshold.\n",
    "    4. Merge adjacent blocks (delete '#' between them) if their combined size < N.\n",
    "    5. First pass left-to-right, then right-to-left.\n",
    "\n",
    "    Parameters:\n",
    "        alignment (dict): {sequence_name: aligned_sequence}\n",
    "        log (bool): If True, print logs of changes.\n",
    "        partitioning_round (int): Use the N-th longest block as the threshold.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated alignment with balanced '#' partitioning.\n",
    "    \"\"\"\n",
    "    from copy import deepcopy\n",
    "\n",
    "    alignment = insert_pound_around_questions(alignment)\n",
    "    names = list(alignment.keys())\n",
    "    matrix = [list(alignment[name]) for name in names]\n",
    "    num_cols = len(matrix[0])\n",
    "\n",
    "    # Identify block boundaries (flanked by '#')\n",
    "    pound_indices = [i for i in range(num_cols) if all(row[i] == '#' for row in matrix)]\n",
    "    pound_indices = [-1] + pound_indices + [num_cols]  # Add virtual boundaries\n",
    "    blocks = [(pound_indices[i] + 1, pound_indices[i + 1]) for i in range(len(pound_indices) - 1)]\n",
    "\n",
    "    # Compute lengths and positions of all blocks\n",
    "    block_lengths = [(i, end - start, start, end - 1) for i, (start, end) in enumerate(blocks)]  # (index, size, start, end)\n",
    "    sorted_blocks = sorted(block_lengths, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    if partitioning_round > len(sorted_blocks):\n",
    "        raise ValueError(f\"partitioning_round={partitioning_round} is greater than total number of blocks.\")\n",
    "\n",
    "    block_idx, N, start_col, end_col = sorted_blocks[partitioning_round - 1]\n",
    "\n",
    "    merged_log = []\n",
    "    if log:\n",
    "        suffix = {1: \"st\", 2: \"nd\", 3: \"rd\"}.get(partitioning_round, \"th\")\n",
    "        merged_log.append(\n",
    "            f\"Threshold = size of the {partitioning_round}{suffix} largest block: {N} columns \"\n",
    "            f\"(columns {start_col} to {end_col})\"\n",
    "        )\n",
    "\n",
    "    def merge_blocks(direction):\n",
    "        nonlocal matrix\n",
    "        while True:\n",
    "            # Recompute pound indices and blocks after each merge\n",
    "            num_cols = len(matrix[0])\n",
    "            pound_indices = [i for i in range(num_cols) if all(row[i] == '#' for row in matrix)]\n",
    "            pound_indices = [-1] + pound_indices + [num_cols]\n",
    "            blocks = [(pound_indices[i] + 1, pound_indices[i + 1]) for i in range(len(pound_indices) - 1)]\n",
    "            changed = False\n",
    "\n",
    "            if direction == \"left\":\n",
    "                i_range = range(0, len(blocks) - 1)\n",
    "            else:\n",
    "                i_range = range(len(blocks) - 2, -1, -1)\n",
    "\n",
    "            for i in i_range:\n",
    "                left_start, left_end = blocks[i]\n",
    "                right_start, right_end = blocks[i + 1]\n",
    "                combined_size = (left_end - left_start) + (right_end - right_start)\n",
    "\n",
    "                if combined_size < N:\n",
    "                    # Remove pound between blocks\n",
    "                    pound_col_index = left_end\n",
    "                    for row in matrix:\n",
    "                        del row[pound_col_index]\n",
    "                    merged_log.append(f\"Merged blocks at columns {left_start}-{left_end - 1} and {right_start}-{right_end - 1} \"\n",
    "                                      f\"(deleted '#' at col {pound_col_index}) â†’ size={combined_size}\")\n",
    "                    changed = True\n",
    "                    break  # Restart loop\n",
    "\n",
    "            if not changed:\n",
    "                break\n",
    "\n",
    "    merge_blocks(\"left\")\n",
    "    merge_blocks(\"right\")\n",
    "\n",
    "    if log:\n",
    "        print(\"\\n\".join(merged_log))\n",
    "\n",
    "    return {names[i]: ''.join(row) for i, row in enumerate(matrix)}\n",
    "\n",
    "\n",
    "# Print\n",
    "#alignment = classify_and_insert_hashtags(alignment,partitioning_round=1, log_csv_output=False)\n",
    "#alignment = equal_length_partitioning(alignment, partitioning_size=10)\n",
    "#alignment = insert_pound_around_questions(alignment)\n",
    "alignment = balanced_partitioning(alignment, partitioning_round=1, log=True)\n",
    "print(\"\\n\") # Jump a line\n",
    "print(\"Step 3.1. Insert pound signs\")\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "########################\n",
    "# Step 3.2: Refinement #\n",
    "########################\n",
    "\n",
    "def refinement_question2hyphen(alignment):\n",
    "    \"\"\"\n",
    "    Replaces contiguous '?' characters flanked by '#' with '-' in the sequence alignment.\n",
    "    This includes blocks surrounded by '#' as well as the first and last blocks that are flanked only on one side.\n",
    "\n",
    "    Parameters:\n",
    "        alignment (dict): Dictionary with sequence names as keys and sequences as values.\n",
    "\n",
    "    Returns:\n",
    "        dict: Modified alignment with '?' replaced by '-' where flanked by '#'.\n",
    "    \"\"\"\n",
    "    # Iterate through each sequence in the alignment\n",
    "    for name, sequence in alignment.items():\n",
    "        # Replace '?' surrounded by '#' on both sides (internal blocks)\n",
    "        modified_sequence = re.sub(r'(?<=#)\\?+(?=#)', lambda m: '-' * len(m.group(0)), sequence)\n",
    "        \n",
    "        # Replace '?' in the first block (flanked by # on the right)\n",
    "        modified_sequence = re.sub(r'^(\\?+)(?=#)', lambda m: '-' * len(m.group(0)), modified_sequence)\n",
    "        \n",
    "        # Replace '?' in the last block (flanked by # on the left)\n",
    "        modified_sequence = re.sub(r'(?<=#)(\\?+)$', lambda m: '-' * len(m.group(0)), modified_sequence)\n",
    "\n",
    "        # Update the alignment dictionary with the modified sequence\n",
    "        alignment[name] = modified_sequence\n",
    "\n",
    "    return alignment\n",
    "\n",
    "# Print\n",
    "refinement_question2hyphen(alignment) # Replaces blocks of missing data (consecutive '?' characters flanked by '#') with '-'.\n",
    "print(\"\\n\") # Jump a line\n",
    "print(\"Step 3.2.1 Replace ? flanked by # with -\")\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "def remove_columns_with_W(alignment: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Remove problematic columns in a DNA alignment:\n",
    "    \n",
    "    1. For each sequence, identify 15 contiguous 'W' or 'w' characters. \n",
    "       Mark those columns for deletion.\n",
    "    2. Remove columns where all values are:\n",
    "       - only 'W'\n",
    "       - only 'W' and '?'\n",
    "       - only 'W', '?', and '-'\n",
    "    \n",
    "    Args:\n",
    "        alignment (dict): Dictionary of {sequence_id: sequence_string}\n",
    "\n",
    "    Returns:\n",
    "        dict: Cleaned alignment with columns removed\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # Convert alignment to matrix\n",
    "    seq_ids = list(alignment.keys())\n",
    "    seqs = list(alignment.values())\n",
    "    alignment_array = np.array([list(seq) for seq in seqs])\n",
    "    n_rows, n_cols = alignment_array.shape\n",
    "\n",
    "    columns_to_remove = set()\n",
    "\n",
    "    # Step 1: Find 15 contiguous W/w in any sequence\n",
    "    for row in alignment_array:\n",
    "        upper_row = [char.upper() for char in row]\n",
    "        for i in range(n_cols - 14):\n",
    "            if all(base == 'W' for base in upper_row[i:i+15]):\n",
    "                columns_to_remove.update(range(i, i + 15))\n",
    "\n",
    "    # Step 2: Remove columns with only W / W+? / W+?+-\n",
    "    for col_idx in range(n_cols):\n",
    "        col_bases = set(alignment_array[:, col_idx].astype(str).flatten().tolist())\n",
    "        col_bases_upper = {b.upper() for b in col_bases}\n",
    "        if col_bases_upper.issubset({'W'}) or \\\n",
    "           col_bases_upper.issubset({'W', '?'}) or \\\n",
    "           col_bases_upper.issubset({'W', '?', '-'}):\n",
    "            columns_to_remove.add(col_idx)\n",
    "\n",
    "    # Remove columns\n",
    "    columns_to_keep = sorted(set(range(n_cols)) - columns_to_remove)\n",
    "    cleaned_array = alignment_array[:, columns_to_keep]\n",
    "\n",
    "    # Convert back to dictionary\n",
    "    cleaned_alignment = {\n",
    "        seq_id: ''.join(cleaned_array[i]) for i, seq_id in enumerate(seq_ids)\n",
    "    }\n",
    "\n",
    "    return cleaned_alignment\n",
    "\n",
    "# Print\n",
    "remove_columns_with_W(alignment) \n",
    "print(\"\\n\") # Jump a line\n",
    "print(\"Step 3.2.2 Delete columns of W (artifacts from internal missing data identification by GB2MSA)\")\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "def n2question_func(alignment: dict, leaves='all', log=False):\n",
    "    \"\"\"\n",
    "    Replace all ambiguous nucleotides 'N' or 'n' with '?' in selected sequences.\n",
    "\n",
    "    Parameters:\n",
    "    - alignment (dict): Dictionary where keys are sequence names and values are DNA sequences (str).\n",
    "    - leaves (str or list): Sequence name(s) to apply the replacement. Use 'all' to apply to all sequences.\n",
    "    - log (bool): If True, also return a log of replaced blocks with their positions.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Modified alignment with 'N'/'n' replaced with '?'.\n",
    "    - list (optional): List of tuples (seq_name, start, end) for each replaced block.\n",
    "    \"\"\"\n",
    "    if leaves == 'all':\n",
    "        leaves_to_process = alignment.keys()\n",
    "    elif isinstance(leaves, str):\n",
    "        leaves_to_process = [leaves]\n",
    "    else:\n",
    "        leaves_to_process = leaves\n",
    "\n",
    "    updated_alignment = {}\n",
    "    replacement_log = []\n",
    "\n",
    "    for name, seq in alignment.items():\n",
    "        if name in leaves_to_process:\n",
    "            new_seq = []\n",
    "            i = 0\n",
    "            while i < len(seq):\n",
    "                if seq[i] in ('N', 'n'):\n",
    "                    start = i\n",
    "                    while i < len(seq) and seq[i] in ('N', 'n'):\n",
    "                        i += 1\n",
    "                    end = i - 1\n",
    "                    replacement_log.append((name, start, end))\n",
    "                    new_seq.extend(['?'] * (end - start + 1))\n",
    "                else:\n",
    "                    new_seq.append(seq[i])\n",
    "                    i += 1\n",
    "            updated_alignment[name] = ''.join(new_seq)\n",
    "        else:\n",
    "            updated_alignment[name] = seq\n",
    "\n",
    "    if log:\n",
    "        return updated_alignment, replacement_log\n",
    "    return updated_alignment\n",
    "\n",
    "\n",
    "# Print\n",
    "alignment = n2question_func(alignment, leaves='sp4') \n",
    "print(\"\\n\") # Jump a line\n",
    "print(\"Step 3.2.3 Replace IUPAC N with question marks)\")\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "###########################\n",
    "# ADDITIONAL LOG FUNCTION #\n",
    "###########################\n",
    "\n",
    "def compute_summary_after(alignment):\n",
    "    num_seqs = len(alignment)\n",
    "\n",
    "    # Transpose to columns\n",
    "    columns = list(zip(*alignment.values()))\n",
    "    \n",
    "    # Count columns that contain pound signs\n",
    "    total_pound = sum('#' in col for col in columns)\n",
    "    \n",
    "    # Alignment length excluding columns of only pound signs\n",
    "    aln_length = len(columns)\n",
    "\n",
    "\n",
    "    # Count nucleotide and gap characters\n",
    "    total_nt = sum(c in \"ACGTacgt\" for seq in alignment.values() for c in seq)\n",
    "    total_gaps = sum(seq.count(\"-\") for seq in alignment.values())\n",
    "    total_ns = sum(c in \"Nn\" for seq in alignment.values() for c in seq)\n",
    "    total_qm = sum(seq.count(\"?\") for seq in alignment.values())  # <- includes everything now\n",
    "\n",
    "    # Count additional missing data as gap-only blocks between #\n",
    "    missing_by_partition = 0\n",
    "    for seq in alignment.values():\n",
    "        parts = seq.split(\"#\")\n",
    "        for part in parts:\n",
    "            if all(c == '-' for c in part):\n",
    "                missing_by_partition += len(part)\n",
    "\n",
    "    total_missing = total_qm + missing_by_partition\n",
    "\n",
    "    return {\n",
    "        \"num_seqs\": num_seqs,\n",
    "        \"aln_length\": aln_length,\n",
    "        \"total_nt\": total_nt,\n",
    "        \"total_gaps\": total_gaps,\n",
    "        \"total_ns\": total_ns,\n",
    "        \"total_qm\": total_qm,\n",
    "        \"total_pound\": total_pound,\n",
    "        \"missing_by_partition\": missing_by_partition,\n",
    "        \"total_missing\": total_missing\n",
    "    }\n",
    "\n",
    "def detect_fully_missing_partitions(alignment):\n",
    "    \"\"\"\n",
    "    Logs all `?` and `-` from fully missing partitions.\n",
    "    A fully missing partition is a region (between #) in which a sequence has only dashes.\n",
    "    \"\"\"\n",
    "    log_entries = []\n",
    "    total_question_marks = 0\n",
    "    total_dash_from_missing_partitions = 0\n",
    "\n",
    "    for seq_id, seq in alignment.items():\n",
    "        total_question_marks += seq.count(\"?\")\n",
    "\n",
    "        parts = seq.split(\"#\")\n",
    "        col_index = 0  # absolute position tracker\n",
    "        for i, part in enumerate(parts):\n",
    "            if all(c == '-' for c in part):\n",
    "                dash_count = len(part)\n",
    "                total_dash_from_missing_partitions += dash_count\n",
    "                start = col_index\n",
    "                end = col_index + dash_count - 1\n",
    "                log_entries.append(f\"{seq_id}: partition {i} ({start}â€“{end}, length {dash_count}) fully missing (all '-')\")\n",
    "            col_index += len(part) + 1  # +1 for the '#' removed in split\n",
    "\n",
    "    summary = (\n",
    "        f\"Total '?' characters: {total_question_marks}\\n\"\n",
    "        f\"Total '-' characters in fully missing partitions: {total_dash_from_missing_partitions}\\n\"\n",
    "        f\"Combined total: {total_question_marks + total_dash_from_missing_partitions}\\n\"\n",
    "    )\n",
    "\n",
    "    return summary + \"\\n\" + \"\\n\".join(log_entries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5d3e9c",
   "metadata": {},
   "source": [
    "## Command-line functions and debugging tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25404b0",
   "metadata": {},
   "source": [
    "Functions to run in the command line using argparse (see file prepDyn.py, addSeq.py, and GB2MSA.py). They were tested in Mac OS. Here I test the version without argparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de218d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Debug when GB_input is used\\nprepDyn(GB_input= \"../test_data/tutorial/ex1.1/ex1.1_input.csv\",\\n        output_file=\"../test_data/tutorial/ex1.1/ex1.1\",\\n        orphan_method=\"semi\", orphan_threshold=20,\\n        internal_method=\"semi\", internal_threshold=10,\\n        del_inv=True,\\n        partitioning_method=\"max\", \\n        #partitioning_round=3,\\n        #partitioning_size=100,\\n        log=True, sequence_names=True,\\n        n2question=\\'all\\')\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the integrated function\n",
    "\n",
    "def prepDyn(input_file=None,\n",
    "            GB_input=None,\n",
    "            input_format=\"fasta\",\n",
    "            MSA=False,\n",
    "            output_file=None,\n",
    "            output_format=\"fasta\",\n",
    "            log=False,\n",
    "            sequence_names=True,\n",
    "            # Trimming parameters\n",
    "            orphan_method=None,\n",
    "            orphan_threshold=10,\n",
    "            percentile=25,\n",
    "            del_inv=True,\n",
    "            # Missing data parameters\n",
    "            internal_method=None,\n",
    "            internal_column_ranges=None,\n",
    "            internal_leaves=\"all\",\n",
    "            internal_threshold=None,\n",
    "            n2question=None,\n",
    "            # Partitioning parameters\n",
    "            partitioning_round=0,\n",
    "            partitioning_method=\"balanced\",\n",
    "            partitioning_size=None\n",
    "            ):\n",
    "    \"\"\"\n",
    "    Preprocess missing data for dynamic homology in PhyG. First, columns containing\n",
    "    only gaps, orphan nucleotides, and invariant columns can be trimmed. Second,\n",
    "    missing data is coded with question marks. Third, partitions are delimited in\n",
    "    highly conserved regions.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to the input alignment file or directory. Ignored if GB_input is provided.\n",
    "        GB_input (str): Path to a CSV/TSV file containing GenBank accession numbers. If provided,\n",
    "                        sequences will be downloaded from GenBank and aligned before preprocessing.\n",
    "        input_format (str): Format of the input alignment. Options: 'fasta' (default),\n",
    "                            'clustal', 'phylip', or any format accepted by Biopython.\n",
    "        output_file (str): Custom prefix for output files. If None, base_name from input_file is used.\n",
    "        output_format (str): Output format. Default is 'fasta'.\n",
    "        log (bool): Whether to write a log with wall-clock time. Default is False.\n",
    "        sequence_names (bool): If True, writes a TXT file listing all sorted unique sequence names. Default is True.\n",
    "        MSA (bool): Whether to perform MSA if input sequences specified in input_file are unaligned\n",
    "        orphan_method (str): The trimming method. By default, trimming orphan nucleotides\n",
    "                             is not performed. Options:\n",
    "                            - 'auto': trim using the 25th percentile;\n",
    "                            - 'semi': trim with a manual threshold.\n",
    "        orphan_threshold (int): Threshold used to trim orphan nucleotides if orphan_method = 'semi'.\n",
    "        percentile (float): Used with orphan_method = 'auto' to define trimming threshold.\n",
    "        del_inv (bool): Whether to trim invariant terminal columns. Default is True.\n",
    "        internal_method (str): Defines how to identify internal missing data. Automatic identificaton\n",
    "                               of missing data is made if GB_input is provided. Otherwise, naive\n",
    "                               options to identify internal missing data are:\n",
    "                               - \"manual\": Use column ranges;\n",
    "                               - \"semi\": Use a threshold for gaps.\n",
    "        internal_column_ranges (list): Column ranges (inclusive) if internal_method = 'manual'.\n",
    "        internal_leaves (str or list): Sequences to apply internal missing data replacement\n",
    "                                       if internal_method is not \"None\".\n",
    "        internal_threshold (int): Used with internal_method = 'semi' to define gap threshold.\n",
    "                                  Contiguous '-' larger than the threshold are replaced with '?'.\n",
    "        n2question (str or list): If specified, replaces ambiguous nucleotide 'N' or 'n' with '?'. If None (default), n2question is not performed. If 'all', apply to all sequences. If you want to apply to only one sequence, write the name of this sequence. If you want to apply to multiple sequences but no all, wrie the list of sequences.    \n",
    "        partitioning_method (str): Method of partitioning:\n",
    "                                   - 'balanced': Based on initial '#' inserted with 'max', iteratively \n",
    "                                   merges adjacent blocks flanked by # if their combined length is below\n",
    "                                   a threshold (the n-largest block of missing data).\n",
    "                                   - 'conservative': Blocks containing only invariant columns are sorted\n",
    "                                   by length and '#' column(s) inserted at the midpoint of the n-largest\n",
    "                                   block(s). Must define n using partitioning_round.\n",
    "                                   - 'equal: Equal-length partitions are created by specifying their size\n",
    "                                   or their round. If partitioning_round = 1, only 1 '#' column is\n",
    "                                   inserted; if partitioning_round = 2, then 2 '#' columns are inserted.\n",
    "                                   - 'max': '#' columns are inserted around blocks of missing data (every\n",
    "                                   instance of '?' opening/closure but not '?' extension).\n",
    "        partitioning_round (int): Number of partitioning round. Invariant regions are sorted by length\n",
    "                                  in descendant order and the n-largest block(s) partitioned using '#'.\n",
    "                                  If \"max\" is specified, pound signs are inserted arund all blocks of\n",
    "                                  missing data.\n",
    "        partitioning_size (int): Size of equal-length partitions if partitioning_method = 'equal'.\n",
    "\n",
    "    Returns:\n",
    "        dict: The preprocessed unaligned sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Start overall timer for the top-level call ---\n",
    "    overall_start_wall_time = time.time()\n",
    "    overall_start_cpu_time = time.process_time()\n",
    "\n",
    "    # --- Initialize the shared sequence ID set for the top-level call ---\n",
    "    _all_sequence_ids_shared = set()\n",
    "\n",
    "    # --- Store original output_file for final sequence_names.txt path ---\n",
    "    original_output_file_arg = output_file # Store the exact argument passed to prepDyn\n",
    "    \n",
    "    # --- Generate the original command line string here ---\n",
    "    cmd_parts = [\"prepDyn(\"]\n",
    "    params = {\n",
    "        \"input_file\": input_file,\n",
    "        \"GB_input\": GB_input,\n",
    "        \"input_format\": input_format,\n",
    "        \"MSA\": MSA,\n",
    "        \"output_file\": output_file,\n",
    "        \"output_format\": output_format,\n",
    "        \"log\": log,\n",
    "        \"sequence_names\": sequence_names,\n",
    "        \"orphan_method\": orphan_method,\n",
    "        \"orphan_threshold\": orphan_threshold,\n",
    "        \"percentile\": percentile,\n",
    "        \"del_inv\": del_inv,\n",
    "        \"internal_method\": internal_method,\n",
    "        \"internal_column_ranges\": internal_column_ranges,\n",
    "        \"internal_leaves\": internal_leaves,\n",
    "        \"internal_threshold\": internal_threshold,\n",
    "        \"n2question\": n2question,\n",
    "        \"partitioning_method\": partitioning_method,\n",
    "        \"partitioning_round\": partitioning_round,\n",
    "        \"partitioning_size\": partitioning_size,\n",
    "    }\n",
    "\n",
    "    param_strs = []\n",
    "    for k, v in params.items():\n",
    "        # Represent strings with quotes, lists/tuples as is, others repr()\n",
    "        if isinstance(v, str):\n",
    "            param_strs.append(f\"{k}='{v}'\")\n",
    "        elif isinstance(v, (list, tuple)):\n",
    "            param_strs.append(f\"{k}={v}\")\n",
    "        elif v is not None: \n",
    "            param_strs.append(f\"{k}={repr(v)}\")\n",
    "    \n",
    "    cmd_parts.append(\", \".join(param_strs))\n",
    "    cmd_parts.append(\")\")\n",
    "    original_cmd_line = \"\".join(cmd_parts)\n",
    "\n",
    "\n",
    "    def _prepDyn_recursive(input_val, # Renamed to input_val to be more generic for file path or dict\n",
    "                           GB_input,\n",
    "                           input_format,\n",
    "                           MSA,\n",
    "                           output_file, # This is crucial: the output prefix for THIS specific call\n",
    "                           output_format,\n",
    "                           log,\n",
    "                           sequence_names, # This will be False for recursive calls\n",
    "                           _all_sequence_ids, # This is the shared set\n",
    "                           _is_top_level_call, # Flag for controlling final write\n",
    "                           _original_cmd_line, # Parameter to pass the original command\n",
    "                           orphan_method,\n",
    "                           orphan_threshold,\n",
    "                           percentile,\n",
    "                           del_inv,\n",
    "                           internal_method,\n",
    "                           internal_column_ranges,\n",
    "                           internal_leaves,\n",
    "                           internal_threshold,\n",
    "                           n2question,\n",
    "                           partitioning_round,\n",
    "                           partitioning_method,\n",
    "                           partitioning_size\n",
    "                           ):\n",
    "\n",
    "        # Start timers if logging is enabled (only for current processing, not recursive overall)\n",
    "        if log:\n",
    "            start_wall_time = time.time()\n",
    "            start_cpu_time = time.process_time()\n",
    "\n",
    "        # --- Pre-processing for output_file path handling for *this* recursive call ---\n",
    "        current_output_dir = os.path.dirname(output_file)\n",
    "        if not current_output_dir:\n",
    "            current_output_dir = \".\" # Default to current directory if no path in output_file\n",
    "        os.makedirs(current_output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "        # Step 1: Run GB2MSA if GenBank input is provided\n",
    "        if GB_input is not None:\n",
    "            # (This part of the logic remains unchanged)\n",
    "            print(\"Running GB2MSA on GenBank input...\")\n",
    "            \n",
    "            gb_output_prefix_for_gb2msa = output_file \n",
    "            cleaned_files = GB2MSA(GB_input, output_prefix=gb_output_prefix_for_gb2msa, write_names=False, log=False)\n",
    "            \n",
    "            for file_path_from_gb2msa in cleaned_files:\n",
    "                file_basename_no_ext = os.path.splitext(os.path.basename(file_path_from_gb2msa))[0]\n",
    "                gene_name_part = file_basename_no_ext.replace(\"_aligned\", \"\").replace(\"_GB2MSA\", \"\")\n",
    "                base_from_original_output = os.path.basename(original_output_file_arg) if original_output_file_arg else \"\"\n",
    "                if base_from_original_output and base_from_original_output != os.path.basename(os.path.normpath(original_output_file_arg)):\n",
    "                    gene_specific_prefix_base = f\"{base_from_original_output}_{gene_name_part}\"\n",
    "                elif os.path.isdir(original_output_file_arg):\n",
    "                    gene_specific_prefix_base = gene_name_part\n",
    "                else:\n",
    "                    gene_specific_prefix_base = gene_name_part\n",
    "\n",
    "                specific_output_prefix_for_recursion = os.path.join(current_output_dir, gene_specific_prefix_base)\n",
    "                alignment = AlignIO.read(file_path_from_gb2msa, \"fasta\")\n",
    "                alignment_dict = {record.id: str(record.seq) for record in alignment}\n",
    "                _all_sequence_ids.update(alignment_dict.keys())\n",
    "                _prepDyn_recursive(input_val=alignment_dict,\n",
    "                                GB_input=None, input_format=\"dict\", MSA=MSA,\n",
    "                                orphan_method=orphan_method, orphan_threshold=orphan_threshold, percentile=percentile, del_inv=del_inv,\n",
    "                                internal_method=internal_method, internal_column_ranges=internal_column_ranges, internal_leaves=internal_leaves, internal_threshold=internal_threshold,\n",
    "                                n2question=n2question, partitioning_method=partitioning_method, partitioning_round=partitioning_round, partitioning_size=partitioning_size,\n",
    "                                output_format=output_format, log=log, sequence_names=False,\n",
    "                                _all_sequence_ids=_all_sequence_ids, _is_top_level_call=False, _original_cmd_line=_original_cmd_line,\n",
    "                                output_file=specific_output_prefix_for_recursion)\n",
    "            return\n",
    "\n",
    "        # Step 2: If a folder is provided, process each alignment inside\n",
    "        if isinstance(input_val, str) and os.path.isdir(input_val):\n",
    "            # (This part of the logic remains unchanged)\n",
    "            processed_any_file = False\n",
    "            base_name_for_output_prefix = \"\"\n",
    "            if original_output_file_arg and os.path.isdir(original_output_file_arg):\n",
    "                base_name_for_output_prefix = os.path.basename(os.path.normpath(original_output_file_arg))\n",
    "            elif original_output_file_arg:\n",
    "                 base_name_for_output_prefix = os.path.basename(original_output_file_arg)\n",
    "\n",
    "            for file_name in os.listdir(input_val):\n",
    "                file_extension = os.path.splitext(file_name)[1].lstrip('.')\n",
    "                if file_extension == input_format:\n",
    "                    processed_any_file = True\n",
    "                    file_path = os.path.join(input_val, file_name)\n",
    "                    base_name_of_file = os.path.splitext(file_name)[0]\n",
    "                    \n",
    "                    if base_name_for_output_prefix:\n",
    "                        specific_output_prefix = os.path.join(current_output_dir, f\"{base_name_for_output_prefix}_{base_name_of_file}\")\n",
    "                    else: \n",
    "                        specific_output_prefix = os.path.join(current_output_dir, base_name_of_file)\n",
    "\n",
    "                    current_file_alignment = None\n",
    "                    if MSA:\n",
    "                        # (MSA logic for folder input unchanged)\n",
    "                        print(f\"Processing unaligned file for MAFFT: {file_path}\")\n",
    "                        temp_dir = current_output_dir if current_output_dir != \".\" else tempfile.gettempdir()\n",
    "                        os.makedirs(temp_dir, exist_ok=True)\n",
    "                        with tempfile.NamedTemporaryFile(mode=\"w\", delete=False, dir=temp_dir, suffix=f\".{input_format}\") as tmp_in:\n",
    "                            sequences = list(SeqIO.parse(file_path, input_format))\n",
    "                            SeqIO.write(sequences, tmp_in, \"fasta\")\n",
    "                            tmp_in_path = tmp_in.name\n",
    "                        tmp_out_path = os.path.join(temp_dir, f\"{os.path.basename(tmp_in_path)}_aligned.fasta\")\n",
    "                        try:\n",
    "                            mafft_result = subprocess.run([\"mafft\", \"--auto\", tmp_in_path], capture_output=True, text=True, check=False)\n",
    "                            if mafft_result.returncode != 0: raise RuntimeError(f\"MAFFT alignment failed for {file_name}.\")\n",
    "                            with open(tmp_out_path, \"w\") as f_out: f_out.write(mafft_result.stdout)\n",
    "                            current_file_alignment = {record.id: str(record.seq) for record in AlignIO.read(tmp_out_path, \"fasta\")}\n",
    "                        finally:\n",
    "                            if os.path.exists(tmp_in_path): os.remove(tmp_in_path)\n",
    "                            if os.path.exists(tmp_out_path): os.remove(tmp_out_path)\n",
    "                    else:\n",
    "                        alignment_temp = AlignIO.read(file_path, input_format)\n",
    "                        current_file_alignment = {record.id: str(record.seq) for record in alignment_temp}\n",
    "\n",
    "                    if current_file_alignment:\n",
    "                        _all_sequence_ids.update(current_file_alignment.keys())\n",
    "                        _prepDyn_recursive(input_val=current_file_alignment,\n",
    "                                GB_input=None, input_format=\"dict\", MSA=False,\n",
    "                                orphan_method=orphan_method, orphan_threshold=orphan_threshold, percentile=percentile, del_inv=del_inv,\n",
    "                                internal_method=internal_method, internal_column_ranges=internal_column_ranges, internal_leaves=internal_leaves, internal_threshold=internal_threshold,\n",
    "                                n2question=n2question, partitioning_method=partitioning_method, partitioning_round=partitioning_round, partitioning_size=partitioning_size,\n",
    "                                output_format=output_format, log=log, sequence_names=False,\n",
    "                                _all_sequence_ids=_all_sequence_ids, _is_top_level_call=False, _original_cmd_line=_original_cmd_line,\n",
    "                                output_file=specific_output_prefix)\n",
    "            if not processed_any_file:\n",
    "                print(f\"WARNING: No files with extension '.{input_format}' found in directory: {input_val}\")\n",
    "            return\n",
    "\n",
    "\n",
    "        # Step 3: Read and process alignment\n",
    "        alignment = None\n",
    "        if isinstance(input_val, dict):\n",
    "            alignment = input_val\n",
    "        elif isinstance(input_val, str) and os.path.isfile(input_val):\n",
    "            # (MSA logic for single file input unchanged)\n",
    "            if MSA:\n",
    "                temp_dir = current_output_dir if current_output_dir != \".\" else tempfile.gettempdir()\n",
    "                os.makedirs(temp_dir, exist_ok=True)\n",
    "                with tempfile.NamedTemporaryFile(mode=\"w\", delete=False, dir=temp_dir, suffix=f\".{input_format}\") as tmp_in:\n",
    "                    sequences = list(SeqIO.parse(input_val, input_format))\n",
    "                    SeqIO.write(sequences, tmp_in, \"fasta\")\n",
    "                    tmp_in_path = tmp_in.name\n",
    "                tmp_out_path = os.path.join(temp_dir, f\"{os.path.basename(tmp_in_path)}_aligned.fasta\")\n",
    "                try:\n",
    "                    mafft_result = subprocess.run([\"mafft\", \"--auto\", tmp_in_path], capture_output=True, text=True, check=False)\n",
    "                    if mafft_result.returncode != 0: raise RuntimeError(f\"MAFFT alignment failed for {os.path.basename(input_val)}.\")\n",
    "                    with open(tmp_out_path, \"w\") as f_out: f_out.write(mafft_result.stdout)\n",
    "                    alignment_obj = AlignIO.read(tmp_out_path, \"fasta\")\n",
    "                finally:\n",
    "                    if os.path.exists(tmp_in_path): os.remove(tmp_in_path)\n",
    "                    if os.path.exists(tmp_out_path): os.remove(tmp_out_path)\n",
    "            else:\n",
    "                alignment_obj = AlignIO.read(input_val, input_format)\n",
    "            \n",
    "            alignment = {record.id: str(record.seq) for record in alignment_obj}\n",
    "            _all_sequence_ids.update(alignment.keys())\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid input_val type or path: {input_val}.\")\n",
    "\n",
    "\n",
    "        ### FIX: PART 1 - Capture \"before\" statistics right after loading ###\n",
    "        # Initialize variables to store the \"before\" summary\n",
    "        num_seqs_before, aln_length_before, total_nt_before, total_gaps_before, total_ns_before = 0, 0, 0, 0, 0\n",
    "        \n",
    "        # If logging is enabled, calculate and store the initial state of the alignment\n",
    "        if log:\n",
    "            num_seqs_before = len(alignment)\n",
    "            aln_length_before = len(next(iter(alignment.values())) if alignment else 0)\n",
    "            total_nt_before = sum(c.upper() in \"ACGT\" for seq in alignment.values() for c in seq)\n",
    "            total_gaps_before = sum(seq.count(\"-\") for seq in alignment.values())\n",
    "            total_ns_before = sum(c in \"Nn\" for seq in alignment.values() for c in seq)\n",
    "        ### END FIX PART 1 ###\n",
    "\n",
    "\n",
    "        # 3.1 Remove columns with gaps in all leaves\n",
    "        remove_all_gap_columns(alignment)\n",
    "\n",
    "        # 3.2 Trim orphan nucleotides\n",
    "        orphan_log = None\n",
    "        if orphan_method == \"percentile\":\n",
    "            orphan_threshold = calculate_orphan_threshold_from_percentile(alignment, percentile, terminal_only=True)\n",
    "            if log:\n",
    "                alignment, orphan_log = delete_orphan_nucleotides2(alignment, orphan_threshold, log_changes=True)\n",
    "            else:\n",
    "                alignment = delete_orphan_nucleotides2(alignment, orphan_threshold)\n",
    "        elif orphan_method == \"semi\":\n",
    "            if log:\n",
    "                alignment, orphan_log = delete_orphan_nucleotides2(alignment, orphan_threshold, log_changes=True)\n",
    "            else:\n",
    "                alignment = delete_orphan_nucleotides2(alignment, orphan_threshold)\n",
    "\n",
    "\n",
    "        # 3.3 Replace terminal gaps with ?\n",
    "        alignment = replace_terminal_gaps_dict(alignment)\n",
    "\n",
    "        # 3.4 Trim invariant columns\n",
    "        removed_cols = []\n",
    "        if del_inv:\n",
    "            alignment = remove_non_informative_positions(alignment, removed_indices=removed_cols)\n",
    "\n",
    "        alignment = replace_terminal_gaps_dict(alignment)\n",
    "\n",
    "        # 3.5 Replace internal gaps with ?\n",
    "        if internal_method == \"manual\":\n",
    "            alignment = replace_dashes_with_question_marks(alignment=alignment,\n",
    "                                                           internal_column_ranges=internal_column_ranges,\n",
    "                                                           internal_leaves=internal_leaves,\n",
    "                                                           internal_method=\"manual\")\n",
    "        elif internal_method == \"semi\":\n",
    "            alignment = replace_dashes_with_question_marks(alignment=alignment,\n",
    "                                                           internal_leaves=internal_leaves,\n",
    "                                                           internal_method=\"semi\",\n",
    "                                                           internal_threshold=internal_threshold)\n",
    "\n",
    "        # 3.6 Replace ambiguous nucleotides N/n with ?\n",
    "        n_blocks = []\n",
    "        if n2question is not None:\n",
    "            alignment, n_blocks = n2question_func(alignment, leaves=n2question, log=True)\n",
    "\n",
    "\n",
    "        # 3.7 Partitioning\n",
    "        partitioning_log_entry = None\n",
    "\n",
    "        if partitioning_method == \"conservative\" and partitioning_round > 0:\n",
    "            classify_and_insert_hashtags(alignment, partitioning_round=partitioning_round)\n",
    "\n",
    "        elif partitioning_method == \"equal\":\n",
    "            if partitioning_round > 0:\n",
    "                alignment = equal_length_partitioning(alignment=alignment, partitioning_round=partitioning_round, partitioning_size=None, log=False)\n",
    "            elif partitioning_size:\n",
    "                alignment = equal_length_partitioning(alignment=alignment, partitioning_round=None, partitioning_size=partitioning_size, log=False)\n",
    "\n",
    "        elif partitioning_method == \"max\":\n",
    "            alignment = insert_pound_around_questions(alignment)\n",
    "        \n",
    "        elif partitioning_method == \"balanced\":\n",
    "            try:\n",
    "                # Attempt to run the balanced partitioning. This may fail if there are not enough '?' blocks.\n",
    "                processed_alignment = balanced_partitioning(\n",
    "                    alignment,\n",
    "                    log=False,\n",
    "                    partitioning_round=partitioning_round\n",
    "                )\n",
    "                alignment = processed_alignment\n",
    "            except (IndexError, ValueError) as e:\n",
    "                # If it fails, catch the error, prepare a log message, and continue.\n",
    "                warning_message = (\n",
    "                    f\"WARNING: 'balanced' partitioning with partitioning_round={partitioning_round} was skipped for this alignment. \"\n",
    "                    \"This typically happens when the alignment has fewer blocks of missing data ('?') than required. \"\n",
    "                    \"The process will continue without partitioning this file.\"\n",
    "                )\n",
    "                print(f\"\\n{warning_message}\\n\") # Print to console for immediate user feedback.\n",
    "                partitioning_log_entry = warning_message # Save the message for the log file.\n",
    "\n",
    "        refinement_question2hyphen(alignment)\n",
    "        alignment = remove_columns_with_W(alignment)\n",
    "        alignment = remove_adjacent_pound_columns(alignment)\n",
    "\n",
    "\n",
    "        # Step 4: Write output file\n",
    "        records = [SeqRecord(Seq(seq), id=key, description=\"\") for key, seq in alignment.items()]\n",
    "        final_output_path_prefix = output_file\n",
    "        output_directory_for_final_write = os.path.dirname(final_output_path_prefix)\n",
    "        if output_directory_for_final_write and not os.path.exists(output_directory_for_final_write):\n",
    "            os.makedirs(output_directory_for_final_write, exist_ok=True)\n",
    "\n",
    "        output_path = f\"{final_output_path_prefix}_preprocessed.{output_format}\"\n",
    "        with open(output_path, \"w\") as output_handle:\n",
    "            SeqIO.write(records, output_handle, output_format)\n",
    "\n",
    "        # Step 5: Write log (local to this gene/file)\n",
    "        if log:\n",
    "            end_wall_time = time.time()\n",
    "            end_cpu_time = time.process_time()\n",
    "            wall_time = end_wall_time - start_wall_time\n",
    "            cpu_time = end_cpu_time - start_cpu_time\n",
    "\n",
    "            log_path = f\"{final_output_path_prefix}_log.txt\"\n",
    "            with open(log_path, \"w\") as log_file:\n",
    "                log_file.write(\"--- Command used ---\\n\")\n",
    "                log_file.write(f\"{_original_cmd_line}\\n\\n\")\n",
    "\n",
    "                ### FIX: PART 2 - Use the stored \"before\" values for logging ###\n",
    "                log_file.write(\"--- Step 1: Summary before preprocessing ---\\n\")\n",
    "                log_file.write(f\"No. sequences: {num_seqs_before}\\n\")\n",
    "                log_file.write(f\"No. columns: {aln_length_before}\\n\")\n",
    "                log_file.write(f\"Total no. nucleotides (A/C/G/T only): {total_nt_before} bp\\n\")\n",
    "                log_file.write(f\"Total no. gaps (-): {total_gaps_before}\\n\")\n",
    "                log_file.write(f\"Total no. IUPAC N: {total_ns_before}\\n\\n\")\n",
    "                ### END FIX PART 2 ###\n",
    "\n",
    "                if del_inv:\n",
    "                    log_file.write(\"--- Step 2: Trimming (invariant columns) ---\\n\")\n",
    "                    log_file.write(f\"{removed_cols}\\n\\n\")\n",
    "                if orphan_log:\n",
    "                    log_file.write(\"--- Step 2: Trimming (orphan nucleotides) ---\\n\")\n",
    "                    log_file.write(f\"{orphan_log}\\n\\n\")\n",
    "\n",
    "                if n_blocks:\n",
    "                    log_file.write(\"--- Step 3: Missing data identification (Ns replaced with '?') ---\\n\")\n",
    "                    for seq_name, start, end in n_blocks:\n",
    "                        log_file.write(f\"{seq_name}: {start}â€“{end}\\n\")\n",
    "                    log_file.write(\"\\n\")\n",
    "\n",
    "                missing_partition_log = detect_fully_missing_partitions(alignment)\n",
    "                if missing_partition_log:\n",
    "                    log_file.write(\"--- Step 3: Missing data identification (gaps replaced with '?') ---\\n\")\n",
    "                    log_file.write(f\"{missing_partition_log}\\n\\n\")\n",
    "\n",
    "                log_file.write(\"--- Step 4: Partitioning ---\\n\")\n",
    "                if partitioning_log_entry:\n",
    "                    # If partitioning failed, write the stored warning message.\n",
    "                    log_file.write(f\"{partitioning_log_entry}\\n\\n\")\n",
    "                else:\n",
    "                    # Otherwise, report success as usual.\n",
    "                    columns = list(zip(*alignment.values()))\n",
    "                    pound_indices = [i for i, col in enumerate(columns) if '#' in col]\n",
    "                    if pound_indices:\n",
    "                        log_file.write(f\"Method used: {partitioning_method}\")\n",
    "                        if partitioning_method == \"equal\" and partitioning_size:\n",
    "                            log_file.write(f\" (partitioning_size={partitioning_size})\")\n",
    "                        elif partitioning_method in [\"conservative\", \"balanced\"]:\n",
    "                            log_file.write(f\" (partitioning_round={partitioning_round})\")\n",
    "                        elif partitioning_method == \"max\":\n",
    "                            log_file.write(\" (inserted at '?' block boundaries)\")\n",
    "                        log_file.write(\"\\n\")\n",
    "                        log_file.write(f\"Columns with '#' inserted: {pound_indices}\\n\\n\")\n",
    "                    elif partitioning_method and partitioning_method != 'none':\n",
    "                        log_file.write(f\"Method used: {partitioning_method}\\nNo partitions were inserted based on the criteria.\\n\\n\")\n",
    "                    else:\n",
    "                        log_file.write(\"No partitioning method was specified.\\n\\n\")\n",
    "\n",
    "                summary_post = compute_summary_after(alignment)\n",
    "                log_file.write(\"--- Summary after preprocessing ---\\n\")\n",
    "                log_file.write(f\"No. sequences: {summary_post['num_seqs']}\\n\")\n",
    "                log_file.write(f\"No. columns: {summary_post['aln_length']}\\n\")\n",
    "                log_file.write(f\"No. pound sign columns (#): {summary_post['total_pound']}\\n\")\n",
    "                log_file.write(f\"Total no. nucleotides (A/C/G/T): {summary_post['total_nt']} bp\\n\")\n",
    "                log_file.write(f\"Total no. gaps (-): {summary_post['total_gaps']}\\n\")\n",
    "                log_file.write(f\"Total no. IUPAC N: {summary_post['total_ns']}\\n\")\n",
    "                log_file.write(f\"Total no. missing values (?): {summary_post['total_missing']}\\n\\n\")\n",
    "\n",
    "                log_file.write(\"--- Run time ---\\n\")\n",
    "                log_file.write(f\"Wall-clock time: {wall_time:.8f} seconds\\n\")\n",
    "                log_file.write(f\"CPU time: {cpu_time:.8f} seconds\\n\")\n",
    "        \n",
    "        return alignment\n",
    "\n",
    "    # --- Initial call to the recursive helper function (Unchanged) ---\n",
    "    final_processed_alignment = _prepDyn_recursive(input_val=input_file,\n",
    "                                                   GB_input=GB_input,\n",
    "                                                   input_format=input_format,\n",
    "                                                   MSA=MSA,\n",
    "                                                   output_file=output_file,\n",
    "                                                   output_format=output_format,\n",
    "                                                   log=log,\n",
    "                                                   sequence_names=sequence_names,\n",
    "                                                   _all_sequence_ids=_all_sequence_ids_shared,\n",
    "                                                   _is_top_level_call=True,\n",
    "                                                   _original_cmd_line=original_cmd_line,\n",
    "                                                   orphan_method=orphan_method,\n",
    "                                                   orphan_threshold=orphan_threshold,\n",
    "                                                   percentile=percentile,\n",
    "                                                   del_inv=del_inv,\n",
    "                                                   internal_method=internal_method,\n",
    "                                                   internal_column_ranges=internal_column_ranges,\n",
    "                                                   internal_leaves=internal_leaves,\n",
    "                                                   internal_threshold=internal_threshold,\n",
    "                                                   n2question=n2question,\n",
    "                                                   partitioning_round=partitioning_round,\n",
    "                                                   partitioning_method=partitioning_method,\n",
    "                                                   partitioning_size=partitioning_size)\n",
    "\n",
    "    # --- Final sequence_names.txt and overall log writing (Unchanged) ---\n",
    "    if sequence_names:\n",
    "        sorted_unique_names = sorted(list(_all_sequence_ids_shared))\n",
    "        names_file_path = None\n",
    "        if original_output_file_arg:\n",
    "            output_dir_for_final_names = os.path.dirname(original_output_file_arg)\n",
    "            if not output_dir_for_final_names: output_dir_for_final_names = \".\"\n",
    "            output_base_for_final_names = os.path.basename(original_output_file_arg)\n",
    "            if os.path.isdir(original_output_file_arg) and not output_base_for_final_names:\n",
    "                output_base_for_final_names = os.path.basename(os.path.normpath(original_output_file_arg))\n",
    "            names_file_path = os.path.join(output_dir_for_final_names, f\"{output_base_for_final_names}_sequence_names.txt\")\n",
    "        elif GB_input: names_file_path = \"output_sequence_names.txt\"\n",
    "        elif isinstance(input_file, str) and os.path.isdir(input_file):\n",
    "            base_for_names = os.path.basename(os.path.normpath(input_file))\n",
    "            names_file_path = f\"{base_for_names}_sequence_names.txt\"\n",
    "        elif isinstance(input_file, str) and not os.path.isdir(input_file):\n",
    "            base_for_names = os.path.splitext(os.path.basename(input_file))[0]\n",
    "            names_file_path = f\"{base_for_names}_sequence_names.txt\"\n",
    "        else: names_file_path = \"alignment_sequence_names.txt\"\n",
    "        \n",
    "        if names_file_path:\n",
    "            os.makedirs(os.path.dirname(names_file_path), exist_ok=True)\n",
    "            with open(names_file_path, 'w') as nf:\n",
    "                for name in sorted_unique_names:\n",
    "                    nf.write(f\"{name}\\n\")\n",
    "\n",
    "    if log and (GB_input is not None or (isinstance(input_file, str) and os.path.isdir(input_file))):\n",
    "        overall_end_wall_time = time.time()\n",
    "        overall_end_cpu_time = time.process_time()\n",
    "        total_wall_time = overall_end_wall_time - overall_start_wall_time\n",
    "        total_cpu_time = overall_end_cpu_time - overall_start_cpu_time\n",
    "\n",
    "        overall_log_path = None\n",
    "        if original_output_file_arg:\n",
    "            output_dir_for_overall_log = os.path.dirname(original_output_file_arg)\n",
    "            if not output_dir_for_overall_log: output_dir_for_overall_log = \".\"\n",
    "            output_base_for_overall_log = os.path.basename(original_output_file_arg)\n",
    "            if os.path.isdir(original_output_file_arg) and not output_base_for_overall_log:\n",
    "                output_base_for_overall_log = os.path.basename(os.path.normpath(original_output_file_arg))\n",
    "            overall_log_path = os.path.join(output_dir_for_overall_log, f\"{output_base_for_overall_log}_overall_log.txt\")\n",
    "        elif GB_input: overall_log_path = \"overall_prepDyn_log.txt\"\n",
    "        elif isinstance(input_file, str) and os.path.isdir(input_file):\n",
    "            base_for_overall_log = os.path.basename(os.path.normpath(input_file))\n",
    "            overall_log_path = f\"{base_for_overall_log}_overall_log.txt\"\n",
    "        \n",
    "        if overall_log_path:\n",
    "            os.makedirs(os.path.dirname(overall_log_path), exist_ok=True)\n",
    "            with open(overall_log_path, 'w') as of:\n",
    "                of.write(\"--- Overall prepDyn Execution Summary ---\\n\")\n",
    "                of.write(f\"Command used:\\n{original_cmd_line}\\n\\n\")\n",
    "                of.write(f\"Total Wall-clock time: {total_wall_time:.8f} seconds\\n\")\n",
    "                of.write(f\"Total CPU time: {total_cpu_time:.8f} seconds\\n\")\n",
    "                of.write(\"\\nNote: Individual gene/alignment logs provide detailed information.\\n\")\n",
    "\n",
    "    if GB_input or (isinstance(input_file, str) and os.path.isdir(input_file)):\n",
    "        return None \n",
    "    else:\n",
    "        return final_processed_alignment\n",
    "\n",
    "###########\n",
    "# Example #\n",
    "###########\n",
    "alignment = {\n",
    "    'sp1': \"TGCACCGT??????CAGTAGTCCTCCACCGTCGCCACCGTCNC????????\",\n",
    "    'sp2': \"TGCACCGTCGCCAACAGTAGTCCTCCACCGTCGCCNNNNNNTCCAACA??G\",\n",
    "    'sp3': \"TG-ACCGTCGCCAACAGTAGTCCTCCACCGTCGCCACCGT----AA----G\",\n",
    "    'sp4': \"--C-CCGTCGCCNNNNNNNNNNNNNNNNNNTCGCNACCGTCGCCAACATTG\",\n",
    "    'sp5': \"TACA-CGTCGCCAACAGTAGTCCTCCACCGTCGCCACCGTCGCCAACAGTG\",\n",
    "    'sp6': \"--CAC-GTCGCCAACAGT---------CCGTCGCCACCGTCGCCA----TG\",\n",
    "} \n",
    "\n",
    "# Convert to SeqRecord list\n",
    "records = [SeqRecord(Seq(seq), id=name, description=\"\") for name, seq in alignment.items()]\n",
    "# Write to FASTA\n",
    "with open(\"debug_data/prepDyn_debug2.fasta\", \"w\") as output_handle:\n",
    "    SeqIO.write(records, output_handle, \"fasta\")\n",
    "\n",
    "'''\n",
    "# Debug for a single input_file\n",
    "prepDyn(input_file= \"../test_data/tutorial/ex1.2/ex1.2_input.fasta\",\n",
    "        output_file=\"../test_data/tutorial/ex1.2/ex1.2\",\n",
    "        orphan_method=\"semi\", orphan_threshold=3,\n",
    "        internal_method=\"semi\", internal_threshold=8,\n",
    "        del_inv=True,\n",
    "        partitioning_method=\"balanced\", partitioning_round=1,\n",
    "        #partitioning_size=10,\n",
    "        log=True\n",
    "        )\n",
    "'''\n",
    "\n",
    "'''\n",
    "# Debug for multiple input_file\n",
    "prepDyn(input_file= \"../test_data/tutorial/ex3.1/\",\n",
    "        output_file=\"../test_data/tutorial/ex3.1/out\",\n",
    "        MSA=True,\n",
    "        orphan_method=\"semi\", orphan_threshold=3,\n",
    "        internal_method=\"semi\", internal_threshold=8,\n",
    "        del_inv=True,\n",
    "        partitioning_method=\"max\", \n",
    "        #partitioning_round=2,\n",
    "        #partitioning_size=10,\n",
    "        log=True,\n",
    "        n2question='all')\n",
    "'''\n",
    "\n",
    "'''\n",
    "# Debug when GB_input is used\n",
    "prepDyn(GB_input= \"../test_data/tutorial/ex1.1/ex1.1_input.csv\",\n",
    "        output_file=\"../test_data/tutorial/ex1.1/ex1.1\",\n",
    "        orphan_method=\"semi\", orphan_threshold=20,\n",
    "        internal_method=\"semi\", internal_threshold=10,\n",
    "        del_inv=True,\n",
    "        partitioning_method=\"max\", \n",
    "        #partitioning_round=3,\n",
    "        #partitioning_size=100,\n",
    "        log=True, sequence_names=True,\n",
    "        n2question='all')\n",
    "'''\n",
    "# Command in bash to debug\n",
    "# python src/prepDyn.py --input_file jupyter/debug_data/prepDyn_debug3/ --output_file jupyter/debug_data/prepDyn_debug3/out --orphan_method semi --orphan_threshold 2 --del_inv --partitioning_method equal --partitioning_size 10 --log --n2question all --internal_method semi --internal_threshold 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "021aa84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Debug tests: GB2MSA\\nGB2MSA(input_file = \"debug_data/v5_GenBank_prepDyn.csv\", \\n       output_prefix=\"debug_data/v5_TEST\",\\n       delimiter=\\',\\',write_names=True,log=False)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Debug tests: GB2MSA\n",
    "GB2MSA(input_file = \"debug_data/v5_GenBank_prepDyn.csv\", \n",
    "       output_prefix=\"debug_data/v5_TEST\",\n",
    "       delimiter=',',write_names=True,log=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "880467d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Debug tests: addSeq\\nalignment = {\\n    \\'sp1\\': \"TGCACCGTCGCC#AACAGTAGTCCTCCACCG#AAGTTACCGTCNCCAACAG??\",\\n    \\'sp2\\': \"T-CACCGTCGCC#AACAGTAGTCCTCCACCG#AAGTTNNNNNNTCCAACA???\",\\n    \\'sp3\\': \"TG-ACCGTCGCC#AACAGTAGTCCTCCACCG#AAGTTACCGTCGCCAA-----\",\\n    \\'sp4\\': \"TGC-CCGTCGCC#NNNNNNNNNNNNNNNNNN#AAGTTACCGTCGCCAACATTT\",\\n    \\'sp5\\': \"TGCA-CGTCGCC#AACAGTAGTCCTCCACCG#AAGTTACCGTCGCCAACAGTG\",\\n    \\'sp6\\': \"TGCAC-GTCGCC#AACAGTAGTCCTCCACCG#AAGTTACCGTCGCCAACAGTA\",\\n} \\nnew_seqs = {\\n    \\'sp7\\': \"TGCACCGTCGCCAACAGTAGTCCTCCACCGTCG\",\\n    \\'sp8\\': \"TGCACCGTCGCCAACAGTAGTCCTCCACCGAAGCAACAGTA\"\\n}\\n\\n#addSeq(alignment, new_seqs, \\n#       output=\"debug_data/addSeq_debug3_output.fas\", \\n#       orphan_threshold=0,\\n#       write_names=False)\\n\\naddSeq(alignment=\"debug_data/ex4.3_aln.fas\",\\n       new_seqs=\"debug_data/ex4.3_new_seqs.fas\",\\n       output=\"debug_data/ex4.3_out.fas\",\\n       #orphan_threshold=0, \\n       #n2question=\"Thoropa_miliaris_CFBH10125\",\\n       #gaps2question=25,\\n       #write_names=True,\\n       log=True)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Debug tests: addSeq\n",
    "alignment = {\n",
    "    'sp1': \"TGCACCGTCGCC#AACAGTAGTCCTCCACCG#AAGTTACCGTCNCCAACAG??\",\n",
    "    'sp2': \"T-CACCGTCGCC#AACAGTAGTCCTCCACCG#AAGTTNNNNNNTCCAACA???\",\n",
    "    'sp3': \"TG-ACCGTCGCC#AACAGTAGTCCTCCACCG#AAGTTACCGTCGCCAA-----\",\n",
    "    'sp4': \"TGC-CCGTCGCC#NNNNNNNNNNNNNNNNNN#AAGTTACCGTCGCCAACATTT\",\n",
    "    'sp5': \"TGCA-CGTCGCC#AACAGTAGTCCTCCACCG#AAGTTACCGTCGCCAACAGTG\",\n",
    "    'sp6': \"TGCAC-GTCGCC#AACAGTAGTCCTCCACCG#AAGTTACCGTCGCCAACAGTA\",\n",
    "} \n",
    "new_seqs = {\n",
    "    'sp7': \"TGCACCGTCGCCAACAGTAGTCCTCCACCGTCG\",\n",
    "    'sp8': \"TGCACCGTCGCCAACAGTAGTCCTCCACCGAAGCAACAGTA\"\n",
    "}\n",
    "\n",
    "#addSeq(alignment, new_seqs, \n",
    "#       output=\"debug_data/addSeq_debug3_output.fas\", \n",
    "#       orphan_threshold=0,\n",
    "#       write_names=False)\n",
    "\n",
    "addSeq(alignment=\"debug_data/ex4.3_aln.fas\",\n",
    "       new_seqs=\"debug_data/ex4.3_new_seqs.fas\",\n",
    "       output=\"debug_data/ex4.3_out.fas\",\n",
    "       #orphan_threshold=0, \n",
    "       #n2question=\"Thoropa_miliaris_CFBH10125\",\n",
    "       #gaps2question=25,\n",
    "       #write_names=True,\n",
    "       log=True)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331357c5",
   "metadata": {},
   "source": [
    "## Time complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc271c21",
   "metadata": {},
   "source": [
    "To understand time complexity, I initially simulated six trees with different number of leaves (10, 20, 40, 80). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac13aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   n_leaves  n_characters\n",
      "0        10             0\n",
      "1        20             0\n",
      "2        40             0\n",
      "3        80             0\n",
      "4       160             0\n",
      "5       320             0\n",
      "Generating tree with 10 leaves...\n",
      "Saved t10.nwk\n",
      "Generating tree with 20 leaves...\n",
      "Saved t20.nwk\n",
      "Generating tree with 40 leaves...\n",
      "Saved t40.nwk\n",
      "Generating tree with 80 leaves...\n",
      "Saved t80.nwk\n",
      "Generating tree with 160 leaves...\n",
      "Saved t160.nwk\n",
      "Generating tree with 320 leaves...\n",
      "Saved t320.nwk\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ngesh\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "# Define function to randomly sample values of parameters of simulations using uniform distributions\n",
    "def sample_phylo_parameters(\n",
    "    leaves=[10, 20, 40, 80],\n",
    "    characters=[100, 1000, 10000]\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a dataframe of all combinations of number of leaves and characters.\n",
    "\n",
    "    Parameters:\n",
    "    - leaves (list): possible values for number of taxa\n",
    "    - characters (list): possible values for number of alignment sites\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame with columns:\n",
    "        - 'n_leaves': one of the values in `leaves`\n",
    "        - 'n_characters': one of the values in `characters`\n",
    "    \"\"\"\n",
    "    \n",
    "    combinations = list(itertools.product(leaves, characters))\n",
    "    df = pd.DataFrame(combinations, columns=[\"n_leaves\", \"n_characters\"])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Simulate parameters\n",
    "simulations_parameters = sample_phylo_parameters(leaves=[10, 20, 40, 80], \n",
    "                                                 characters=[0]) # ignore it\n",
    "print(simulations_parameters)\n",
    "\n",
    "# Define function to simulate trees from the values of column n_leaves\n",
    "def simulate_and_save_trees(simulations_parameters, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for i, row in simulations_parameters.iterrows():\n",
    "        n_leaves = row[\"n_leaves\"]\n",
    "        print(f\"Generating tree with {n_leaves} leaves...\")\n",
    "\n",
    "        tree = ngesh.gen_tree(num_leaves=n_leaves, seed=44)\n",
    "        newick = tree.write(format=5)\n",
    "\n",
    "        filename = f\"t{n_leaves}.nwk\"\n",
    "        file_path = os.path.join(output_dir, filename)\n",
    "\n",
    "        with open(file_path, \"w\") as f:\n",
    "            f.write(newick + \"\\n\")\n",
    "\n",
    "        print(f\"Saved {filename}\")\n",
    "\n",
    "# Simulate trees\n",
    "simulate_and_save_trees(simulations_parameters, output_dir=\"../test_data/simulations/\")\n",
    "'''    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c7d33e",
   "metadata": {},
   "source": [
    "The four trees are written locally. Now, I need to simulate DNA alignments based on these trees using IQ-TREE 3 (number of characters = 100, 1000, 10000), generating a combination of 12 data sets (4 x 3). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941507c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: iqtree3 --alisim t10_len100.fasta -m JC -t ../test_data/simulations/t10.nwk --indel 0,0.0005 --length 100 --out-format fasta -seed 44\n",
      "âœ… Success: t10_len100.fasta.unaligned.fa\n",
      "Running: iqtree3 --alisim t10_len1000.fasta -m JC -t ../test_data/simulations/t10.nwk --indel 0,0.0005 --length 1000 --out-format fasta -seed 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(60311) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(60312) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(60313) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Success: t10_len1000.fasta.unaligned.fa\n",
      "Running: iqtree3 --alisim t10_len10000.fasta -m JC -t ../test_data/simulations/t10.nwk --indel 0,0.0005 --length 10000 --out-format fasta -seed 44\n",
      "âœ… Success: t10_len10000.fasta.unaligned.fa\n",
      "Running: iqtree3 --alisim t160_len100.fasta -m JC -t ../test_data/simulations/t160.nwk --indel 0,0.0005 --length 100 --out-format fasta -seed 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(60314) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(60315) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Success: t160_len100.fasta.unaligned.fa\n",
      "Running: iqtree3 --alisim t160_len1000.fasta -m JC -t ../test_data/simulations/t160.nwk --indel 0,0.0005 --length 1000 --out-format fasta -seed 44\n",
      "âœ… Success: t160_len1000.fasta.unaligned.fa\n",
      "Running: iqtree3 --alisim t160_len10000.fasta -m JC -t ../test_data/simulations/t160.nwk --indel 0,0.0005 --length 10000 --out-format fasta -seed 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(60316) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Success: t160_len10000.fasta.unaligned.fa\n",
      "Running: iqtree3 --alisim t20_len100.fasta -m JC -t ../test_data/simulations/t20.nwk --indel 0,0.0005 --length 100 --out-format fasta -seed 44\n",
      "âœ… Success: t20_len100.fasta.unaligned.fa\n",
      "Running: iqtree3 --alisim t20_len1000.fasta -m JC -t ../test_data/simulations/t20.nwk --indel 0,0.0005 --length 1000 --out-format fasta -seed 44\n",
      "âœ… Success: t20_len1000.fasta.unaligned.fa\n",
      "Running: iqtree3 --alisim t20_len10000.fasta -m JC -t ../test_data/simulations/t20.nwk --indel 0,0.0005 --length 10000 --out-format fasta -seed 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(60320) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(60322) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(60323) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Success: t20_len10000.fasta.unaligned.fa\n",
      "Running: iqtree3 --alisim t320_len100.fasta -m JC -t ../test_data/simulations/t320.nwk --indel 0,0.0005 --length 100 --out-format fasta -seed 44\n",
      "âœ… Success: t320_len100.fasta.unaligned.fa\n",
      "Running: iqtree3 --alisim t320_len1000.fasta -m JC -t ../test_data/simulations/t320.nwk --indel 0,0.0005 --length 1000 --out-format fasta -seed 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(60324) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(60325) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Success: t320_len1000.fasta.unaligned.fa\n",
      "Running: iqtree3 --alisim t320_len10000.fasta -m JC -t ../test_data/simulations/t320.nwk --indel 0,0.0005 --length 10000 --out-format fasta -seed 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(60327) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Success: t320_len10000.fasta.unaligned.fa\n",
      "Running: iqtree3 --alisim t40_len100.fasta -m JC -t ../test_data/simulations/t40.nwk --indel 0,0.0005 --length 100 --out-format fasta -seed 44\n",
      "âœ… Success: t40_len100.fasta.unaligned.fa\n",
      "Running: iqtree3 --alisim t40_len1000.fasta -m JC -t ../test_data/simulations/t40.nwk --indel 0,0.0005 --length 1000 --out-format fasta -seed 44\n",
      "âœ… Success: t40_len1000.fasta.unaligned.fa\n",
      "Running: iqtree3 --alisim t40_len10000.fasta -m JC -t ../test_data/simulations/t40.nwk --indel 0,0.0005 --length 10000 --out-format fasta -seed 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(60333) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(60334) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(60339) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Success: t40_len10000.fasta.unaligned.fa\n",
      "Running: iqtree3 --alisim t80_len100.fasta -m JC -t ../test_data/simulations/t80.nwk --indel 0,0.0005 --length 100 --out-format fasta -seed 44\n",
      "âœ… Success: t80_len100.fasta.unaligned.fa\n",
      "Running: iqtree3 --alisim t80_len1000.fasta -m JC -t ../test_data/simulations/t80.nwk --indel 0,0.0005 --length 1000 --out-format fasta -seed 44\n",
      "âœ… Success: t80_len1000.fasta.unaligned.fa\n",
      "Running: iqtree3 --alisim t80_len10000.fasta -m JC -t ../test_data/simulations/t80.nwk --indel 0,0.0005 --length 10000 --out-format fasta -seed 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(60340) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(60341) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(60342) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Success: t80_len10000.fasta.unaligned.fa\n",
      "ðŸ“¦ Moved t10_len10000.fasta.unaligned.fa â†’ ../test_data/simulations/1_unaligned/t10_len10000.fasta.unaligned.fa\n",
      "ðŸ“¦ Moved t80_len1000.fasta.unaligned.fa â†’ ../test_data/simulations/1_unaligned/t80_len1000.fasta.unaligned.fa\n",
      "ðŸ“¦ Moved t80_len1000.fasta.fa â†’ ../test_data/simulations/2_aligned/t80_len1000.fasta.fa\n",
      "ðŸ“¦ Moved t20_len100.fasta.fa â†’ ../test_data/simulations/2_aligned/t20_len100.fasta.fa\n",
      "ðŸ“¦ Moved t20_len1000.fasta.unaligned.fa â†’ ../test_data/simulations/1_unaligned/t20_len1000.fasta.unaligned.fa\n",
      "ðŸ“¦ Moved t160_len10000.fasta.unaligned.fa â†’ ../test_data/simulations/1_unaligned/t160_len10000.fasta.unaligned.fa\n",
      "ðŸ“¦ Moved t20_len10000.fasta.unaligned.fa â†’ ../test_data/simulations/1_unaligned/t20_len10000.fasta.unaligned.fa\n",
      "ðŸ“¦ Moved t160_len1000.fasta.fa â†’ ../test_data/simulations/2_aligned/t160_len1000.fasta.fa\n",
      "ðŸ“¦ Moved t320_len10000.fasta.unaligned.fa â†’ ../test_data/simulations/1_unaligned/t320_len10000.fasta.unaligned.fa\n",
      "ðŸ“¦ Moved t320_len10000.fasta.fa â†’ ../test_data/simulations/2_aligned/t320_len10000.fasta.fa\n",
      "ðŸ“¦ Moved t10_len100.fasta.unaligned.fa â†’ ../test_data/simulations/1_unaligned/t10_len100.fasta.unaligned.fa\n",
      "ðŸ“¦ Moved t40_len10000.fasta.unaligned.fa â†’ ../test_data/simulations/1_unaligned/t40_len10000.fasta.unaligned.fa\n",
      "ðŸ“¦ Moved t320_len1000.fasta.unaligned.fa â†’ ../test_data/simulations/1_unaligned/t320_len1000.fasta.unaligned.fa\n",
      "ðŸ“¦ Moved t80_len100.fasta.fa â†’ ../test_data/simulations/2_aligned/t80_len100.fasta.fa\n",
      "ðŸ“¦ Moved t80_len10000.fasta.fa â†’ ../test_data/simulations/2_aligned/t80_len10000.fasta.fa\n",
      "ðŸ“¦ Moved t80_len10000.fasta.unaligned.fa â†’ ../test_data/simulations/1_unaligned/t80_len10000.fasta.unaligned.fa\n",
      "ðŸ“¦ Moved t40_len100.fasta.fa â†’ ../test_data/simulations/2_aligned/t40_len100.fasta.fa\n",
      "ðŸ“¦ Moved t40_len10000.fasta.fa â†’ ../test_data/simulations/2_aligned/t40_len10000.fasta.fa\n",
      "ðŸ“¦ Moved t20_len1000.fasta.fa â†’ ../test_data/simulations/2_aligned/t20_len1000.fasta.fa\n",
      "ðŸ“¦ Moved t80_len100.fasta.unaligned.fa â†’ ../test_data/simulations/1_unaligned/t80_len100.fasta.unaligned.fa\n",
      "ðŸ“¦ Moved t40_len100.fasta.unaligned.fa â†’ ../test_data/simulations/1_unaligned/t40_len100.fasta.unaligned.fa\n",
      "ðŸ“¦ Moved t320_len100.fasta.fa â†’ ../test_data/simulations/2_aligned/t320_len100.fasta.fa\n",
      "ðŸ“¦ Moved t160_len1000.fasta.unaligned.fa â†’ ../test_data/simulations/1_unaligned/t160_len1000.fasta.unaligned.fa\n",
      "ðŸ“¦ Moved t40_len1000.fasta.unaligned.fa â†’ ../test_data/simulations/1_unaligned/t40_len1000.fasta.unaligned.fa\n",
      "ðŸ“¦ Moved t160_len100.fasta.fa â†’ ../test_data/simulations/2_aligned/t160_len100.fasta.fa\n",
      "ðŸ“¦ Moved t160_len100.fasta.unaligned.fa â†’ ../test_data/simulations/1_unaligned/t160_len100.fasta.unaligned.fa\n",
      "ðŸ“¦ Moved t320_len1000.fasta.fa â†’ ../test_data/simulations/2_aligned/t320_len1000.fasta.fa\n",
      "ðŸ“¦ Moved t10_len1000.fasta.unaligned.fa â†’ ../test_data/simulations/1_unaligned/t10_len1000.fasta.unaligned.fa\n",
      "ðŸ“¦ Moved t10_len10000.fasta.fa â†’ ../test_data/simulations/2_aligned/t10_len10000.fasta.fa\n",
      "ðŸ“¦ Moved t20_len10000.fasta.fa â†’ ../test_data/simulations/2_aligned/t20_len10000.fasta.fa\n",
      "ðŸ“¦ Moved t320_len100.fasta.unaligned.fa â†’ ../test_data/simulations/1_unaligned/t320_len100.fasta.unaligned.fa\n",
      "ðŸ“¦ Moved t160_len10000.fasta.fa â†’ ../test_data/simulations/2_aligned/t160_len10000.fasta.fa\n",
      "ðŸ“¦ Moved t40_len1000.fasta.fa â†’ ../test_data/simulations/2_aligned/t40_len1000.fasta.fa\n",
      "ðŸ“¦ Moved t10_len100.fasta.fa â†’ ../test_data/simulations/2_aligned/t10_len100.fasta.fa\n",
      "ðŸ“¦ Moved t20_len100.fasta.unaligned.fa â†’ ../test_data/simulations/1_unaligned/t20_len100.fasta.unaligned.fa\n",
      "ðŸ“¦ Moved t10_len1000.fasta.fa â†’ ../test_data/simulations/2_aligned/t10_len1000.fasta.fa\n",
      "âœ… Renamed ../test_data/simulations/2_aligned/t80_len1000.fasta.fa â†’ ../test_data/simulations/2_aligned/t80_len1000.fasta\n",
      "âœ… Renamed ../test_data/simulations/2_aligned/t20_len100.fasta.fa â†’ ../test_data/simulations/2_aligned/t20_len100.fasta\n",
      "âœ… Renamed ../test_data/simulations/2_aligned/t160_len1000.fasta.fa â†’ ../test_data/simulations/2_aligned/t160_len1000.fasta\n",
      "âœ… Renamed ../test_data/simulations/2_aligned/t320_len10000.fasta.fa â†’ ../test_data/simulations/2_aligned/t320_len10000.fasta\n",
      "âœ… Renamed ../test_data/simulations/2_aligned/t80_len100.fasta.fa â†’ ../test_data/simulations/2_aligned/t80_len100.fasta\n",
      "âœ… Renamed ../test_data/simulations/2_aligned/t80_len10000.fasta.fa â†’ ../test_data/simulations/2_aligned/t80_len10000.fasta\n",
      "âœ… Renamed ../test_data/simulations/2_aligned/t40_len100.fasta.fa â†’ ../test_data/simulations/2_aligned/t40_len100.fasta\n",
      "âœ… Renamed ../test_data/simulations/2_aligned/t40_len10000.fasta.fa â†’ ../test_data/simulations/2_aligned/t40_len10000.fasta\n",
      "âœ… Renamed ../test_data/simulations/2_aligned/t20_len1000.fasta.fa â†’ ../test_data/simulations/2_aligned/t20_len1000.fasta\n",
      "âœ… Renamed ../test_data/simulations/2_aligned/t320_len100.fasta.fa â†’ ../test_data/simulations/2_aligned/t320_len100.fasta\n",
      "âœ… Renamed ../test_data/simulations/2_aligned/t160_len100.fasta.fa â†’ ../test_data/simulations/2_aligned/t160_len100.fasta\n",
      "âœ… Renamed ../test_data/simulations/2_aligned/t320_len1000.fasta.fa â†’ ../test_data/simulations/2_aligned/t320_len1000.fasta\n",
      "âœ… Renamed ../test_data/simulations/2_aligned/t10_len10000.fasta.fa â†’ ../test_data/simulations/2_aligned/t10_len10000.fasta\n",
      "âœ… Renamed ../test_data/simulations/2_aligned/t20_len10000.fasta.fa â†’ ../test_data/simulations/2_aligned/t20_len10000.fasta\n",
      "âœ… Renamed ../test_data/simulations/2_aligned/t160_len10000.fasta.fa â†’ ../test_data/simulations/2_aligned/t160_len10000.fasta\n",
      "âœ… Renamed ../test_data/simulations/2_aligned/t40_len1000.fasta.fa â†’ ../test_data/simulations/2_aligned/t40_len1000.fasta\n",
      "âœ… Renamed ../test_data/simulations/2_aligned/t10_len100.fasta.fa â†’ ../test_data/simulations/2_aligned/t10_len100.fasta\n",
      "âœ… Renamed ../test_data/simulations/2_aligned/t10_len1000.fasta.fa â†’ ../test_data/simulations/2_aligned/t10_len1000.fasta\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "import subprocess\n",
    "import re\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "def simulate_alignments_from_trees(\n",
    "    tree_dir=\"../test_data/simulations/\",\n",
    "    lengths=[100, 1000, 10000],\n",
    "    model=\"JC\",\n",
    "    indel=\"0,0.0005\",\n",
    "    out_format=\"fasta\",\n",
    "    seed=44\n",
    "):\n",
    "    # Match files like t10.nwk, t20.nwk, etc.\n",
    "    tree_files = [f for f in os.listdir(tree_dir) if re.match(r\"t\\d+\\.nwk$\", f)]\n",
    "    tree_files.sort()\n",
    "\n",
    "    for tree_file in tree_files:\n",
    "        tree_path = os.path.join(tree_dir, tree_file)\n",
    "        n_leaves = re.findall(r\"t(\\d+)\\.nwk\", tree_file)[0]  # extract 10, 20, 40...\n",
    "\n",
    "        for length in lengths:\n",
    "            prefix = f\"t{n_leaves}_len{length}\"\n",
    "            alisim_arg = f\"{prefix}.fasta\"\n",
    "            expected_output = f\"{alisim_arg}.unaligned.fa\"  # IQ-TREE's default output\n",
    "\n",
    "            cmd = [\n",
    "                \"iqtree3\",\n",
    "                \"--alisim\", alisim_arg,\n",
    "                \"-m\", model,\n",
    "                \"-t\", tree_path,\n",
    "                \"--indel\", indel,\n",
    "                \"--length\", str(length),\n",
    "                \"--out-format\", out_format,\n",
    "                \"-seed\", str(seed)\n",
    "            ]\n",
    "\n",
    "            print(\"Running:\", \" \".join(cmd))\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "\n",
    "            if result.returncode != 0:\n",
    "                print(f\"Error for tree {tree_file} with length {length}:\")\n",
    "                print(result.stderr)\n",
    "                continue\n",
    "\n",
    "            if os.path.exists(expected_output):\n",
    "                print(f\"Success: {expected_output}\")\n",
    "            else:\n",
    "                print(f\"Output missing: {expected_output}\")\n",
    "                print(result.stdout)\n",
    "\n",
    "# Run the simulation\n",
    "simulate_alignments_from_trees()\n",
    "\n",
    "# Organize output files\n",
    "unaligned_dir = \"../test_data/simulations/1_unaligned\"\n",
    "aligned_dir = \"../test_data/simulations/2_aligned\"\n",
    "os.makedirs(unaligned_dir, exist_ok=True)\n",
    "os.makedirs(aligned_dir, exist_ok=True)\n",
    "\n",
    "# Move .fa files into the appropriate subdirectories\n",
    "for file in glob.glob(\"*.fa\"):\n",
    "    if \"unaligned\" in file:\n",
    "        dest_path = os.path.join(unaligned_dir, file)\n",
    "    elif file.endswith(\"fasta.fa\"):\n",
    "        dest_path = os.path.join(aligned_dir, file)\n",
    "    else:\n",
    "        print(f\"Skipped file (no match): {file}\")\n",
    "        continue\n",
    "\n",
    "    shutil.move(file, dest_path)\n",
    "    print(f\"Moved {file} â†’ {dest_path}\")\n",
    "\n",
    "# Rename *.fasta.fa â†’ *.fasta for use in PrepDyn\n",
    "for filepath in glob.glob(os.path.join(aligned_dir, \"*fasta.fa\")):\n",
    "    new_filepath = filepath[:-3]  # remove the \".fa\"\n",
    "    os.rename(filepath, new_filepath)\n",
    "    print(f\"Renamed {filepath} â†’ {new_filepath}\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82cd4dc",
   "metadata": {},
   "source": [
    "Now I have 12 alignments (ignore the 12 unaligned files). For each data set, let's run prepDyn to insert variable number of pound sign columns (0, 1, 2, 4, 8), resulting in 60 data sets.\n",
    "\n",
    "- Number of leaves = 10, 20, 40, 80\n",
    "- Number of columns = 100, 1000, 10000\n",
    "- Number of partitions = 0, 1, 2, 4, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54326635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t10_len10000.fasta\n",
      "DEBUG: Calling recursive for t10_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.1_noPartition/3.1_t10_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.1_noPartition/3.1_t10_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.1_noPartition/3.1_t10_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t80_len10000.fasta\n",
      "DEBUG: Calling recursive for t80_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.1_noPartition/3.1_t80_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.1_noPartition/3.1_t80_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.1_noPartition/3.1_t80_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t320_len1000.fasta\n",
      "DEBUG: Calling recursive for t320_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.1_noPartition/3.1_t320_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.1_noPartition/3.1_t320_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.1_noPartition/3.1_t320_len1000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t40_len100.fasta\n",
      "DEBUG: Calling recursive for t40_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.1_noPartition/3.1_t40_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.1_noPartition/3.1_t40_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.1_noPartition/3.1_t40_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t160_len10000.fasta\n",
      "DEBUG: Calling recursive for t160_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.1_noPartition/3.1_t160_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.1_noPartition/3.1_t160_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.1_noPartition/3.1_t160_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t20_len1000.fasta\n",
      "DEBUG: Calling recursive for t20_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.1_noPartition/3.1_t20_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.1_noPartition/3.1_t20_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.1_noPartition/3.1_t20_len1000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t320_len10000.fasta\n",
      "DEBUG: Calling recursive for t320_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.1_noPartition/3.1_t320_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.1_noPartition/3.1_t320_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.1_noPartition/3.1_t320_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t10_len100.fasta\n",
      "DEBUG: Calling recursive for t10_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.1_noPartition/3.1_t10_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.1_noPartition/3.1_t10_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.1_noPartition/3.1_t10_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t20_len100.fasta\n",
      "DEBUG: Calling recursive for t20_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.1_noPartition/3.1_t20_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.1_noPartition/3.1_t20_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.1_noPartition/3.1_t20_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t20_len10000.fasta\n",
      "DEBUG: Calling recursive for t20_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.1_noPartition/3.1_t20_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.1_noPartition/3.1_t20_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.1_noPartition/3.1_t20_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t40_len1000.fasta\n",
      "DEBUG: Calling recursive for t40_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.1_noPartition/3.1_t40_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.1_noPartition/3.1_t40_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.1_noPartition/3.1_t40_len1000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t320_len100.fasta\n",
      "DEBUG: Calling recursive for t320_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.1_noPartition/3.1_t320_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.1_noPartition/3.1_t320_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.1_noPartition/3.1_t320_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t10_len1000.fasta\n",
      "DEBUG: Calling recursive for t10_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.1_noPartition/3.1_t10_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.1_noPartition/3.1_t10_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.1_noPartition/3.1_t10_len1000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t160_len100.fasta\n",
      "DEBUG: Calling recursive for t160_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.1_noPartition/3.1_t160_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.1_noPartition/3.1_t160_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.1_noPartition/3.1_t160_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t80_len100.fasta\n",
      "DEBUG: Calling recursive for t80_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.1_noPartition/3.1_t80_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.1_noPartition/3.1_t80_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.1_noPartition/3.1_t80_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t40_len10000.fasta\n",
      "DEBUG: Calling recursive for t40_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.1_noPartition/3.1_t40_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.1_noPartition/3.1_t40_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.1_noPartition/3.1_t40_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t160_len1000.fasta\n",
      "DEBUG: Calling recursive for t160_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.1_noPartition/3.1_t160_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.1_noPartition/3.1_t160_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.1_noPartition/3.1_t160_len1000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t80_len1000.fasta\n",
      "DEBUG: Calling recursive for t80_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.1_noPartition/3.1_t80_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.1_noPartition/3.1_t80_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.1_noPartition/3.1_t80_len1000_log.txt\n",
      "Preprocessing complete for all files in the directory/GenBank input.\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t10_len10000.fasta\n",
      "DEBUG: Calling recursive for t10_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.2_elPartition1/3.2_t10_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.2_elPartition1/3.2_t10_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.2_elPartition1/3.2_t10_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t80_len10000.fasta\n",
      "DEBUG: Calling recursive for t80_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.2_elPartition1/3.2_t80_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.2_elPartition1/3.2_t80_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.2_elPartition1/3.2_t80_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t320_len1000.fasta\n",
      "DEBUG: Calling recursive for t320_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.2_elPartition1/3.2_t320_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.2_elPartition1/3.2_t320_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.2_elPartition1/3.2_t320_len1000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t40_len100.fasta\n",
      "DEBUG: Calling recursive for t40_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.2_elPartition1/3.2_t40_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.2_elPartition1/3.2_t40_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.2_elPartition1/3.2_t40_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t160_len10000.fasta\n",
      "DEBUG: Calling recursive for t160_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.2_elPartition1/3.2_t160_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.2_elPartition1/3.2_t160_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.2_elPartition1/3.2_t160_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t20_len1000.fasta\n",
      "DEBUG: Calling recursive for t20_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.2_elPartition1/3.2_t20_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.2_elPartition1/3.2_t20_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.2_elPartition1/3.2_t20_len1000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t320_len10000.fasta\n",
      "DEBUG: Calling recursive for t320_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.2_elPartition1/3.2_t320_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.2_elPartition1/3.2_t320_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.2_elPartition1/3.2_t320_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t10_len100.fasta\n",
      "DEBUG: Calling recursive for t10_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.2_elPartition1/3.2_t10_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.2_elPartition1/3.2_t10_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.2_elPartition1/3.2_t10_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t20_len100.fasta\n",
      "DEBUG: Calling recursive for t20_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.2_elPartition1/3.2_t20_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.2_elPartition1/3.2_t20_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.2_elPartition1/3.2_t20_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t20_len10000.fasta\n",
      "DEBUG: Calling recursive for t20_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.2_elPartition1/3.2_t20_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.2_elPartition1/3.2_t20_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.2_elPartition1/3.2_t20_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t40_len1000.fasta\n",
      "DEBUG: Calling recursive for t40_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.2_elPartition1/3.2_t40_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.2_elPartition1/3.2_t40_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.2_elPartition1/3.2_t40_len1000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t320_len100.fasta\n",
      "DEBUG: Calling recursive for t320_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.2_elPartition1/3.2_t320_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.2_elPartition1/3.2_t320_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.2_elPartition1/3.2_t320_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t10_len1000.fasta\n",
      "DEBUG: Calling recursive for t10_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.2_elPartition1/3.2_t10_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.2_elPartition1/3.2_t10_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.2_elPartition1/3.2_t10_len1000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t160_len100.fasta\n",
      "DEBUG: Calling recursive for t160_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.2_elPartition1/3.2_t160_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.2_elPartition1/3.2_t160_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.2_elPartition1/3.2_t160_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t80_len100.fasta\n",
      "DEBUG: Calling recursive for t80_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.2_elPartition1/3.2_t80_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.2_elPartition1/3.2_t80_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.2_elPartition1/3.2_t80_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t40_len10000.fasta\n",
      "DEBUG: Calling recursive for t40_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.2_elPartition1/3.2_t40_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.2_elPartition1/3.2_t40_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.2_elPartition1/3.2_t40_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t160_len1000.fasta\n",
      "DEBUG: Calling recursive for t160_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.2_elPartition1/3.2_t160_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.2_elPartition1/3.2_t160_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.2_elPartition1/3.2_t160_len1000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t80_len1000.fasta\n",
      "DEBUG: Calling recursive for t80_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.2_elPartition1/3.2_t80_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.2_elPartition1/3.2_t80_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.2_elPartition1/3.2_t80_len1000_log.txt\n",
      "Preprocessing complete for all files in the directory/GenBank input.\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t10_len10000.fasta\n",
      "DEBUG: Calling recursive for t10_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.3_elPartition2/3.3_t10_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.3_elPartition2/3.3_t10_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.3_elPartition2/3.3_t10_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t80_len10000.fasta\n",
      "DEBUG: Calling recursive for t80_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.3_elPartition2/3.3_t80_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.3_elPartition2/3.3_t80_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.3_elPartition2/3.3_t80_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t320_len1000.fasta\n",
      "DEBUG: Calling recursive for t320_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.3_elPartition2/3.3_t320_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.3_elPartition2/3.3_t320_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.3_elPartition2/3.3_t320_len1000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t40_len100.fasta\n",
      "DEBUG: Calling recursive for t40_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.3_elPartition2/3.3_t40_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.3_elPartition2/3.3_t40_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.3_elPartition2/3.3_t40_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t160_len10000.fasta\n",
      "DEBUG: Calling recursive for t160_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.3_elPartition2/3.3_t160_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.3_elPartition2/3.3_t160_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.3_elPartition2/3.3_t160_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t20_len1000.fasta\n",
      "DEBUG: Calling recursive for t20_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.3_elPartition2/3.3_t20_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.3_elPartition2/3.3_t20_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.3_elPartition2/3.3_t20_len1000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t320_len10000.fasta\n",
      "DEBUG: Calling recursive for t320_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.3_elPartition2/3.3_t320_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.3_elPartition2/3.3_t320_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.3_elPartition2/3.3_t320_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t10_len100.fasta\n",
      "DEBUG: Calling recursive for t10_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.3_elPartition2/3.3_t10_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.3_elPartition2/3.3_t10_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.3_elPartition2/3.3_t10_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t20_len100.fasta\n",
      "DEBUG: Calling recursive for t20_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.3_elPartition2/3.3_t20_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.3_elPartition2/3.3_t20_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.3_elPartition2/3.3_t20_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t20_len10000.fasta\n",
      "DEBUG: Calling recursive for t20_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.3_elPartition2/3.3_t20_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.3_elPartition2/3.3_t20_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.3_elPartition2/3.3_t20_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t40_len1000.fasta\n",
      "DEBUG: Calling recursive for t40_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.3_elPartition2/3.3_t40_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.3_elPartition2/3.3_t40_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.3_elPartition2/3.3_t40_len1000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t320_len100.fasta\n",
      "DEBUG: Calling recursive for t320_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.3_elPartition2/3.3_t320_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.3_elPartition2/3.3_t320_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.3_elPartition2/3.3_t320_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t10_len1000.fasta\n",
      "DEBUG: Calling recursive for t10_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.3_elPartition2/3.3_t10_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.3_elPartition2/3.3_t10_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.3_elPartition2/3.3_t10_len1000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t160_len100.fasta\n",
      "DEBUG: Calling recursive for t160_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.3_elPartition2/3.3_t160_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.3_elPartition2/3.3_t160_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.3_elPartition2/3.3_t160_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t80_len100.fasta\n",
      "DEBUG: Calling recursive for t80_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.3_elPartition2/3.3_t80_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.3_elPartition2/3.3_t80_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.3_elPartition2/3.3_t80_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t40_len10000.fasta\n",
      "DEBUG: Calling recursive for t40_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.3_elPartition2/3.3_t40_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.3_elPartition2/3.3_t40_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.3_elPartition2/3.3_t40_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t160_len1000.fasta\n",
      "DEBUG: Calling recursive for t160_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.3_elPartition2/3.3_t160_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.3_elPartition2/3.3_t160_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.3_elPartition2/3.3_t160_len1000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t80_len1000.fasta\n",
      "DEBUG: Calling recursive for t80_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.3_elPartition2/3.3_t80_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.3_elPartition2/3.3_t80_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.3_elPartition2/3.3_t80_len1000_log.txt\n",
      "Preprocessing complete for all files in the directory/GenBank input.\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t10_len10000.fasta\n",
      "DEBUG: Calling recursive for t10_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.4_elPartition4/3.4_t10_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.4_elPartition4/3.4_t10_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.4_elPartition4/3.4_t10_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t80_len10000.fasta\n",
      "DEBUG: Calling recursive for t80_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.4_elPartition4/3.4_t80_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.4_elPartition4/3.4_t80_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.4_elPartition4/3.4_t80_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t320_len1000.fasta\n",
      "DEBUG: Calling recursive for t320_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.4_elPartition4/3.4_t320_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.4_elPartition4/3.4_t320_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.4_elPartition4/3.4_t320_len1000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t40_len100.fasta\n",
      "DEBUG: Calling recursive for t40_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.4_elPartition4/3.4_t40_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.4_elPartition4/3.4_t40_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.4_elPartition4/3.4_t40_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t160_len10000.fasta\n",
      "DEBUG: Calling recursive for t160_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.4_elPartition4/3.4_t160_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.4_elPartition4/3.4_t160_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.4_elPartition4/3.4_t160_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t20_len1000.fasta\n",
      "DEBUG: Calling recursive for t20_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.4_elPartition4/3.4_t20_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.4_elPartition4/3.4_t20_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.4_elPartition4/3.4_t20_len1000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t320_len10000.fasta\n",
      "DEBUG: Calling recursive for t320_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.4_elPartition4/3.4_t320_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.4_elPartition4/3.4_t320_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.4_elPartition4/3.4_t320_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t10_len100.fasta\n",
      "DEBUG: Calling recursive for t10_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.4_elPartition4/3.4_t10_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.4_elPartition4/3.4_t10_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.4_elPartition4/3.4_t10_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t20_len100.fasta\n",
      "DEBUG: Calling recursive for t20_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.4_elPartition4/3.4_t20_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.4_elPartition4/3.4_t20_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.4_elPartition4/3.4_t20_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t20_len10000.fasta\n",
      "DEBUG: Calling recursive for t20_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.4_elPartition4/3.4_t20_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.4_elPartition4/3.4_t20_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.4_elPartition4/3.4_t20_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t40_len1000.fasta\n",
      "DEBUG: Calling recursive for t40_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.4_elPartition4/3.4_t40_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.4_elPartition4/3.4_t40_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.4_elPartition4/3.4_t40_len1000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t320_len100.fasta\n",
      "DEBUG: Calling recursive for t320_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.4_elPartition4/3.4_t320_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.4_elPartition4/3.4_t320_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.4_elPartition4/3.4_t320_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t10_len1000.fasta\n",
      "DEBUG: Calling recursive for t10_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.4_elPartition4/3.4_t10_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.4_elPartition4/3.4_t10_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.4_elPartition4/3.4_t10_len1000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t160_len100.fasta\n",
      "DEBUG: Calling recursive for t160_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.4_elPartition4/3.4_t160_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.4_elPartition4/3.4_t160_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.4_elPartition4/3.4_t160_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t80_len100.fasta\n",
      "DEBUG: Calling recursive for t80_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.4_elPartition4/3.4_t80_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.4_elPartition4/3.4_t80_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.4_elPartition4/3.4_t80_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t40_len10000.fasta\n",
      "DEBUG: Calling recursive for t40_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.4_elPartition4/3.4_t40_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.4_elPartition4/3.4_t40_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.4_elPartition4/3.4_t40_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t160_len1000.fasta\n",
      "DEBUG: Calling recursive for t160_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.4_elPartition4/3.4_t160_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.4_elPartition4/3.4_t160_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.4_elPartition4/3.4_t160_len1000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t80_len1000.fasta\n",
      "DEBUG: Calling recursive for t80_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.4_elPartition4/3.4_t80_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.4_elPartition4/3.4_t80_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.4_elPartition4/3.4_t80_len1000_log.txt\n",
      "Preprocessing complete for all files in the directory/GenBank input.\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t10_len10000.fasta\n",
      "DEBUG: Calling recursive for t10_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.5_elPartition8/3.5_t10_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.5_elPartition8/3.5_t10_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.5_elPartition8/3.5_t10_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t80_len10000.fasta\n",
      "DEBUG: Calling recursive for t80_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.5_elPartition8/3.5_t80_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.5_elPartition8/3.5_t80_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.5_elPartition8/3.5_t80_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t320_len1000.fasta\n",
      "DEBUG: Calling recursive for t320_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.5_elPartition8/3.5_t320_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.5_elPartition8/3.5_t320_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.5_elPartition8/3.5_t320_len1000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t40_len100.fasta\n",
      "DEBUG: Calling recursive for t40_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.5_elPartition8/3.5_t40_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.5_elPartition8/3.5_t40_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.5_elPartition8/3.5_t40_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t160_len10000.fasta\n",
      "DEBUG: Calling recursive for t160_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.5_elPartition8/3.5_t160_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.5_elPartition8/3.5_t160_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.5_elPartition8/3.5_t160_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t20_len1000.fasta\n",
      "DEBUG: Calling recursive for t20_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.5_elPartition8/3.5_t20_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.5_elPartition8/3.5_t20_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.5_elPartition8/3.5_t20_len1000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t320_len10000.fasta\n",
      "DEBUG: Calling recursive for t320_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.5_elPartition8/3.5_t320_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.5_elPartition8/3.5_t320_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.5_elPartition8/3.5_t320_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t10_len100.fasta\n",
      "DEBUG: Calling recursive for t10_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.5_elPartition8/3.5_t10_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.5_elPartition8/3.5_t10_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.5_elPartition8/3.5_t10_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t20_len100.fasta\n",
      "DEBUG: Calling recursive for t20_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.5_elPartition8/3.5_t20_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.5_elPartition8/3.5_t20_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.5_elPartition8/3.5_t20_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t20_len10000.fasta\n",
      "DEBUG: Calling recursive for t20_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.5_elPartition8/3.5_t20_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.5_elPartition8/3.5_t20_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.5_elPartition8/3.5_t20_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t40_len1000.fasta\n",
      "DEBUG: Calling recursive for t40_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.5_elPartition8/3.5_t40_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.5_elPartition8/3.5_t40_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.5_elPartition8/3.5_t40_len1000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t320_len100.fasta\n",
      "DEBUG: Calling recursive for t320_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.5_elPartition8/3.5_t320_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.5_elPartition8/3.5_t320_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.5_elPartition8/3.5_t320_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t10_len1000.fasta\n",
      "DEBUG: Calling recursive for t10_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.5_elPartition8/3.5_t10_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.5_elPartition8/3.5_t10_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.5_elPartition8/3.5_t10_len1000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t160_len100.fasta\n",
      "DEBUG: Calling recursive for t160_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.5_elPartition8/3.5_t160_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.5_elPartition8/3.5_t160_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.5_elPartition8/3.5_t160_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t80_len100.fasta\n",
      "DEBUG: Calling recursive for t80_len100.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.5_elPartition8/3.5_t80_len100\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.5_elPartition8/3.5_t80_len100_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.5_elPartition8/3.5_t80_len100_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t40_len10000.fasta\n",
      "DEBUG: Calling recursive for t40_len10000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.5_elPartition8/3.5_t40_len10000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.5_elPartition8/3.5_t40_len10000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.5_elPartition8/3.5_t40_len10000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t160_len1000.fasta\n",
      "DEBUG: Calling recursive for t160_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.5_elPartition8/3.5_t160_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.5_elPartition8/3.5_t160_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.5_elPartition8/3.5_t160_len1000_log.txt\n",
      "DEBUG: Reading pre-aligned file: ../test_data/simulations/2_aligned/t80_len1000.fasta\n",
      "DEBUG: Calling recursive for t80_len1000.fasta with aligned data (dict). Output prefix: ../test_data/simulations/3.5_elPartition8/3.5_t80_len1000\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/simulations/3.5_elPartition8/3.5_t80_len1000_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/simulations/3.5_elPartition8/3.5_t80_len1000_log.txt\n",
      "Preprocessing complete for all files in the directory/GenBank input.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# No partitioning (0)\n",
    "prepDyn(input_file=\"../test_data/simulations/2_aligned\", \n",
    "        output_file=\"../test_data/simulations/3.1_noPartition/3.1\",\n",
    "        log=True,\n",
    "        del_inv=False, orphan_method=None,\n",
    "        partitioning_method=None)\n",
    "\n",
    "# Equal-length partitioning (round 1)\n",
    "prepDyn(input_file=\"../test_data/simulations/2_aligned\", \n",
    "        output_file=\"../test_data/simulations/3.2_elPartition1/3.2\",\n",
    "        log=True,\n",
    "        del_inv=False, orphan_method=None,\n",
    "        partitioning_method=\"equal\", partitioning_round=1)\n",
    "\n",
    "# Equal-length partitioning (round 2)\n",
    "prepDyn(input_file=\"../test_data/simulations/2_aligned\", \n",
    "        output_file=\"../test_data/simulations/3.3_elPartition2/3.3\",\n",
    "        log=True,\n",
    "        del_inv=False, orphan_method=None,\n",
    "        partitioning_method=\"equal\", partitioning_round=2)\n",
    "\n",
    "# Equal-length partitioning (round 4)\n",
    "prepDyn(input_file=\"../test_data/simulations/2_aligned\", \n",
    "        output_file=\"../test_data/simulations/3.4_elPartition4/3.4\",\n",
    "        log=True,\n",
    "        del_inv=False, orphan_method=None,\n",
    "        partitioning_method=\"equal\", partitioning_round=4)\n",
    "\n",
    "# Equal-length partitioning (round 8)\n",
    "prepDyn(input_file=\"../test_data/simulations/2_aligned\", \n",
    "        output_file=\"../test_data/simulations/3.5_elPartition8/3.5\",\n",
    "        log=True,\n",
    "        del_inv=False, orphan_method=None,\n",
    "        partitioning_method=\"equal\", partitioning_round=8)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8898486",
   "metadata": {},
   "source": [
    "Now, let's create the PhyG scripts for each data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11507768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All .pg script files generated successfully.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "\n",
    "# All target directories with their corresponding prep identifiers\n",
    "directories = {\n",
    "    \"3.1_noPartition\": \"3.1\",\n",
    "    \"3.2_elPartition1\": \"3.2\",\n",
    "    \"3.3_elPartition2\": \"3.3\",\n",
    "    \"3.4_elPartition4\": \"3.4\",\n",
    "    \"3.5_elPartition8\": \"3.5\"\n",
    "}\n",
    "\n",
    "# Root path\n",
    "base_path = \"../test_data/simulations\"\n",
    "\n",
    "# Values to substitute\n",
    "leaves_list = [10, 20, 40, 80]\n",
    "columns_list = [100, 1000, 10000]\n",
    "\n",
    "# Script template\n",
    "template = \"\"\"set (seed:22)\n",
    "\n",
    "--LOAD DATA\n",
    "read (\"{prep}_t{leaves}_len{columns}_preprocessed.fasta\")\n",
    "\n",
    "--SEARCH SETTINGS\n",
    "set (partitioncharacter:#)\n",
    "set (criterion:parsimony)\n",
    "set (graphtype:tree)\n",
    "build()\n",
    "swap()\n",
    "\n",
    "--REPORT RESULTS\n",
    "report (\"{prep}_t{leaves}_len{columns}_searchstats.csv\", search) \n",
    "report (\"{prep}_t{leaves}_len{columns}_searchTrees.nwk\", graphs, newick, nohtulabels)\n",
    "select (unique)\n",
    "select (best)\n",
    "report (\"{prep}_t{leaves}_len{columns}_bestUnique.nwk\", graphs, newick, nohtulabels)\n",
    "report (\"{prep}_t{leaves}_len{columns}_ia.tnt\", tnt, concatenate, overwrite)\n",
    "report (\"{prep}_t{leaves}_len{columns}_bestConsensus.nwk\", newick, reconcile, method:strict, nohtulabels)\n",
    "\"\"\"\n",
    "\n",
    "# Generate scripts for each directory\n",
    "for dir_name, prep in directories.items():\n",
    "    output_dir = os.path.join(base_path, dir_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for leaves in leaves_list:\n",
    "        for columns in columns_list:\n",
    "            script_content = template.format(prep=prep, leaves=leaves, columns=columns)\n",
    "            filename = f\"{prep}_t{leaves}_len{columns}_script.pg\"\n",
    "            filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "            with open(filepath, \"w\") as f:\n",
    "                f.write(script_content)\n",
    "\n",
    "print(\"All .pg script files generated successfully.\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98543a87",
   "metadata": {},
   "source": [
    "## Empirical examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff2955d",
   "metadata": {},
   "source": [
    "### de SÃ¡ et al. (2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c0449f",
   "metadata": {},
   "source": [
    "We used the dataset of Hylodidae from de SÃ¡ et al. (2022), which comprises 75 leaves with ~4,291 bp (12S, tRNA-Val, 16S, COI, RAG1-a, RAG1-b). We compared tree costs and topologies from three preprocessing strategies:\n",
    "\n",
    "1. Without preprocessing\n",
    "\n",
    "2. With preprocessing, without partitioning\n",
    "\n",
    "3. With preprocessing, with balanced partitioning\n",
    "\n",
    "4. With preprocessing, with conservative partitioning\n",
    "\n",
    "5. With preprocessing, with equal-length partitioning\n",
    "\n",
    "6. With preprocessing, with maximum partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6a10d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/labanfibios/anaconda3/lib/python3.10/site-packages/Bio/Entrez/__init__.py:570: UserWarning: \n",
      "Email address is not specified.\n",
      "\n",
      "To make use of NCBI's E-utilities, NCBI requires you to specify your\n",
      "email address with each request.  As an example, if your email address\n",
      "is A.N.Other@example.com, you can specify it as follows:\n",
      "   from Bio import Entrez\n",
      "   Entrez.email = 'A.N.Other@example.com'\n",
      "In case of excessive usage of the E-utilities, NCBI will attempt to contact\n",
      "a user at the email address provided before blocking access to the\n",
      "E-utilities.\n",
      "  warnings.warn(\"\"\"\n",
      "outputhat23=16\n",
      "treein = 0\n",
      "compacttree = 0\n",
      "stacksize: 8192 kb\n",
      "generating a scoring matrix for nucleotide (dist=200) ... done\n",
      "All-to-all alignment.\n",
      "tbfast-pair (nuc) Version 7.520\n",
      "alg=L, model=DNA200 (2), 2.00 (6.00), -0.10 (-0.30), noshift, amax=0.0\n",
      "0 thread(s)\n",
      "\n",
      "outputhat23=16\n",
      "Loading 'hat3.seed' ... \n",
      "done.\n",
      "Writing hat3 for iterative refinement\n",
      "generating a scoring matrix for nucleotide (dist=200) ... done\n",
      "Gap Penalty = -1.53, +0.00, +0.00\n",
      "tbutree = 1, compacttree = 0\n",
      "Constructing a UPGMA tree ... \n",
      "   70 / 75\n",
      "done.\n",
      "\n",
      "Progressive alignment ... \n",
      "STEP    63 /74 \n",
      "Reallocating..done. *alloclen = 5897\n",
      "STEP    74 /74 \n",
      "done.\n",
      "tbfast (nuc) Version 7.520\n",
      "alg=A, model=DNA200 (2), 1.53 (4.59), -0.00 (-0.00), noshift, amax=0.0\n",
      "1 thread(s)\n",
      "\n",
      "minimumweight = 0.000010\n",
      "autosubalignment = 0.000000\n",
      "nthread = 0\n",
      "randomseed = 0\n",
      "blosum 62 / kimura 200\n",
      "poffset = 0\n",
      "niter = 16\n",
      "sueff_global = 0.100000\n",
      "nadd = 16\n",
      "Loading 'hat3' ... done.\n",
      "generating a scoring matrix for nucleotide (dist=200) ... done\n",
      "\n",
      "   70 / 75\n",
      "Segment   1/  1    1-2506\n",
      "STEP 003-041-0  identical.    identical.    identical.    identical.    identical.    identical.    rejected. identical.    identical.    identical.    identical.    identical.    identical.    rejected. rejected. rejected. rejected. accepted. identical.    identical.    rejected. accepted. identical.    rejected. identical.    rejected. identical.    identical.    identical.    accepted. accepted. identical.    identical.    identical.    identical.    identical.    identical.    identical.    identical.    identical.    identical.    identical.    identical.    identical.    identical.    identical.    identical.    identical.    identical.    identical.    identical.    identical.    rejected. identical.   \n",
      "Converged.\n",
      "\n",
      "done\n",
      "dvtditr (nuc) Version 7.520\n",
      "alg=A, model=DNA200 (2), 1.53 (4.59), -0.00 (-0.00), noshift, amax=0.0\n",
      "0 thread(s)\n",
      "\n",
      "\n",
      "Strategy:\n",
      " L-INS-i (Probably most accurate, very slow)\n",
      " Iterative refinement method (<16) with LOCAL pairwise alignment information\n",
      "\n",
      "If unsure which option to use, try 'mafft --auto input > output'.\n",
      "For more information, see 'mafft --help', 'mafft --man' and the mafft page.\n",
      "\n",
      "The default gap scoring scheme has been changed in version 7.110 (2013 Oct).\n",
      "It tends to insert more gaps into gap-rich regions than previous versions.\n",
      "To disable this change, add the --leavegappyregion option.\n",
      "\n",
      "/Users/labanfibios/anaconda3/lib/python3.10/site-packages/Bio/Entrez/__init__.py:570: UserWarning: \n",
      "Email address is not specified.\n",
      "\n",
      "To make use of NCBI's E-utilities, NCBI requires you to specify your\n",
      "email address with each request.  As an example, if your email address\n",
      "is A.N.Other@example.com, you can specify it as follows:\n",
      "   from Bio import Entrez\n",
      "   Entrez.email = 'A.N.Other@example.com'\n",
      "In case of excessive usage of the E-utilities, NCBI will attempt to contact\n",
      "a user at the email address provided before blocking access to the\n",
      "E-utilities.\n",
      "  warnings.warn(\"\"\"\n",
      "outputhat23=16\n",
      "treein = 0\n",
      "compacttree = 0\n",
      "stacksize: 8192 kb\n",
      "generating a scoring matrix for nucleotide (dist=200) ... done\n",
      "All-to-all alignment.\n",
      "tbfast-pair (nuc) Version 7.520\n",
      "alg=L, model=DNA200 (2), 2.00 (6.00), -0.10 (-0.30), noshift, amax=0.0\n",
      "0 thread(s)\n",
      "\n",
      "outputhat23=16\n",
      "Loading 'hat3.seed' ... \n",
      "done.\n",
      "Writing hat3 for iterative refinement\n",
      "generating a scoring matrix for nucleotide (dist=200) ... done\n",
      "Gap Penalty = -1.53, +0.00, +0.00\n",
      "tbutree = 1, compacttree = 0\n",
      "Constructing a UPGMA tree ... \n",
      "   20 / 27\n",
      "done.\n",
      "\n",
      "Progressive alignment ... \n",
      "STEP    23 /26 \n",
      "Reallocating..done. *alloclen = 2422\n",
      "STEP    26 /26 \n",
      "done.\n",
      "tbfast (nuc) Version 7.520\n",
      "alg=A, model=DNA200 (2), 1.53 (4.59), -0.00 (-0.00), noshift, amax=0.0\n",
      "1 thread(s)\n",
      "\n",
      "minimumweight = 0.000010\n",
      "autosubalignment = 0.000000\n",
      "nthread = 0\n",
      "randomseed = 0\n",
      "blosum 62 / kimura 200\n",
      "poffset = 0\n",
      "niter = 16\n",
      "sueff_global = 0.100000\n",
      "nadd = 16\n",
      "Loading 'hat3' ... done.\n",
      "generating a scoring matrix for nucleotide (dist=200) ... done\n",
      "\n",
      "   20 / 27\n",
      "Segment   1/  1    1- 716\n",
      "STEP 002-025-1  identical.    identical.   \n",
      "Converged.\n",
      "\n",
      "done\n",
      "dvtditr (nuc) Version 7.520\n",
      "alg=A, model=DNA200 (2), 1.53 (4.59), -0.00 (-0.00), noshift, amax=0.0\n",
      "0 thread(s)\n",
      "\n",
      "\n",
      "Strategy:\n",
      " L-INS-i (Probably most accurate, very slow)\n",
      " Iterative refinement method (<16) with LOCAL pairwise alignment information\n",
      "\n",
      "If unsure which option to use, try 'mafft --auto input > output'.\n",
      "For more information, see 'mafft --help', 'mafft --man' and the mafft page.\n",
      "\n",
      "The default gap scoring scheme has been changed in version 7.110 (2013 Oct).\n",
      "It tends to insert more gaps into gap-rich regions than previous versions.\n",
      "To disable this change, add the --leavegappyregion option.\n",
      "\n",
      "/Users/labanfibios/anaconda3/lib/python3.10/site-packages/Bio/Entrez/__init__.py:570: UserWarning: \n",
      "Email address is not specified.\n",
      "\n",
      "To make use of NCBI's E-utilities, NCBI requires you to specify your\n",
      "email address with each request.  As an example, if your email address\n",
      "is A.N.Other@example.com, you can specify it as follows:\n",
      "   from Bio import Entrez\n",
      "   Entrez.email = 'A.N.Other@example.com'\n",
      "In case of excessive usage of the E-utilities, NCBI will attempt to contact\n",
      "a user at the email address provided before blocking access to the\n",
      "E-utilities.\n",
      "  warnings.warn(\"\"\"\n",
      "outputhat23=16\n",
      "treein = 0\n",
      "compacttree = 0\n",
      "stacksize: 8192 kb\n",
      "generating a scoring matrix for nucleotide (dist=200) ... done\n",
      "All-to-all alignment.\n",
      "tbfast-pair (nuc) Version 7.520\n",
      "alg=L, model=DNA200 (2), 2.00 (6.00), -0.10 (-0.30), noshift, amax=0.0\n",
      "0 thread(s)\n",
      "\n",
      "outputhat23=16\n",
      "Loading 'hat3.seed' ... \n",
      "done.\n",
      "Writing hat3 for iterative refinement\n",
      "generating a scoring matrix for nucleotide (dist=200) ... done\n",
      "Gap Penalty = -1.53, +0.00, +0.00\n",
      "tbutree = 1, compacttree = 0\n",
      "Constructing a UPGMA tree ... \n",
      "   20 / 22\n",
      "done.\n",
      "\n",
      "Progressive alignment ... \n",
      "STEP    21 /21 \n",
      "done.\n",
      "tbfast (nuc) Version 7.520\n",
      "alg=A, model=DNA200 (2), 1.53 (4.59), -0.00 (-0.00), noshift, amax=0.0\n",
      "1 thread(s)\n",
      "\n",
      "minimumweight = 0.000010\n",
      "autosubalignment = 0.000000\n",
      "nthread = 0\n",
      "randomseed = 0\n",
      "blosum 62 / kimura 200\n",
      "poffset = 0\n",
      "niter = 16\n",
      "sueff_global = 0.100000\n",
      "nadd = 16\n",
      "Loading 'hat3' ... done.\n",
      "generating a scoring matrix for nucleotide (dist=200) ... done\n",
      "\n",
      "   20 / 22\n",
      "Segment   1/  1    1- 825\n",
      "STEP 002-020-1  identical.    identical.   \n",
      "Converged.\n",
      "\n",
      "done\n",
      "dvtditr (nuc) Version 7.520\n",
      "alg=A, model=DNA200 (2), 1.53 (4.59), -0.00 (-0.00), noshift, amax=0.0\n",
      "0 thread(s)\n",
      "\n",
      "\n",
      "Strategy:\n",
      " L-INS-i (Probably most accurate, very slow)\n",
      " Iterative refinement method (<16) with LOCAL pairwise alignment information\n",
      "\n",
      "If unsure which option to use, try 'mafft --auto input > output'.\n",
      "For more information, see 'mafft --help', 'mafft --man' and the mafft page.\n",
      "\n",
      "The default gap scoring scheme has been changed in version 7.110 (2013 Oct).\n",
      "It tends to insert more gaps into gap-rich regions than previous versions.\n",
      "To disable this change, add the --leavegappyregion option.\n",
      "\n",
      "/Users/labanfibios/anaconda3/lib/python3.10/site-packages/Bio/Entrez/__init__.py:570: UserWarning: \n",
      "Email address is not specified.\n",
      "\n",
      "To make use of NCBI's E-utilities, NCBI requires you to specify your\n",
      "email address with each request.  As an example, if your email address\n",
      "is A.N.Other@example.com, you can specify it as follows:\n",
      "   from Bio import Entrez\n",
      "   Entrez.email = 'A.N.Other@example.com'\n",
      "In case of excessive usage of the E-utilities, NCBI will attempt to contact\n",
      "a user at the email address provided before blocking access to the\n",
      "E-utilities.\n",
      "  warnings.warn(\"\"\"\n",
      "outputhat23=16\n",
      "treein = 0\n",
      "compacttree = 0\n",
      "stacksize: 8192 kb\n",
      "generating a scoring matrix for nucleotide (dist=200) ... done\n",
      "All-to-all alignment.\n",
      "tbfast-pair (nuc) Version 7.520\n",
      "alg=L, model=DNA200 (2), 2.00 (6.00), -0.10 (-0.30), noshift, amax=0.0\n",
      "0 thread(s)\n",
      "\n",
      "outputhat23=16\n",
      "Loading 'hat3.seed' ... \n",
      "done.\n",
      "Writing hat3 for iterative refinement\n",
      "generating a scoring matrix for nucleotide (dist=200) ... done\n",
      "Gap Penalty = -1.53, +0.00, +0.00\n",
      "tbutree = 1, compacttree = 0\n",
      "Constructing a UPGMA tree ... \n",
      "   20 / 28\n",
      "done.\n",
      "\n",
      "Progressive alignment ... \n",
      "STEP    23 /27 \n",
      "Reallocating..done. *alloclen = 1877\n",
      "STEP    27 /27 \n",
      "done.\n",
      "tbfast (nuc) Version 7.520\n",
      "alg=A, model=DNA200 (2), 1.53 (4.59), -0.00 (-0.00), noshift, amax=0.0\n",
      "1 thread(s)\n",
      "\n",
      "minimumweight = 0.000010\n",
      "autosubalignment = 0.000000\n",
      "nthread = 0\n",
      "randomseed = 0\n",
      "blosum 62 / kimura 200\n",
      "poffset = 0\n",
      "niter = 16\n",
      "sueff_global = 0.100000\n",
      "nadd = 16\n",
      "Loading 'hat3' ... done.\n",
      "generating a scoring matrix for nucleotide (dist=200) ... done\n",
      "\n",
      "   20 / 28\n",
      "Segment   1/  1    1- 443\n",
      "STEP 002-026-1  identical.   \n",
      "Converged.\n",
      "\n",
      "done\n",
      "dvtditr (nuc) Version 7.520\n",
      "alg=A, model=DNA200 (2), 1.53 (4.59), -0.00 (-0.00), noshift, amax=0.0\n",
      "0 thread(s)\n",
      "\n",
      "\n",
      "Strategy:\n",
      " L-INS-i (Probably most accurate, very slow)\n",
      " Iterative refinement method (<16) with LOCAL pairwise alignment information\n",
      "\n",
      "If unsure which option to use, try 'mafft --auto input > output'.\n",
      "For more information, see 'mafft --help', 'mafft --man' and the mafft page.\n",
      "\n",
      "The default gap scoring scheme has been changed in version 7.110 (2013 Oct).\n",
      "It tends to insert more gaps into gap-rich regions than previous versions.\n",
      "To disable this change, add the --leavegappyregion option.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: Phantasmarana_apuana_CFBH10811\n",
      "Sequence: Phantasmarana_apuana_CFBHT09118\n",
      "Sequence: Phantasmarana_apuana_MTR12614\n",
      "Sequence: Phantasmarana_apuana_MTR12650\n",
      "Sequence: Phantasmarana_apuana_SMRP526.4\n",
      "Sequence: Phantasmarana_apuana_TLFT1981\n",
      "Sequence: Phantasmarana_apuana_TLFT1911\n",
      "Sequence: Phantasmarana_boticariana_CFBH06292\n",
      "Sequence: Phantasmarana_boticariana_CFBH06293\n",
      "Sequence: Phantasmarana_boticariana_SMRP109.1\n",
      "Sequence: Phantasmarana_boticariana_SMRP109.2\n",
      "Sequence: Phantasmarana_boticariana_ZUEC24587\n",
      "Sequence: Phantasmarana_curucutuensis_CFBH17668_\n",
      "Sequence: Phantasmarana_jordanensis_CFBH28578\n",
      "Sequence: Phantasmarana_jordanensis_SMRP81.3\n",
      "Sequence: Phantasmarana_jordanensis_SMRP81.6\n",
      "Sequence: Phantasmarana_jordanensis_SMRP81.8\n",
      "Sequence: Phantasmarana_jordanensis_MCP11575\n",
      "Sequence: Phantasmarana_lutzae_SMRP555.1\n",
      "Sequence: Phantasmarana_lutzae_SMRP555.2\n",
      "Sequence: Phantasmarana_lutzae_SMRP555.3\n",
      "Sequence: Phantasmarana_lutzae_SMRP555.4\n",
      "Sequence: Phantasmarana_lutzae_SMRP555.5\n",
      "Sequence: Phantasmarana_massarti_CFBHT23090\n",
      "Sequence: Phantasmarana_massarti_ZUEC11395\n",
      "Sequence: Phantasmarana_massarti_ZUEC11553\n",
      "Sequence: Phantasmarana_tamuia_CFBH38735\n",
      "Sequence: Phantasmarana_tamuia_CFBH38976\n",
      "Sequence: Phantasmarana_tamuia_CFBH38977\n",
      "Sequence: Phantasmarana_tamuia_CFBH38978a\n",
      "Sequence: Phantasmarana_tamuia_CFBH38978b\n",
      "Sequence: Phantasmarana_tamuia_CFBH38978c\n",
      "Sequence: Megaelosia_goeldii_CFBH14028\n",
      "Sequence: Megaelosia_goeldii_CFBH18821\n",
      "Sequence: Megaelosia_goeldii_CFBHT02911\n",
      "Sequence: Megaelosia_goeldii_MZUSP95879\n",
      "Sequence: Megaelosia_goeldii_SMRP527.2\n",
      "Sequence: Megaelosia_goeldii_SMRP527.3\n",
      "Sequence: Megaelosia_goeldii_SMRP527.4\n",
      "Sequence: Megaelosia_aff_goeldii_CFBH28108\n",
      "Sequence: Hylodes_amnicola_CFBH30971\n",
      "Sequence: Hylodes_amnicola_CFBH30972\n",
      "Sequence: Hylodes_asper_ZUEC11555\n",
      "Sequence: Hylodes_caete_CFBH40524\n",
      "Sequence: Hylodes_caete_CFBH40526\n",
      "Sequence: Hylodes_fredi_MNRJ36077\n",
      "Sequence: Hylodes_heyeri_ZUEC24650\n",
      "Sequence: Hylodes_japi_CFBH25403\n",
      "Sequence: Hylodes_japi_CFBH25426\n",
      "Sequence: Hylodes_meridionalis_MCP11546\n",
      "Sequence: Hylodes_meridionalis_MCP11547\n",
      "Sequence: Hylodes_nasus_MNRJ35435\n",
      "Sequence: Hylodes_ornatus_CFBH34905\n",
      "Sequence: Hylodes_perere_CFBH31106\n",
      "Sequence: Hylodes_perere_CFBH31107\n",
      "Sequence: Hylodes_phyllodes_CFBH39961\n",
      "Sequence: Hylodes_phyllodes_CFBHT03131\n",
      "Sequence: Hylodes_pipilans_MNRJ37307\n",
      "Sequence: Hylodes_pipilans_MNRJ39371\n",
      "Sequence: Hylodes_regius_CFBH30970\n",
      "Sequence: Hylodes_regius_CFBH34910\n",
      "Sequence: Hylodes_sazimai_CFBH29586\n",
      "Sequence: Hylodes_sazimai_CFBHT14629\n",
      "Sequence: Crossodactylus_caramaschii_CFBH17647\n",
      "Sequence: Crossodactylus_gaudichaudii_ZUEC17569\n",
      "Sequence: Crossodactylus_gaudichaudii_ZUEC17570\n",
      "Sequence: Crossodactylus_gaudichaudii_novoucher\n",
      "Sequence: Crossodactylus_schmidti_MLPA01414\n",
      "Sequence: Crossodactylus_trachystomus_MCNAM18054\n",
      "Sequence: Crossodactylus_werneri_AA01320\n",
      "Sequence: Crossodactylus_werneri_AAGUFU01876\n",
      "Sequence: Alsodes_coppingeri\n",
      "Sequence: Alsodes_vanzolinii\n",
      "Sequence: Eupsophus_nahuelbutensis\n",
      "Sequence: Cycloramphus_boraceiensis\n",
      "Sequence: Phantasmarana_apuana_MTR12614\n",
      "Sequence: Phantasmarana_apuana_MTR12650\n",
      "Sequence: Phantasmarana_boticariana_CFBH06292\n",
      "Sequence: Phantasmarana_curucutuensis_CFBH17668_\n",
      "Sequence: Phantasmarana_tamuia_CFBH38976\n",
      "Sequence: Phantasmarana_tamuia_CFBH38977\n",
      "Sequence: Megaelosia_goeldii_CFBH18821\n",
      "Sequence: Megaelosia_goeldii_MZUSP95879\n",
      "Sequence: Megaelosia_aff_goeldii_CFBH28108\n",
      "Sequence: Hylodes_amnicola_CFBH30971\n",
      "Sequence: Hylodes_amnicola_CFBH30972\n",
      "Sequence: Hylodes_japi_CFBH25403\n",
      "Sequence: Hylodes_japi_CFBH25426\n",
      "Sequence: Hylodes_nasus_MNRJ35435\n",
      "Sequence: Hylodes_ornatus_CFBH34905\n",
      "Sequence: Hylodes_perere_CFBH31106\n",
      "Sequence: Hylodes_perere_CFBH31107\n",
      "Sequence: Hylodes_pipilans_MNRJ37307\n",
      "Sequence: Hylodes_sazimai_CFBH29586\n",
      "Sequence: Hylodes_sazimai_CFBHT14629\n",
      "Sequence: Crossodactylus_caramaschii_CFBH17647\n",
      "Sequence: Crossodactylus_gaudichaudii_novoucher\n",
      "Sequence: Crossodactylus_schmidti_MLPA01414\n",
      "Sequence: Alsodes_coppingeri\n",
      "Sequence: Alsodes_vanzolinii\n",
      "Sequence: Eupsophus_nahuelbutensis\n",
      "Sequence: Cycloramphus_boraceiensis\n",
      "Sequence: Phantasmarana_apuana_CFBH10811\n",
      "Sequence: Phantasmarana_apuana_MTR12614\n",
      "Sequence: Phantasmarana_apuana_MTR12650\n",
      "Sequence: Phantasmarana_boticariana_CFBH06292\n",
      "Sequence: Phantasmarana_curucutuensis_CFBH17668_\n",
      "Sequence: Phantasmarana_tamuia_CFBH38735\n",
      "Sequence: Phantasmarana_tamuia_CFBH38978a\n",
      "Sequence: Phantasmarana_tamuia_CFBH38978c\n",
      "Sequence: Megaelosia_goeldii_CFBHT02911\n",
      "Sequence: Hylodes_amnicola_CFBH30971\n",
      "Sequence: Hylodes_amnicola_CFBH30972\n",
      "Sequence: Hylodes_japi_CFBH25403\n",
      "Sequence: Hylodes_japi_CFBH25426\n",
      "Sequence: Hylodes_nasus_MNRJ35435\n",
      "Sequence: Hylodes_ornatus_CFBH34905\n",
      "Sequence: Hylodes_perere_CFBH31106\n",
      "Sequence: Hylodes_perere_CFBH31107\n",
      "Sequence: Hylodes_pipilans_MNRJ37307\n",
      "Sequence: Hylodes_sazimai_CFBH29586\n",
      "Sequence: Hylodes_sazimai_CFBHT14629\n",
      "Sequence: Crossodactylus_caramaschii_CFBH17647\n",
      "Sequence: Cycloramphus_boraceiensis\n",
      "Sequence: Phantasmarana_apuana_SMRP526.4\n",
      "Sequence: Phantasmarana_apuana_TLFT1981\n",
      "Sequence: Phantasmarana_apuana_TLFT1911\n",
      "Sequence: Phantasmarana_boticariana_SMRP109.1\n",
      "Sequence: Phantasmarana_boticariana_SMRP109.2\n",
      "Sequence: Phantasmarana_boticariana_ZUEC24587\n",
      "Sequence: Phantasmarana_jordanensis_SMRP81.3\n",
      "Sequence: Phantasmarana_jordanensis_SMRP81.6\n",
      "Sequence: Phantasmarana_jordanensis_SMRP81.8\n",
      "Sequence: Phantasmarana_lutzae_SMRP555.1\n",
      "Sequence: Phantasmarana_lutzae_SMRP555.2\n",
      "Sequence: Phantasmarana_lutzae_SMRP555.3\n",
      "Sequence: Phantasmarana_lutzae_SMRP555.4\n",
      "Sequence: Phantasmarana_lutzae_SMRP555.5\n",
      "Sequence: Phantasmarana_massarti_ZUEC11395\n",
      "Sequence: Phantasmarana_massarti_ZUEC11553\n",
      "Sequence: Megaelosia_goeldii_MZUSP95879\n",
      "Sequence: Megaelosia_goeldii_SMRP527.2\n",
      "Sequence: Megaelosia_goeldii_SMRP527.3\n",
      "Sequence: Megaelosia_goeldii_SMRP527.4\n",
      "Sequence: Hylodes_asper_ZUEC11555\n",
      "Sequence: Hylodes_heyeri_ZUEC24650\n",
      "Sequence: Hylodes_meridionalis_MCP11547\n",
      "Sequence: Crossodactylus_gaudichaudii_ZUEC17570\n",
      "Sequence: Crossodactylus_schmidti_MLPA01414\n",
      "Sequence: Alsodes_coppingeri\n",
      "Sequence: Alsodes_vanzolinii\n",
      "Sequence: Eupsophus_nahuelbutensis\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../test_data/empirical/empirical1_deSa/1_noPreprocessing/output_H1_aligned_GB2MSA.fasta',\n",
       " '../test_data/empirical/empirical1_deSa/1_noPreprocessing/output_COI_aligned_GB2MSA.fasta',\n",
       " '../test_data/empirical/empirical1_deSa/1_noPreprocessing/output_RAG1_A_aligned_GB2MSA.fasta',\n",
       " '../test_data/empirical/empirical1_deSa/1_noPreprocessing/output_RAG1_B_aligned_GB2MSA.fasta']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. No preprocessing\n",
    "'''\n",
    "GB2MSA(input_file = \"../test_data/empirical/empirical1_deSa/1_noPreprocessing/Sa2022.csv\", \n",
    "       output_prefix=\"../test_data/empirical/empirical1_deSa/1_noPreprocessing/output\",\n",
    "       write_names=True,log=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcac3e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pre-aligned file: ../test_data/empirical/empirical1_deSa/2_noPartitioning/output_H1_aligned_GB2MSA.fasta\n",
      "Calling recursive for output_H1_aligned_GB2MSA.fasta with aligned data (dict). Output prefix: ../test_data/empirical/empirical1_deSa/2_noPartitioning/output_output_H1_aligned_GB2MSA\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/empirical/empirical1_deSa/2_noPartitioning/output_output_H1_aligned_GB2MSA_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/empirical/empirical1_deSa/2_noPartitioning/output_output_H1_aligned_GB2MSA_log.txt\n",
      "Reading pre-aligned file: ../test_data/empirical/empirical1_deSa/2_noPartitioning/output_RAG1_A_aligned_GB2MSA.fasta\n",
      "Calling recursive for output_RAG1_A_aligned_GB2MSA.fasta with aligned data (dict). Output prefix: ../test_data/empirical/empirical1_deSa/2_noPartitioning/output_output_RAG1_A_aligned_GB2MSA\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/empirical/empirical1_deSa/2_noPartitioning/output_output_RAG1_A_aligned_GB2MSA_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/empirical/empirical1_deSa/2_noPartitioning/output_output_RAG1_A_aligned_GB2MSA_log.txt\n",
      "Reading pre-aligned file: ../test_data/empirical/empirical1_deSa/2_noPartitioning/output_COI_aligned_GB2MSA.fasta\n",
      "Calling recursive for output_COI_aligned_GB2MSA.fasta with aligned data (dict). Output prefix: ../test_data/empirical/empirical1_deSa/2_noPartitioning/output_output_COI_aligned_GB2MSA\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/empirical/empirical1_deSa/2_noPartitioning/output_output_COI_aligned_GB2MSA_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/empirical/empirical1_deSa/2_noPartitioning/output_output_COI_aligned_GB2MSA_log.txt\n",
      "Reading pre-aligned file: ../test_data/empirical/empirical1_deSa/2_noPartitioning/output_RAG1_B_aligned_GB2MSA.fasta\n",
      "Calling recursive for output_RAG1_B_aligned_GB2MSA.fasta with aligned data (dict). Output prefix: ../test_data/empirical/empirical1_deSa/2_noPartitioning/output_output_RAG1_B_aligned_GB2MSA\n",
      "Processing input as dictionary.\n",
      "Writing preprocessed output to: ../test_data/empirical/empirical1_deSa/2_noPartitioning/output_output_RAG1_B_aligned_GB2MSA_preprocessed.fasta\n",
      "Writing individual log to: ../test_data/empirical/empirical1_deSa/2_noPartitioning/output_output_RAG1_B_aligned_GB2MSA_log.txt\n",
      "Preprocessing complete for all files in the directory/GenBank input.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# 2. prepDyn with no partitions\n",
    "prepDyn(input_file = \"../test_data/empirical/empirical1_deSa/2_noPartitioning/\", \n",
    "         output_file=\"../test_data/empirical/empirical1_deSa/2_noPartitioning/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_method=None,\n",
    "         sequence_names=True,log=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17f72c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: 'balanced' partitioning with partitioning_round=2 was skipped for this alignment. This typically happens when the alignment has fewer blocks of missing data ('?') than required. The process will continue without partitioning this file.\n",
      "\n",
      "\n",
      "WARNING: 'balanced' partitioning with partitioning_round=3 was skipped for this alignment. This typically happens when the alignment has fewer blocks of missing data ('?') than required. The process will continue without partitioning this file.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# 3.1 prepDyn with \"balanced\" round 1\n",
    "prepDyn(input_file = \"../test_data/empirical/empirical1_deSa/3_balanced/round1/\",  \n",
    "         output_file=\"../test_data/empirical/empirical1_deSa/3_balanced/round1/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_method=\"balanced\",partitioning_round=1,\n",
    "         sequence_names=True,log=True)\n",
    "\n",
    "# 3.2 prepDyn with \"balanced\" round 2\n",
    "prepDyn(input_file = \"../test_data/empirical/empirical1_deSa/3_balanced/round2/\", \n",
    "         output_file=\"../test_data/empirical/empirical1_deSa/3_balanced/round2/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_method=\"balanced\",partitioning_round=2,\n",
    "         sequence_names=True,log=True)\n",
    "\n",
    "# 3.3 prepDyn with \"balanced\" round 3\n",
    "prepDyn(input_file = \"../test_data/empirical/empirical1_deSa/3_balanced/round3/\", \n",
    "         output_file=\"../test_data/empirical/empirical1_deSa/3_balanced/round3/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_method=\"balanced\",partitioning_round=3,\n",
    "         sequence_names=True,log=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b523f23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 4.1 prepDyn with \"conservative\" round 1\n",
    "prepDyn(input_file = \"../test_data/empirical/empirical1_deSa/4_conservative/round1\", \n",
    "         output_file=\"../test_data/empirical/empirical1_deSa/4_conservative/round1/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_method=\"conservative\",partitioning_round=1,\n",
    "         sequence_names=True,log=True)\n",
    "\n",
    "# 4.2 prepDyn with \"conservative\" round 2\n",
    "prepDyn(input_file = \"../test_data/empirical/empirical1_deSa/4_conservative/round2\", \n",
    "         output_file=\"../test_data/empirical/empirical1_deSa/4_conservative/round2/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_method=\"conservative\",partitioning_round=2,\n",
    "         sequence_names=True,log=True)\n",
    "\n",
    "# 4.3 prepDyn with \"conservative\" round 3\n",
    "prepDyn(input_file = \"../test_data/empirical/empirical1_deSa/4_conservative/round3\", \n",
    "         output_file=\"../test_data/empirical/empirical1_deSa/4_conservative/round3/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_method=\"conservative\",partitioning_round=3,\n",
    "         sequence_names=True,log=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc5afe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 5.1 prepDyn with \"equal\" round 1\n",
    "prepDyn(input_file = \"../test_data/empirical/empirical1_deSa/5_equal/round1/\", \n",
    "         output_file=\"../test_data/empirical/empirical1_deSa/5_equal/round1/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_method=\"equal\",partitioning_round=1,\n",
    "         sequence_names=True,log=True)\n",
    "\n",
    "# 5.2 prepDyn with \"equal\" round 2\n",
    "prepDyn(input_file = \"../test_data/empirical/empirical1_deSa/5_equal/round2/\", \n",
    "         output_file=\"../test_data/empirical/empirical1_deSa/5_equal/round2/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_method=\"equal\",partitioning_round=2,\n",
    "         sequence_names=True,log=True)\n",
    "\n",
    "# 5.3 prepDyn with \"equal\" round 3\n",
    "prepDyn(input_file = \"../test_data/empirical/empirical1_deSa/5_equal/round3/\", \n",
    "         output_file=\"../test_data/empirical/empirical1_deSa/5_equal/round3/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_method=\"equal\",partitioning_round=3,\n",
    "         sequence_names=True,log=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e98ffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 6. prepDyn with \"max\"\n",
    "prepDyn(input_file = \"../test_data/empirical/empirical1_deSa/6_max/\", \n",
    "         output_file=\"../test_data/empirical/empirical1_deSa/6_max/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_method=\"max\",\n",
    "         sequence_names=True,log=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc66f0e",
   "metadata": {},
   "source": [
    "### Chiari et al. (2012)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b2dd8c",
   "metadata": {},
   "source": [
    "Empirical example of Amniota including 16 leaves and 248 gene alignments (concatenated length of 187,026). In the no preprocessed files, empty sequences were deleted to avoid downstreams problems (PhyG v.1.3 does not accept empty sequences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ede4aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing my_ENSGALG00000001741.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000007931.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000020605.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000001949.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000007520.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000015485.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000015889.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000004773.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000016119.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000016409.macse_DNA_gb.fasta\n",
      "  Removed 2 empty sequences.\n",
      "Processing my_ENSGALG00000001241.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000001413.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000015334.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000004431.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000002002.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000015339.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000005902.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000009369.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000003046.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000013227.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000016973.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000003924.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000014226.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000005807.macse_DNA_gb.fasta\n",
      "  Removed 2 empty sequences.\n",
      "Processing my_ENSGALG00000012529.macse_DNA_gb.fasta\n",
      "  Removed 1 empty sequences.\n",
      "Processing my_ENSGALG00000012798.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000012034.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000002745.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000001568.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000016320.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000004220.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000014920.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000003954.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000016633.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000012233.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000012365.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000008758.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000004554.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000017153.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000001005.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000005229.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000003337.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000005834.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000012355.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000010243.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000000041.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000013962.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000010315.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000005694.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000015917.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000014447.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000017289.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000002973.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000013352.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000016996.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000003907.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000016479.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000010298.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000010618.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000000169.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000003702.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000008454.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000015689.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000012011.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000005832.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000009264.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000003331.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000004825.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000005516.macse_DNA_gb.fasta\n",
      "  Removed 1 empty sequences.\n",
      "Processing my_ENSGALG00000003709.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000014648.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000009961.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000007289.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000005337.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000016753.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000008338.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000001550.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000001782.macse_DNA_gb.fasta\n",
      "  Removed 1 empty sequences.\n",
      "Processing my_ENSGALG00000016666.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000014786.macse_DNA_gb.fasta\n",
      "  Removed 1 empty sequences.\n",
      "Processing my_ENSGALG00000010848.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000012109.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000004953.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000006206.macse_DNA_gb.fasta\n",
      "  Removed 1 empty sequences.\n",
      "Processing my_ENSGALG00000008172.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000001217.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000015729.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000004818.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000013160.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000009879.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000007711.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000015405.macse_DNA_gb.fasta\n",
      "  Removed 1 empty sequences.\n",
      "Processing my_ENSGALG00000016122.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000001000.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000009085.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000011887.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000006276.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000006856.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000011917.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000006001.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000011649.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000017125.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000006439.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000001689.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000003807.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000005309.macse_DNA_gb.fasta\n",
      "  Removed 2 empty sequences.\n",
      "Processing my_ENSGALG00000001362.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000002504.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000004696.macse_DNA_gb.fasta\n",
      "  Removed 1 empty sequences.\n",
      "Processing my_ENSGALG00000008916.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000000223.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000002210.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000002514.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000015397.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000011403.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000004686.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000015436.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000009829.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000002194.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000008517.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000011555.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000012149.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000012312.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000009228.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000008812.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000017129.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000003107.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000012244.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000011965.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000006824.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000014862.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000004259.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000006425.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000006156.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000017447.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000014038.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000004302.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000014991.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000014163.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000012259.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000013863.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000006767.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000012013.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000016536.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000004779.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000004269.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000016241.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000002262.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000000374.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000008496.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000005820.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000010178.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000001179.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000016731.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000010671.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000009061.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000007315.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000002433.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000008130.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000007591.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000015439.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000015330.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000007024.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000002969.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000000811.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000012611.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000016357.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000011885.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000012724.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000011452.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000008314.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000011184.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000015225.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000016909.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000007484.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000004588.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000011966.macse_DNA_gb.fasta\n",
      "  Removed 1 empty sequences.\n",
      "Processing my_ENSGALG00000004372.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000011905.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000012543.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000010641.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000009313.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000015995.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000015402.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000006066.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000007492.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000014247.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000010005.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000011192.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000001452.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000014115.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000011323.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000013599.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000001976.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000013021.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000012379.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000000707.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000015240.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000004583.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000002921.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000003936.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000009288.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000002490.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000012964.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000014824.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000006619.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000012196.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000011950.macse_DNA_gb.fasta\n",
      "  Removed 1 empty sequences.\n",
      "Processing my_ENSGALG00000015326.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000001529.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000012568.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000005900.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000015451.macse_DNA_gb.fasta\n",
      "  Removed 1 empty sequences.\n",
      "Processing my_ENSGALG00000011174.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000006731.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000008632.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000014570.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000005002.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000015200.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000000814.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000001803.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000001750.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000015976.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000000921.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000004357.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000001668.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000014445.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000008138.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000004143.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000009975.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000006113.macse_DNA_gb.fasta\n",
      "  Removed 1 empty sequences.\n",
      "Processing my_ENSGALG00000001250.macse_DNA_gb.fasta\n",
      "  Removed 1 empty sequences.\n",
      "Processing my_ENSGALG00000004899.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000003700.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000005182.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000016053.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000002774.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000005670.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000007705.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000013041.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000015285.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000006974.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000008108.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000016847.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000011434.macse_DNA_gb.fasta\n",
      "  Removed 1 empty sequences.\n",
      "Processing my_ENSGALG00000015308.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000005758.macse_DNA_gb.fasta\n",
      "  No empty sequences found.\n",
      "Processing my_ENSGALG00000008229.macse_DNA_gb.fasta\n",
      "  Removed 1 empty sequences.\n"
     ]
    }
   ],
   "source": [
    "# 1. No preprocessing (already written locally)\n",
    "from Bio import SeqIO\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the working directory (current directory)\n",
    "directory = Path(\"../test_data/empirical/empirical2_Chiari/1_noPreprocessing/\")\n",
    "\n",
    "# Characters considered as gaps (edit as needed)\n",
    "gap_chars = set(\"-?.\")\n",
    "\n",
    "# Log file to record deleted sequences\n",
    "log_file = directory / \"deleted_empty_sequences.log\"\n",
    "\n",
    "# Clear previous log\n",
    "log_file.write_text(\"Log of deleted empty sequences:\\n\\n\")\n",
    "\n",
    "# Process all FASTA files in directory and subdirectories\n",
    "for fasta_file in directory.rglob(\"*.fa*\"):\n",
    "    print(f\"Processing {fasta_file.name}\")\n",
    "    \n",
    "    sequences = list(SeqIO.parse(fasta_file, \"fasta\"))\n",
    "\n",
    "    deleted_ids = []\n",
    "    non_empty_sequences = []\n",
    "    \n",
    "    for record in sequences:\n",
    "        seq = str(record.seq)\n",
    "        if all(base in gap_chars for base in seq):\n",
    "            deleted_ids.append(record.id)\n",
    "        else:\n",
    "            non_empty_sequences.append(record)\n",
    "\n",
    "    # Overwrite file with non-empty sequences\n",
    "    SeqIO.write(non_empty_sequences, fasta_file, \"fasta\")\n",
    "\n",
    "    # Write to log\n",
    "    if deleted_ids:\n",
    "        with open(log_file, \"a\") as log:\n",
    "            log.write(f\"{fasta_file}:\\n\")\n",
    "            for seq_id in deleted_ids:\n",
    "                log.write(f\"  - {seq_id}\\n\")\n",
    "            log.write(\"\\n\")\n",
    "\n",
    "        print(f\"  Removed {len(deleted_ids)} empty sequences.\")\n",
    "    else:\n",
    "        print(\"  No empty sequences found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15560ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. With preprocessing, without partitioning\n",
    "prepDyn(input_file = \"../test_data/empirical/empirical2_Chiari/1_noPreprocessing/\", \n",
    "         output_file=\"../test_data/empirical/empirical2_Chiari/2_noPartitioning/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_method=None,\n",
    "         sequence_names=True,log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d120c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: 'balanced' partitioning with partitioning_round=2 was skipped for this alignment. This typically happens when the alignment has fewer blocks of missing data ('?') than required. The process will continue without partitioning this file.\n",
      "\n",
      "\n",
      "WARNING: 'balanced' partitioning with partitioning_round=2 was skipped for this alignment. This typically happens when the alignment has fewer blocks of missing data ('?') than required. The process will continue without partitioning this file.\n",
      "\n",
      "\n",
      "WARNING: 'balanced' partitioning with partitioning_round=2 was skipped for this alignment. This typically happens when the alignment has fewer blocks of missing data ('?') than required. The process will continue without partitioning this file.\n",
      "\n",
      "\n",
      "WARNING: 'balanced' partitioning with partitioning_round=2 was skipped for this alignment. This typically happens when the alignment has fewer blocks of missing data ('?') than required. The process will continue without partitioning this file.\n",
      "\n",
      "\n",
      "WARNING: 'balanced' partitioning with partitioning_round=2 was skipped for this alignment. This typically happens when the alignment has fewer blocks of missing data ('?') than required. The process will continue without partitioning this file.\n",
      "\n",
      "\n",
      "WARNING: 'balanced' partitioning with partitioning_round=3 was skipped for this alignment. This typically happens when the alignment has fewer blocks of missing data ('?') than required. The process will continue without partitioning this file.\n",
      "\n",
      "\n",
      "WARNING: 'balanced' partitioning with partitioning_round=3 was skipped for this alignment. This typically happens when the alignment has fewer blocks of missing data ('?') than required. The process will continue without partitioning this file.\n",
      "\n",
      "\n",
      "WARNING: 'balanced' partitioning with partitioning_round=3 was skipped for this alignment. This typically happens when the alignment has fewer blocks of missing data ('?') than required. The process will continue without partitioning this file.\n",
      "\n",
      "\n",
      "WARNING: 'balanced' partitioning with partitioning_round=3 was skipped for this alignment. This typically happens when the alignment has fewer blocks of missing data ('?') than required. The process will continue without partitioning this file.\n",
      "\n",
      "\n",
      "WARNING: 'balanced' partitioning with partitioning_round=3 was skipped for this alignment. This typically happens when the alignment has fewer blocks of missing data ('?') than required. The process will continue without partitioning this file.\n",
      "\n",
      "\n",
      "WARNING: 'balanced' partitioning with partitioning_round=3 was skipped for this alignment. This typically happens when the alignment has fewer blocks of missing data ('?') than required. The process will continue without partitioning this file.\n",
      "\n",
      "\n",
      "WARNING: 'balanced' partitioning with partitioning_round=3 was skipped for this alignment. This typically happens when the alignment has fewer blocks of missing data ('?') than required. The process will continue without partitioning this file.\n",
      "\n",
      "\n",
      "WARNING: 'balanced' partitioning with partitioning_round=3 was skipped for this alignment. This typically happens when the alignment has fewer blocks of missing data ('?') than required. The process will continue without partitioning this file.\n",
      "\n",
      "\n",
      "WARNING: 'balanced' partitioning with partitioning_round=3 was skipped for this alignment. This typically happens when the alignment has fewer blocks of missing data ('?') than required. The process will continue without partitioning this file.\n",
      "\n",
      "\n",
      "WARNING: 'balanced' partitioning with partitioning_round=3 was skipped for this alignment. This typically happens when the alignment has fewer blocks of missing data ('?') than required. The process will continue without partitioning this file.\n",
      "\n",
      "\n",
      "WARNING: 'balanced' partitioning with partitioning_round=3 was skipped for this alignment. This typically happens when the alignment has fewer blocks of missing data ('?') than required. The process will continue without partitioning this file.\n",
      "\n",
      "\n",
      "WARNING: 'balanced' partitioning with partitioning_round=3 was skipped for this alignment. This typically happens when the alignment has fewer blocks of missing data ('?') than required. The process will continue without partitioning this file.\n",
      "\n",
      "\n",
      "WARNING: 'balanced' partitioning with partitioning_round=3 was skipped for this alignment. This typically happens when the alignment has fewer blocks of missing data ('?') than required. The process will continue without partitioning this file.\n",
      "\n",
      "\n",
      "WARNING: 'balanced' partitioning with partitioning_round=3 was skipped for this alignment. This typically happens when the alignment has fewer blocks of missing data ('?') than required. The process will continue without partitioning this file.\n",
      "\n",
      "\n",
      "WARNING: 'balanced' partitioning with partitioning_round=3 was skipped for this alignment. This typically happens when the alignment has fewer blocks of missing data ('?') than required. The process will continue without partitioning this file.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# 3.1 prepDyn with \"balanced\" round 1\n",
    "prepDyn(input_file = \"../test_data/empirical/empirical2_Chiari/1_noPreprocessing\",  \n",
    "         output_file=\"../test_data/empirical/empirical2_Chiari/3_balanced/round1/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_method=\"balanced\",partitioning_round=1,\n",
    "         sequence_names=True,log=True)\n",
    "\n",
    "# 3.2 prepDyn with \"balanced\" round 2\n",
    "prepDyn(input_file = \"../test_data/empirical/empirical2_Chiari/1_noPreprocessing\", \n",
    "         output_file=\"../test_data/empirical/empirical2_Chiari/3_balanced/round2/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_method=\"balanced\",partitioning_round=2,\n",
    "         sequence_names=True,log=True)\n",
    "\n",
    "# 3.3 prepDyn with \"balanced\" round 3\n",
    "prepDyn(input_file = \"../test_data/empirical/empirical2_Chiari/1_noPreprocessing\", \n",
    "         output_file=\"../test_data/empirical/empirical2_Chiari/3_balanced/round3/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_method=\"balanced\",partitioning_round=3,\n",
    "         sequence_names=True,log=True)\n",
    "'''\n",
    "\n",
    "# 3.3 prepDyn with \"balanced\" round 4\n",
    "prepDyn(input_file = \"../test_data/empirical/empirical2_Chiari/1_noPreprocessing\", \n",
    "         output_file=\"../test_data/empirical/empirical2_Chiari/3_balanced/round4/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_method=\"balanced\",partitioning_round=4,\n",
    "         sequence_names=True,log=True)\n",
    "\n",
    "# 3.3 prepDyn with \"balanced\" round 5\n",
    "prepDyn(input_file = \"../test_data/empirical/empirical2_Chiari/1_noPreprocessing\", \n",
    "         output_file=\"../test_data/empirical/empirical2_Chiari/3_balanced/round5/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_method=\"balanced\",partitioning_round=5,\n",
    "         sequence_names=True,log=True)\n",
    "\n",
    "# 3.3 prepDyn with \"balanced\" round 6\n",
    "prepDyn(input_file = \"../test_data/empirical/empirical2_Chiari/1_noPreprocessing\", \n",
    "         output_file=\"../test_data/empirical/empirical2_Chiari/3_balanced/round6/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_method=\"balanced\",partitioning_round=6,\n",
    "         sequence_names=True,log=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7063e3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 prepDyn with \"conservative\" round 1\n",
    "prepDyn(input_file = \"../test_data/empirical/empirical2_Chiari/1_noPreprocessing\",  \n",
    "         output_file=\"../test_data/empirical/empirical2_Chiari/4_conservative/round1/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_method=\"conservative\",partitioning_round=1,\n",
    "         sequence_names=True,log=True)\n",
    "\n",
    "# 4.2 prepDyn with \"conservative\" round 2\n",
    "prepDyn(input_file = \"../test_data/empirical/empirical2_Chiari/1_noPreprocessing\", \n",
    "         output_file=\"../test_data/empirical/empirical2_Chiari/4_conservative/round2/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_method=\"conservative\",partitioning_round=2,\n",
    "         sequence_names=True,log=True)\n",
    "\n",
    "# 4.3 prepDyn with \"conservative\" round 3\n",
    "prepDyn(input_file = \"../test_data/empirical/empirical2_Chiari/1_noPreprocessing\", \n",
    "         output_file=\"../test_data/empirical/empirical2_Chiari/4_conservative/round3/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_method=\"conservative\",partitioning_round=3,\n",
    "         sequence_names=True,log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a334fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 prepDyn with \"equal\" round 1\n",
    "prepDyn(input_file = \"../test_data/empirical/empirical2_Chiari/1_noPreprocessing\",  \n",
    "         output_file=\"../test_data/empirical/empirical2_Chiari/5_equal/round1/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_method=\"equal\",partitioning_round=1,\n",
    "         sequence_names=True,log=True)\n",
    "\n",
    "# 5.2 prepDyn with \"equal\" round 2\n",
    "prepDyn(input_file = \"../test_data/empirical/empirical2_Chiari/1_noPreprocessing\", \n",
    "         output_file=\"../test_data/empirical/empirical2_Chiari/5_equal/round2/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_method=\"equal\",partitioning_round=2,\n",
    "         sequence_names=True,log=True)\n",
    "\n",
    "# 5.3 prepDyn with \"equal\" round 3\n",
    "prepDyn(input_file = \"../test_data/empirical/empirical2_Chiari/1_noPreprocessing\", \n",
    "         output_file=\"../test_data/empirical/empirical2_Chiari/5_equal/round3/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_method=\"equal\",partitioning_round=3,\n",
    "         sequence_names=True,log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ced36190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. prepDyn with \"max\"\n",
    "prepDyn(input_file = \"../test_data/empirical/empirical2_Chiari/1_noPreprocessing/\", \n",
    "         output_file=\"../test_data/empirical/empirical2_Chiari//6_max/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_method=\"max\",\n",
    "         sequence_names=True,log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22259c0",
   "metadata": {},
   "source": [
    "### Nakamura et al. (2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7433b006",
   "metadata": {},
   "source": [
    "Empirical example of the *Boana pulchella* group using XX gene alignments, including one leaf with hDNA sequences (*Boana cymbalum*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e32c565",
   "metadata": {},
   "source": [
    "## TRASH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7643a37",
   "metadata": {},
   "source": [
    "Other functions used in previous versions of this code (now discarded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3810619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0. Align sequences statically\n",
      "sp1: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp2: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp3: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp4: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp5: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp6: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n",
      "\n",
      "\n",
      "preprocessed\n",
      "sp1: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp2: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp3: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp4: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp5: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp6: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def replace_w_blocks_with_question(alignment_dict):\n",
    "    cleaned_alignment = {}\n",
    "\n",
    "    for seq_name, seq in alignment_dict.items():\n",
    "        sequence = list(seq)\n",
    "\n",
    "        # Use regex to find blocks with at least 15 characters of w/W/- combined\n",
    "        # but must contain at least 15 w or W\n",
    "        pattern = re.finditer(r'([wW\\-]{15,})', ''.join(sequence))\n",
    "\n",
    "        for match in pattern:\n",
    "            block = match.group()\n",
    "            start, end = match.start(), match.end()\n",
    "\n",
    "            # Count number of w/W in the block\n",
    "            w_count = sum(1 for c in block if c in 'wW')\n",
    "            if w_count >= 15:\n",
    "                for i in range(start, end):\n",
    "                    sequence[i] = '?'\n",
    "\n",
    "        cleaned_alignment[seq_name] = ''.join(sequence)\n",
    "\n",
    "    return cleaned_alignment\n",
    "\n",
    "\n",
    "# Input data\n",
    "alignment = {\n",
    "    'sp1': \"ttcatcgtcgacgtaatagt-ctccaccg-cgccac??????-------a\",\n",
    "    'sp2': \"ttcwwwwww------wwwwww--wwwccg-cgccaccgtcgc-------?\",\n",
    "    'sp3': \"ttcatcgtcgacgtaatagt-ctccaccg-cgccaccgt???-------a\",\n",
    "    'sp4': \"ttcatcgtcgacgtaatagt-ctccaccg-cgccaccgtcgc-------a\",\n",
    "    'sp5': \"ttcatcgtcgacgtaatagt-ctccaccg-cgccaccgtcgc-------a\",\n",
    "    'sp6': \"ttcatcgtcgacgtaatagt-ctccaccg-cgccaccgtcgc-------a\"\n",
    "} \n",
    "\n",
    "print(\"Step 0. Align sequences statically\")\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "print('\\n')\n",
    "print(\"preprocessed\")\n",
    "print_colored_alignment(replace_w_blocks_with_question(alignment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33ab3c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Step 1.2. Delete terminal orphan nucleotides (given a threshold)\n",
      "sp1: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp2: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp3: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp4: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp5: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp6: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "\n",
      "\n",
      "Step 2.3 Insert pound signs if INTERNAL block length > X (e.g. X = 1)\n",
      "sp1: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[30m#\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[30m#\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp2: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[30m#\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[30m#\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[30m#\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp3: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[30m#\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[30m#\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp4: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[30m#\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[30m#\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp5: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[30m#\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[30m#\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp6: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[30m#\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[30m#\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "# Step 1.2: Delete orphan nucleotides in the first and last positions #\n",
    "#######################################################################\n",
    "\n",
    "def delete_orphan_nucleotides(alignment, orphan_threshold=10):\n",
    "    \"\"\"\n",
    "    Orphan nucleotides are artifacts from static alignment, in which one or a few nucleotides\n",
    "    are accidentally not aligned in the beginning or end of the sequences. Manual deletion or\n",
    "    local alignment should be performed, but here I provide a heuristic, optional function to\n",
    "    tentatively identify and delete orphan nucleotides in terminal positions.\n",
    "    \n",
    "    For each sequence in the alignment, replaces orphan nucleotides at the \n",
    "    beginning or end of the sequence with a hyphen if they are followed or preceded \n",
    "    by a number of gaps equal to or greater than the orphan_threshold, but only if \n",
    "    the number of orphan nucleotides are fewer than or equal to the orphan_threshold in length.\n",
    "    \n",
    "    Parameters:\n",
    "        alignment (dict): Dictionary with sequence names as keys and sequences as values.\n",
    "        orphan_threshold (int): The number of consecutive gaps after the orphan nucleotide\n",
    "                                 required for it to be considered an orphan.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Modified alignment with orphan nucleotides replaced by '-'.\n",
    "    \"\"\"\n",
    "    \n",
    "    for seq_name, seq in alignment.items():\n",
    "        # Check the first position for orphan nucleotides (beginning of sequence)\n",
    "        if seq[0] != \"-\":\n",
    "            i = 0\n",
    "            # Find contiguous orphan nucleotides at the beginning of the sequence\n",
    "            while i < len(seq) and seq[i] != \"-\" and seq[i] != \"#\":\n",
    "                i += 1\n",
    "            \n",
    "            # If the contiguous orphan nucleotides are followed by enough gaps\n",
    "            if i > 0 and seq[i:i+orphan_threshold] == \"-\" * orphan_threshold:\n",
    "                # Only replace if the length of contiguous orphan nucleotides is <= orphan_threshold\n",
    "                if i <= orphan_threshold:\n",
    "                    seq = \"-\" * i + seq[i:]\n",
    "        \n",
    "        # Check the last position for orphan nucleotides (end of sequence)\n",
    "        if seq[-1] != \"-\":\n",
    "            i = len(seq) - 1\n",
    "            # Find contiguous orphan nucleotides at the end of the sequence\n",
    "            while i >= 0 and seq[i] != \"-\" and seq[i] != \"#\":\n",
    "                i -= 1\n",
    "            \n",
    "            # If the contiguous orphan nucleotides are preceded by enough gaps\n",
    "            if i < len(seq) - 1 and seq[i - orphan_threshold:i] == \"-\" * orphan_threshold:\n",
    "                # Only replace if the length of contiguous orphan nucleotides is <= orphan_threshold\n",
    "                if len(seq) - i - 1 <= orphan_threshold:\n",
    "                    seq = seq[:i+1] + \"-\" * (len(seq) - i - 1)\n",
    "        \n",
    "        # Update the sequence in the alignment dictionary\n",
    "        alignment[seq_name] = seq\n",
    "    \n",
    "    return alignment\n",
    "\n",
    "print('\\n')\n",
    "print(\"Step 1.2. Delete terminal orphan nucleotides (given a threshold)\")\n",
    "print_colored_alignment(delete_orphan_nucleotides(alignment, orphan_threshold=6))\n",
    "        \n",
    "###############################################################################\n",
    "# Step 2.3: Insert breaks in internal gap opening/closure if block length > X #\n",
    "###############################################################################\n",
    "\n",
    "def add_breaks_internal(alignment, X=1, Y=None):\n",
    "    \"\"\"\n",
    "    Add '#' flanking internal gap blocks if the gap block size exceeds X.\n",
    "    \n",
    "    Parameters:\n",
    "        alignment (dict): Dictionary where keys are sequence names and values are sequences.\n",
    "        X (int): Threshold for the minimum gap size to trigger '#' insertion (default: X = 1).\n",
    "        Y (list of ranges): Optional list of ranges (start, end) defining blocks of gaps that must be flanked by '#'. Indexing of columns in the matrix starts with zero (not one). In internal gap blocks, Y should be specified if the minimum size of known missing data blocks (e.g. incomplete sequences due to primers) is not larger than the maximum size of gap blocks.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated alignment with '#' flanking internal gap blocks.\n",
    "    \"\"\"\n",
    "    # Determine the length of the sequences\n",
    "    seq_length = len(next(iter(alignment.values())))\n",
    "\n",
    "    # Initialize a list to keep track of positions that need '#' in all sequences\n",
    "    hash_positions = [False] * seq_length\n",
    "\n",
    "    # Iterate through each sequence to find internal gap block\n",
    "    for seq in alignment.values():\n",
    "        i = 0  # Start from position 0\n",
    "        while i < seq_length:\n",
    "            if seq[i] == '-':\n",
    "                # Find the start of a gap block\n",
    "                start = i\n",
    "                while i < seq_length and seq[i] == '-':\n",
    "                    i += 1\n",
    "                end = i\n",
    "                # Mark the position for this gap opening/closure ONLY if the gap block is larger than X\n",
    "                if (end - start) > X:\n",
    "                    hash_positions[start] = True  # Mark the start of gap block\n",
    "                    if end < seq_length:\n",
    "                        hash_positions[end] = True  # Mark the end of gap block\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    # Add '#' around the ranges specified in Y\n",
    "    if Y:\n",
    "        for range_start, range_end in Y:\n",
    "            if range_start >= 0 and range_end < seq_length:\n",
    "                hash_positions[range_start] = True\n",
    "                if range_end + 1 < seq_length:\n",
    "                    hash_positions[range_end + 1] = True\n",
    "                    \n",
    "    # Add '#' at the identified positions in each sequence\n",
    "    for key in alignment:\n",
    "        new_seq = []\n",
    "        for i in range(seq_length):\n",
    "            if i > 0 and hash_positions[i]:  # Avoid position 1 and single-gap blocks\n",
    "                new_seq.append('#')\n",
    "            new_seq.append(alignment[key][i])\n",
    "        if hash_positions[-1]:\n",
    "            new_seq.append('#')  # Ensure we append '#' at the end if necessary\n",
    "        \n",
    "        # After modifying the sequence, remove consecutive '#' characters\n",
    "        modified_seq = ''.join(new_seq)\n",
    "        modified_seq = re.sub(r'#+', '#', modified_seq)  # Replace multiple '#' with a single '#'\n",
    "        \n",
    "        alignment[key] = modified_seq\n",
    "    \n",
    "    return alignment  # Return the modified alignment\n",
    "\n",
    "\n",
    "# Print\n",
    "add_breaks_internal(alignment, X=2) # insert '#' flanking internal gap blocks larger than 2\n",
    "#add_breaks_internal(alignment, X=2, Y=[(37, 42)]) # insert '#' in columns 37 and 42\n",
    "print(\"\\n\") # Jump a line\n",
    "print(\"Step 2.3 Insert pound signs if INTERNAL block length > X (e.g. X = 1)\")\n",
    "print_colored_alignment(alignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57ea93e",
   "metadata": {},
   "source": [
    "Some functions to use the trimal method. However, even when we use the \"terminal_only\" option, internal columns are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9975b90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0. Align sequences statically\n",
      "sp1: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp2: \u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[32mg\u001b[0m\u001b[97mn\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp3: \u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp4: \u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp5: \u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp6: \u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "\n",
      "\n",
      "Step 1.2. Trim problematic positions using trimai\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Alignment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m alignment \u001b[38;5;241m=\u001b[39m dict_to_multiple_seq_alignment(alignment)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Convert Biopython to pytrimal objects\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m alignment \u001b[38;5;241m=\u001b[39m \u001b[43mAlignment\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_biopython(alignment)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Trim using trimal\u001b[39;00m\n\u001b[1;32m     26\u001b[0m trimmer \u001b[38;5;241m=\u001b[39m AutomaticTrimmer(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgappyout\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'Alignment' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Input data\n",
    "alignment = {\n",
    "    'sp1': \"tt-------------agtagt-ctccaccg-cgccaccgtcgc-------a\",\n",
    "    'sp2': \"-ccaccgtc------gngtag-ctccaccg-cgc-------gccaacagta\",\n",
    "    'sp3': \"-ccaccgtcgccaacagtagt------ccg--------tt-----------\",\n",
    "    'sp4': \"------ct-----------gt-ctccaccg-cgccacc-tcgcca-aagta\",\n",
    "    'sp5': \"--caccgtcgccaacagtagt-ctccaccg-cgccaccgtcgcca-cag-a\",\n",
    "    'sp6': \"--caccgtc-ccaacagtagt-ctccaccgtcgcaccg-tcgcaa-c----\"\n",
    "} \n",
    "\n",
    "print(\"Step 0. Align sequences statically\")\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "#print('\\n')\n",
    "#print(\"Step 1.1 Delete columns presenting gaps in all taxa\")\n",
    "#print_colored_alignment(remove_all_gap_columns(alignment))\n",
    "\n",
    "print('\\n')\n",
    "print(\"Step 1.2. Trim problematic positions using trimai\")\n",
    "\n",
    "# Convert dictionary to Biopython objects\n",
    "alignment = dict_to_multiple_seq_alignment(alignment)\n",
    "# Convert Biopython to pytrimal objects\n",
    "alignment = Alignment.from_biopython(alignment)\n",
    "# Trim using trimal\n",
    "trimmer = AutomaticTrimmer(method=\"gappyout\") \n",
    "#trimmer = AutomaticTrimmer(method=\"automated1\")\n",
    "alignment = trimmer.trim(alignment)\n",
    "alignment = pytrimal.TrimmedAlignment.terminal_only(alignment)\n",
    "\n",
    "\n",
    "# Convert the trimal to Biopython\n",
    "alignment = Alignment.to_biopython(alignment)\n",
    "# Convert MultipleSeqAlignment to dictionary\n",
    "alignment = {record.id: str(record.seq) for record in alignment}\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "# Print\n",
    "#show_alignment(alignment)\n",
    "\n",
    "# Write in fasta format\n",
    "#Alignment.dump(alignment, file=\"Example_trimai.fasta\", format=\"fasta\")\n",
    "\n",
    "def calculate_statistics(values):\n",
    "    mean = sum(values) / len(values)  # Mean = Sum of values / Number of values\n",
    "    min_value = min(values)           # Minimum value\n",
    "    max_value = max(values)           # Maximum value\n",
    "    \n",
    "    return mean, min_value, max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb5c2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OLDaddSeq(\n",
    "    alignment,\n",
    "    new_seqs,\n",
    "    output,\n",
    "    write_names=True,\n",
    "    orphan_threshold=10,\n",
    "    log=False,\n",
    "    n2question=None,\n",
    "    gaps2question=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Add new sequences to an existing alignment using MAFFT, clean and standardize the result.\n",
    "\n",
    "    Steps performed:\n",
    "    1. Remove '#' columns from original alignment.\n",
    "    2. Align new sequences with MAFFT using --add.\n",
    "    3. Trim orphan nucleotide blocks from new sequences.\n",
    "    4. Remove short DNA blocks near # in first and last partitions.\n",
    "    5. Replace terminal '-' with '?'.\n",
    "    6. Optionally replace all N/n with '?' in selected sequences.\n",
    "    7. Optionally replace long gap blocks with '?'.\n",
    "    8. Reinsert '#' columns.\n",
    "    9. Replace all-'?' blocks between '#' with '-'.\n",
    "    10. Write the result and an optional log file.\n",
    "\n",
    "    Parameters:\n",
    "        alignment (str or dict): Existing alignment file path (FASTA) or dictionary {id: sequence}.\n",
    "        new_seqs (str or dict): New sequences to add (FASTA path or dict {id: sequence}).\n",
    "        output (str): Path to write the updated alignment in FASTA format.\n",
    "        write_names (bool): Whether to write a _terminal_names.txt file listing sequence IDs.\n",
    "        orphan_threshold (int): Threshold to detect and remove orphan DNA blocks.\n",
    "        log (bool): If True, writes a log file with trimming and runtime information.\n",
    "        n2question (str, list or None): Replace 'N/n' with '?' in specific sequences:\n",
    "            - 'all': apply to all sequences\n",
    "            - str: apply to a single sequence ID\n",
    "            - list: apply to listed sequence IDs\n",
    "        gaps2question (int or None): Replace contiguous gap blocks larger than this threshold with '?'. Only applied to added sequences.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Example usage:\n",
    "        addSeq(\"alignment.fasta\", \"new.fasta\", \"updated.fasta\", n2question=\"seq123\", log=True)\n",
    "        addSeq(alignment_dict, new_dict, \"out.fas\", n2question='all')\n",
    "    \"\"\"\n",
    "\n",
    "    # Start timing the execution\n",
    "    start_time = time.time()\n",
    "    temp_files_to_remove = []  # Temporary files to be removed after execution\n",
    "    log_lines = [] \n",
    "\n",
    "    # Log the function call and parameters for reproducibility\n",
    "    if log:\n",
    "        cmd_used = f\"addSeq(alignment=..., new_seqs=..., output='{output}', write_names={write_names}, orphan_threshold={orphan_threshold}, log={log}, n2question={n2question}, gaps2question={gaps2question})\"\n",
    "        log_lines.append(f\"Command used: {cmd_used}\")\n",
    "        log_lines.append(\"\")\n",
    "\n",
    "    # === Step 1: Load and clean the alignment ===\n",
    "    def write_dict_to_temp_fasta(seq_dict):\n",
    "        records = [SeqRecord(Seq(seq), id=str(seq_id), description=\"\") for seq_id, seq in seq_dict.items()]\n",
    "        tmp = tempfile.NamedTemporaryFile(\"w+\", delete=False)\n",
    "        SeqIO.write(records, tmp, \"fasta\")\n",
    "        tmp.close()\n",
    "        return tmp.name\n",
    "\n",
    "    if isinstance(alignment, dict):\n",
    "        alignment_path = write_dict_to_temp_fasta(alignment)\n",
    "        temp_files_to_remove.append(alignment_path)\n",
    "    elif isinstance(alignment, str):\n",
    "        alignment_path = alignment\n",
    "    else:\n",
    "        raise ValueError(\"alignment must be a FASTA file path or a dictionary\")\n",
    "\n",
    "    records = list(SeqIO.parse(alignment_path, \"fasta\"))\n",
    "    if not records:\n",
    "        raise ValueError(\"Input alignment is empty or not found\")\n",
    "\n",
    "    aln_len = len(records[0].seq)\n",
    "    # Identify columns that are '#' characters to temporarily remove them for alignment\n",
    "    pound_cols = [i for i in range(aln_len) if any(rec.seq[i] == '#' for rec in records)]\n",
    "\n",
    "    # Log input alignment info\n",
    "    if log:\n",
    "        log_lines.append(f\"Input alignment: {len(records)} sequences\")\n",
    "        log_lines.append(f\"Input alignment: {len(pound_cols)} # columns\")\n",
    "\n",
    "    def remove_cols(seq, cols):\n",
    "        return ''.join(seq[i] for i in range(len(seq)) if i not in cols)\n",
    "\n",
    "    # Remove '#' columns\n",
    "    aln_no_pound = [\n",
    "        SeqRecord(Seq(remove_cols(str(rec.seq), pound_cols)), id=rec.id, description=\"\")\n",
    "        for rec in records\n",
    "    ]\n",
    "    # Write cleaned alignment to a temporary file\n",
    "    with tempfile.NamedTemporaryFile(\"w+\", delete=False) as aln_tmp:\n",
    "        SeqIO.write(aln_no_pound, aln_tmp, \"fasta\")\n",
    "        aln_path = aln_tmp.name\n",
    "        temp_files_to_remove.append(aln_path)\n",
    "\n",
    "    # === Step 2: Load new sequences ===\n",
    "    if isinstance(new_seqs, dict):\n",
    "        new_seqs_path = write_dict_to_temp_fasta(new_seqs)\n",
    "        new_seq_count = len(new_seqs)\n",
    "        temp_files_to_remove.append(new_seqs_path)\n",
    "    elif isinstance(new_seqs, str):\n",
    "        new_seq_count = sum(1 for _ in SeqIO.parse(new_seqs, \"fasta\"))\n",
    "        new_seqs_path = new_seqs\n",
    "    else:\n",
    "        raise ValueError(\"new_seqs must be a FASTA file path or a dictionary\")\n",
    "    # Log new sequence info\n",
    "    if log:\n",
    "        log_lines.append(f\"Input new sequences: {new_seq_count} sequences\")\n",
    "\n",
    "    # === Step 3: Align new sequences with MAFFT ===\n",
    "    with tempfile.NamedTemporaryFile(\"w+\", delete=False) as out_tmp:\n",
    "        out_path = out_tmp.name\n",
    "        temp_files_to_remove.append(out_path)\n",
    "\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            ['mafft', '--add', new_seqs_path, '--keeplength', '--preservecase', aln_path],\n",
    "            check=True,\n",
    "            stdout=open(out_path, 'w'),\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "        )\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        for file in temp_files_to_remove:\n",
    "            try:\n",
    "                os.remove(file)\n",
    "            except Exception:\n",
    "                pass\n",
    "        raise RuntimeError(f\"MAFFT failed!\\nCommand: {e.cmd}\\nExit status: {e.returncode}\\nMAFFT error output:\\n{e.stderr}\")\n",
    "\n",
    "    mafft_aligned_records = list(SeqIO.parse(out_path, \"fasta\"))\n",
    "    original_ids = {rec.id for rec in records}\n",
    "    new_records = [rec for rec in mafft_aligned_records if rec.id not in original_ids]\n",
    "\n",
    "\n",
    "    def replace_gap_blocks(seq, threshold, seq_id=None):\n",
    "        seq_list = list(seq)\n",
    "        replaced_log = []\n",
    "        i = 0\n",
    "        while i < len(seq_list):\n",
    "            if seq_list[i] == '-':\n",
    "                start = i\n",
    "                while i < len(seq_list) and seq_list[i] == '-':\n",
    "                    i += 1\n",
    "                if (i - start) > threshold:\n",
    "                    for j in range(start, i):\n",
    "                        seq_list[j] = '?'\n",
    "                    if seq_id:\n",
    "                        replaced_log.append(f\"{seq_id}: {i - start} contiguous '-' replaced with '?' at {start}â€“{i}\")\n",
    "            else:\n",
    "                i += 1\n",
    "        return ''.join(seq_list), replaced_log\n",
    "\n",
    "    def find_dna_blocks(seq_list, start, end):\n",
    "        blocks = []\n",
    "        i = start\n",
    "        while i < end:\n",
    "            if seq_list[i] not in \"-?#\":\n",
    "                s = i\n",
    "                while i < end and seq_list[i] not in \"-?#\":\n",
    "                    i += 1\n",
    "                e = i\n",
    "                blocks.append((s, e))\n",
    "            else:\n",
    "                i += 1\n",
    "        return blocks\n",
    "\n",
    "    def trim_orphan_blocks(seq, threshold, seq_id=None):\n",
    "        seq_list = list(seq)\n",
    "        trimmed_log = []\n",
    "\n",
    "        def find_blocks(seq_list):\n",
    "            blocks = []\n",
    "            i = 0\n",
    "            while i < len(seq_list):\n",
    "                if seq_list[i] not in \"-?#\":\n",
    "                    start = i\n",
    "                    while i < len(seq_list) and seq_list[i] not in \"-?#\":\n",
    "                        i += 1\n",
    "                    end = i\n",
    "                    blocks.append((start, end))\n",
    "                else:\n",
    "                    i += 1\n",
    "            return blocks\n",
    "\n",
    "        changed = True\n",
    "        while changed:\n",
    "            changed = False\n",
    "\n",
    "            while True:\n",
    "                blocks = find_blocks(seq_list)\n",
    "                if len(blocks) < 2:\n",
    "                    break\n",
    "                first_start, first_end = blocks[0]\n",
    "                next_start = blocks[1][0]\n",
    "                gap_count = seq_list[first_end:next_start].count('-') + seq_list[first_end:next_start].count('?')\n",
    "                size = first_end - first_start\n",
    "                if size < threshold and gap_count > threshold:\n",
    "                    deleted = ''.join(seq_list[first_start:first_end])\n",
    "                    seq_list[first_start:first_end] = ['-'] * size\n",
    "                    if seq_id:\n",
    "                        trimmed_log.append(f\"{seq_id}: Left {first_start}â€“{first_end} (size={size}, '{deleted}')\")\n",
    "                    changed = True\n",
    "                    continue\n",
    "                break\n",
    "\n",
    "            while True:\n",
    "                blocks = find_blocks(seq_list)\n",
    "                if len(blocks) < 2:\n",
    "                    break\n",
    "                last_start, last_end = blocks[-1]\n",
    "                prev_end = blocks[-2][1]\n",
    "                gap_count = seq_list[prev_end:last_start].count('-') + seq_list[prev_end:last_start].count('?')\n",
    "                size = last_end - last_start\n",
    "                if size < threshold and gap_count > threshold:\n",
    "                    deleted = ''.join(seq_list[last_start:last_end])\n",
    "                    seq_list[last_start:last_end] = ['-'] * size\n",
    "                    if seq_id:\n",
    "                        trimmed_log.append(f\"{seq_id}: Right {last_start}â€“{last_end} (size={size}, '{deleted}')\")\n",
    "                    changed = True\n",
    "                    continue\n",
    "                break\n",
    "\n",
    "        if pound_cols:\n",
    "            first_hash = min(pound_cols)\n",
    "            last_hash = max(pound_cols)\n",
    "            blocks = find_dna_blocks(seq_list, 0, first_hash)\n",
    "            if len(blocks) == 1:\n",
    "                s, e = blocks[0]\n",
    "                if e == first_hash and (e - s) < threshold:\n",
    "                    deleted = ''.join(seq_list[s:e])\n",
    "                    seq_list[s:e] = ['-'] * (e - s)\n",
    "                    if seq_id:\n",
    "                        trimmed_log.append(f\"{seq_id}: Left {s}â€“{e} (size={e - s}, '{deleted}')\")\n",
    "            blocks = find_dna_blocks(seq_list, last_hash + 1, len(seq_list))\n",
    "            if len(blocks) == 1:\n",
    "                s, e = blocks[0]\n",
    "                if s == last_hash + 1 and (e - s) < threshold:\n",
    "                    deleted = ''.join(seq_list[s:e])\n",
    "                    seq_list[s:e] = ['-'] * (e - s)\n",
    "                    if seq_id:\n",
    "                        trimmed_log.append(f\"{seq_id}: Right {s}â€“{e} (size={e - s}, '{deleted}')\")\n",
    "\n",
    "        return ''.join(seq_list), trimmed_log\n",
    "\n",
    "    trimmed_new_records = []\n",
    "    all_trim_logs = []\n",
    "    gaps2q_log = []\n",
    "    for rec in new_records:\n",
    "        trimmed_seq, seq_log = trim_orphan_blocks(str(rec.seq), orphan_threshold, seq_id=rec.id)\n",
    "        if gaps2question:\n",
    "            trimmed_seq, gap_log = replace_gap_blocks(trimmed_seq, gaps2question, seq_id=rec.id)\n",
    "            gaps2q_log.extend(gap_log)\n",
    "        trimmed_new_records.append(SeqRecord(Seq(trimmed_seq), id=rec.id, description=\"\"))\n",
    "        all_trim_logs.extend(seq_log)\n",
    "\n",
    "    processed_records = [rec for rec in mafft_aligned_records if rec.id in original_ids] + trimmed_new_records\n",
    "\n",
    "    updated_records = []\n",
    "    for rec in processed_records:\n",
    "        seq_chars = list(str(rec.seq))\n",
    "        for i in range(len(seq_chars)):\n",
    "            if seq_chars[i] == '-':\n",
    "                seq_chars[i] = '?'\n",
    "            else:\n",
    "                break\n",
    "        for i in range(len(seq_chars) - 1, -1, -1):\n",
    "            if seq_chars[i] == '-':\n",
    "                seq_chars[i] = '?'\n",
    "            else:\n",
    "                break\n",
    "        updated_records.append(SeqRecord(Seq(''.join(seq_chars)), id=rec.id, description=\"\"))\n",
    "\n",
    "    n2q_log = []\n",
    "    if n2question:\n",
    "        if isinstance(n2question, str) and n2question != 'all':\n",
    "            target_ids = {n2question}\n",
    "        elif isinstance(n2question, list):\n",
    "            target_ids = set(n2question)\n",
    "        elif n2question == 'all':\n",
    "            target_ids = {rec.id for rec in updated_records}\n",
    "        else:\n",
    "            target_ids = set()\n",
    "\n",
    "        for rec in updated_records:\n",
    "            if rec.id in target_ids:\n",
    "                seq_str = str(rec.seq)\n",
    "                count_n = seq_str.count('N') + seq_str.count('n')\n",
    "                if count_n > 0:\n",
    "                    rec.seq = Seq(seq_str.replace('N', '?').replace('n', '?'))\n",
    "                    n2q_log.append(f\"{rec.id}: {count_n} N/n replaced with ?\")\n",
    "\n",
    "    final_records = []\n",
    "    for rec in updated_records:\n",
    "        seq_list = list(str(rec.seq))\n",
    "        for col in sorted(pound_cols):\n",
    "            seq_list.insert(col, '#')\n",
    "        final_records.append(SeqRecord(Seq(''.join(seq_list)), id=rec.id, description=\"\"))\n",
    "\n",
    "    def process_blocks(seq):\n",
    "        seq_chars = list(seq)\n",
    "        blocks = []\n",
    "        start = 0\n",
    "        for i, c in enumerate(seq_chars):\n",
    "            if c == '#':\n",
    "                blocks.append((start, i))\n",
    "                start = i + 1\n",
    "        blocks.append((start, len(seq_chars)))\n",
    "        for (start, end) in blocks:\n",
    "            if all(seq_chars[i] == '?' for i in range(start, end)):\n",
    "                for i in range(start, end):\n",
    "                    seq_chars[i] = '-'\n",
    "        return ''.join(seq_chars)\n",
    "\n",
    "    final_output = [\n",
    "        SeqRecord(Seq(process_blocks(str(rec.seq))), id=rec.id, description=\"\")\n",
    "        for rec in final_records\n",
    "    ]\n",
    "\n",
    "    SeqIO.write(final_output, output, \"fasta\")\n",
    "    if write_names:\n",
    "        with open(output + \"_terminal_names.txt\", \"w\") as f:\n",
    "            for rec in final_output:\n",
    "                f.write(rec.id + \"\\n\")\n",
    "\n",
    "    if log:\n",
    "        elapsed = time.time() - start_time\n",
    "        log_lines.append(f\"Final alignment: {len(final_output)} sequences\")\n",
    "        log_lines.append(f\"Final alignment: {len(final_output[0].seq)} columns\")\n",
    "        log_lines.append(f\"Final alignment: {sum(1 for i in range(len(final_output[0].seq)) if any(rec.seq[i] == '#' for rec in final_output))} # columns\")\n",
    "        log_lines.append(\"\")\n",
    "        log_lines.append(\"Trimmed orphan blocks from new sequences:\")\n",
    "        log_lines.extend(all_trim_logs or [\"None\"])\n",
    "        if gaps2q_log:\n",
    "            log_lines.append(\"\")\n",
    "            log_lines.append(\"Gap block replacements:\")\n",
    "            log_lines.extend(gaps2q_log)\n",
    "        if n2q_log:\n",
    "            log_lines.append(\"\")\n",
    "            log_lines.append(\"N/n to ? replacements:\")\n",
    "            log_lines.extend(n2q_log)\n",
    "        log_lines.append(\"\")\n",
    "        log_lines.append(f\"Runtime: {elapsed:.2f} seconds\")\n",
    "        with open(output + \".log\", \"w\") as log_file:\n",
    "            log_file.write(\"\\n\".join(log_lines))\n",
    "\n",
    "    for file in temp_files_to_remove:\n",
    "        try:\n",
    "            os.remove(file)\n",
    "        except Exception:\n",
    "            pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
