{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8070de8d",
   "metadata": {},
   "source": [
    "# *prepDyn*: Preprocessing of missing data for dynamic homology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc4d943",
   "metadata": {},
   "source": [
    "In dynamic homology, data are typically preprocessed to distinguish differences in sequence length resulting from missing data or insertion-deletion events. However, previous empirical studies using POY/PhyG manually preprocessed missing data with varying approaches. Here we demonstrate that coding missing data with dashes (â€“) or IUPAC Ns increase tree costs and are biologically inappropriate. Although inserting pound signs (#) around blocks of missing data has been a common solution, it reduces the severity of homology tests and precludes the discovery of the optimal tree. Therefore, missing data should be coded with question marks (?) to minimize tree costs, whereas pound signs should be inserted only on highly conserved regions. To balance time complexity and severity of homology test, we propose a heuristic to successively partition data. All procedures are implemented in a collection of Python scripts to facilitate the preprocessing of input sequences to PhyG. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4b3d0a",
   "metadata": {},
   "source": [
    "## Pre-amble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09021b2e",
   "metadata": {},
   "source": [
    "Mafft should be installed locally in the system PATH (e.g. in Ubuntu, `$ sudo apt install mafft`). In addition, the following Python modules should be installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cc9efc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "from Bio import AlignIO, Entrez, SeqIO\n",
    "from Bio.Align import MultipleSeqAlignment\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import csv\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "#import pytrimal\n",
    "import re\n",
    "#import rich.console\n",
    "#import rich.panel\n",
    "#from rich_msa import RichAlignment\n",
    "import subprocess\n",
    "import tempfile\n",
    "from termcolor import colored\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1b3422",
   "metadata": {},
   "source": [
    "Additional custom functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e660b3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION 1: Define functions to visualize nucleotides in different colors\n",
    "# Define colors for each nucleotide (case insensitive)\n",
    "def color_nucleotide(nucleotide):\n",
    "    color_map = {\n",
    "        \"A\": \"red\",\n",
    "        \"T\": \"blue\",\n",
    "        \"G\": \"green\",\n",
    "        \"C\": \"yellow\",\n",
    "        \"-\": \"white\",\n",
    "        \"#\": \"black\",\n",
    "        \"?\": \"magenta\"\n",
    "    }\n",
    "    return colored(nucleotide, color_map.get(nucleotide.upper(), \"white\"))\n",
    "# Function to print the alignment\n",
    "def print_colored_alignment(alignment):\n",
    "    \"\"\"\n",
    "    Prints a DNA alignment with each nucleotide in a different color.\n",
    "\n",
    "    Parameters:\n",
    "        alignment (dict): A dictionary with sequence names as keys and DNA sequences as values.\n",
    "    \"\"\"\n",
    "    for name, sequence in alignment.items():\n",
    "        colored_seq = ''.join(color_nucleotide(n) for n in sequence)\n",
    "        print(f\"{name}: {colored_seq}\")\n",
    "\n",
    "\n",
    "# FUNCTION 2: Print alignments using trimAl\n",
    "def show_alignment(alignment):\n",
    "    console = rich.console.Console(width=len(alignment.sequences[0])+40)\n",
    "    widget = RichAlignment(names=[n.decode() for n in alignment.names], sequences=alignment.sequences, max_name_width=30)\n",
    "    panel = rich.panel.Panel(widget, title_align=\"left\", title=\"({} residues, {} sequences)\".format(len(alignment.sequences[0]), len(alignment.sequences)))\n",
    "    console.print(panel)\n",
    "\n",
    "# FUNCTION 3: Convert dictionaries to MultipleSeqAlignment\n",
    "def dict_to_multiple_seq_alignment(seq_dict):\n",
    "    \"\"\"\n",
    "    Converts a dictionary of sequences into a MultipleSeqAlignment object.\n",
    "    \n",
    "    Args:\n",
    "        seq_dict (dict): A dictionary where keys are sequence identifiers and values are sequences (strings).\n",
    "        \n",
    "    Returns:\n",
    "        MultipleSeqAlignment: A Biopython MultipleSeqAlignment object.\n",
    "    \"\"\"\n",
    "    # Create a list of SeqRecord objects from the dictionary\n",
    "    seq_records = []\n",
    "    \n",
    "    for seq_id, seq_str in seq_dict.items():\n",
    "        # Create a SeqRecord for each sequence\n",
    "        seq = Seq(seq_str)\n",
    "        seq_record = SeqRecord(seq, id=seq_id, description=\"\")\n",
    "        seq_records.append(seq_record)\n",
    "    \n",
    "    # Create and return the MultipleSeqAlignment object\n",
    "    alignment = MultipleSeqAlignment(seq_records)\n",
    "    return alignment\n",
    "\n",
    "# FUNCTION 4: List lengths of blocks of contiguous gaps in internal and terminal positions\n",
    "def list_gap_blocks_by_type(alignment, plot_distribution=False):\n",
    "    \"\"\"\n",
    "    Identify all blocks of contiguous gaps in the DNA alignment.\n",
    "    Classify gap blocks into terminal and internal blocks.\n",
    "    Optionally, plot the distributions of gap block lengths for terminal and internal blocks.\n",
    "    \n",
    "    Args:\n",
    "        alignment (dict): A dictionary with sequence IDs as keys and sequences as values.\n",
    "        plot_distribution (bool): If True, plot the distributions of terminal and internal gap block lengths.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Two lists:\n",
    "            - terminal_blocks: A list of gap block lengths at the start or end of sequences.\n",
    "            - internal_blocks: A list of gap block lengths in the middle of sequences.\n",
    "    \"\"\"\n",
    "    terminal_blocks = []\n",
    "    internal_blocks = []\n",
    "    \n",
    "    # Iterate over each sequence in the alignment\n",
    "    for seq_id, sequence in alignment.items():\n",
    "        sequence_length = len(sequence)\n",
    "        gap_count = 0\n",
    "        in_gap = False\n",
    "\n",
    "        # Identify gap blocks and separate terminal vs internal\n",
    "        for i, nucleotide in enumerate(sequence):\n",
    "            if nucleotide == '-':  # We are in a gap\n",
    "                if not in_gap:\n",
    "                    in_gap = True\n",
    "                    gap_count = 1  # Start a new gap block\n",
    "                else:\n",
    "                    gap_count += 1  # Continue counting the current gap block\n",
    "            else:  # We encountered a non-gap nucleotide\n",
    "                if in_gap:\n",
    "                    # End of a gap block\n",
    "                    # Check if this gap block is terminal (at start or end of the sequence)\n",
    "                    if i == sequence_length or (i - gap_count == 0):\n",
    "                        terminal_blocks.append(gap_count)\n",
    "                    else:\n",
    "                        internal_blocks.append(gap_count)\n",
    "                    in_gap = False\n",
    "                    gap_count = 0  # Reset gap count for the next block\n",
    "\n",
    "        # If the sequence ends with a gap block, add it to the appropriate list\n",
    "        if in_gap:\n",
    "            if sequence[-1] == '-':  # Last character is a gap\n",
    "                terminal_blocks.append(gap_count)\n",
    "            else:\n",
    "                internal_blocks.append(gap_count)\n",
    "\n",
    "    # Optionally plot the distributions of terminal and internal blocks\n",
    "    if plot_distribution:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        # Plotting terminal blocks\n",
    "        plt.hist(terminal_blocks, bins=20, alpha=0.5, label='Terminal Blocks', color='blue')\n",
    "        # Plotting internal blocks\n",
    "        plt.hist(internal_blocks, bins=20, alpha=0.5, label='Internal Blocks', color='red')\n",
    "        \n",
    "        plt.xlabel('Gap Block Length')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Distribution of Gap Block Lengths')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()\n",
    "\n",
    "    return terminal_blocks, internal_blocks\n",
    "\n",
    "# FUNCTION 5: Remove underscores in the beggining of file names\n",
    "def remove_leading_underscores(file_path):\n",
    "    \"\"\"\n",
    "    Remove leading contiguous underscores from the beginning of the file name.\n",
    "    If a directory is provided, it renames all files in that directory.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The full path of the file or directory.\n",
    "    \n",
    "    Returns:\n",
    "        str or None: The new file path with leading underscores removed if a file, \n",
    "                     or None if a directory (modifies in place).\n",
    "    \"\"\"\n",
    "    if os.path.isdir(file_path):\n",
    "        # If it's a directory, rename all files inside it\n",
    "        for root, dirs, files in os.walk(file_path):\n",
    "            for file in files:\n",
    "                old_file_path = os.path.join(root, file)\n",
    "                new_file_name = file.lstrip('_')\n",
    "                new_file_path = os.path.join(root, new_file_name)\n",
    "                \n",
    "                if old_file_path != new_file_path:\n",
    "                    os.rename(old_file_path, new_file_path)\n",
    "        return None  # No return for directories, as the renaming is in place\n",
    "    else:\n",
    "        # If it's a single file, rename it\n",
    "        dir_name, file_name = os.path.split(file_path)\n",
    "        new_file_name = file_name.lstrip('_')\n",
    "        new_file_path = os.path.join(dir_name, new_file_name)\n",
    "        if file_path != new_file_path:\n",
    "            os.rename(file_path, new_file_path)\n",
    "        return new_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14de974c",
   "metadata": {},
   "source": [
    "## Main functions\n",
    "\n",
    "We define below the main functions of *prepDyn*. To test each of the functions individually, we created a synthetic example. In such example, the step of data collection from GenBank was skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9f6133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "alignment = {\n",
    "    'sp1': \"TGCACCGTCGCCAACAGTAGTCCTCCACCGTCGCCACCGTCNCCAACAG??\",\n",
    "    'sp2': \"T-CACCGTCGCCAACAGTAGTCCTCCACCGTCGCCNNNNNNTCCAACA???\",\n",
    "    'sp3': \"TG-ACCGTCGCCAACAGTAGTCCTCCACCGTCGCCACCGTCGCCAA-----\",\n",
    "    'sp4': \"TGC-CCGTCGCCNNNNNNNNNNNNNNNNNNTCGCNACCGTCGCCAACATTT\",\n",
    "    'sp5': \"TGCA-CGTCGCCAACAGTAGTCCTCCACCGTCGCCACCGTCGCCAACAGTG\",\n",
    "    'sp6': \"TGCAC-GTCGCCAACAGTAGTCCTCCACCGTCGCCACCGTCGCCAACAGTA\",\n",
    "} \n",
    "\n",
    "# Create a temporary file to save the sequences in FASTA format\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix='.fasta', mode='w') as fasta_file:\n",
    "    # Write the sequences to the file\n",
    "    for name, seq in alignment.items():\n",
    "        fasta_file.write(f\">{name}\\n{seq}\\n\")\n",
    "    fasta_file_path = fasta_file.name\n",
    "\n",
    "# Define the output file for the alignment\n",
    "aligned_fasta_file_path = fasta_file_path.replace('.fasta', '_aligned.fasta')\n",
    "# Run MAFFT\n",
    "mafft_command = ['mafft', '--auto', fasta_file_path]\n",
    "with open(aligned_fasta_file_path, 'w') as aligned_file:\n",
    "    subprocess.run(mafft_command, stdout=aligned_file)\n",
    "\n",
    "# Input data\n",
    "alignment = AlignIO.read(aligned_fasta_file_path, \"fasta\")\n",
    "type(alignment) # Check the class of alignment (MultipleSeqAlignment\n",
    "# Convert MultipleSeqAlignment to dictionary\n",
    "alignment = {record.id: str(record.seq) for record in alignment}\n",
    "type(alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "98b172c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0. Align sequences statically\n",
      "sp1: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp2: \u001b[34mT\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp3: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[97m-\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp4: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[34mT\u001b[0m\u001b[34mT\u001b[0m\u001b[34mT\u001b[0m\n",
      "sp5: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp6: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\n",
      "\n",
      "\n",
      "Step 1.1 Delete columns presenting gaps in all taxa (artifacts from MAFFT)\n",
      "sp1: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp2: \u001b[34mT\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp3: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[97m-\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp4: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[34mT\u001b[0m\u001b[34mT\u001b[0m\u001b[34mT\u001b[0m\n",
      "sp5: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp6: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\n",
      "\n",
      "\n",
      "Step 1.2. Delete terminal orphan nucleotides (given a threshold)\n",
      "sp1: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp2: \u001b[34mT\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp3: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[97m-\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp4: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[34mT\u001b[0m\u001b[34mT\u001b[0m\u001b[34mT\u001b[0m\n",
      "sp5: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp6: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\n",
      "\n",
      "\n",
      "Step 2.1 Classify terminal (?) and internal gaps (-)\n",
      "sp1: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp2: \u001b[34mT\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp3: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[97m-\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp4: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[34mT\u001b[0m\u001b[34mT\u001b[0m\u001b[34mT\u001b[0m\n",
      "sp5: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp6: \u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\n",
      "\n",
      "\n",
      "Step 1.3. Trim parsimony non-informative characters in terminal position\n",
      "sp1: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp2: \u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp3: \u001b[32mG\u001b[0m\u001b[97m-\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp4: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[34mT\u001b[0m\u001b[34mT\u001b[0m\u001b[34mT\u001b[0m\n",
      "sp5: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp6: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\n",
      "\n",
      "\n",
      "Step 1.3. Treat internal missing data\n",
      "sp1: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp2: \u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp3: \u001b[32mG\u001b[0m\u001b[97m-\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp4: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[34mT\u001b[0m\u001b[34mT\u001b[0m\u001b[34mT\u001b[0m\n",
      "sp5: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp6: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\n",
      "\n",
      "\n",
      "Step 3.1. Insert pound sign in the n-largest block of contiguous invariants\n",
      "sp1: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp2: \u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp3: \u001b[32mG\u001b[0m\u001b[97m-\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp4: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[34mT\u001b[0m\u001b[34mT\u001b[0m\u001b[34mT\u001b[0m\n",
      "sp5: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp6: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\n",
      "\n",
      "\n",
      "Step 3.2.1 Replace ? flanked by # with -\n",
      "sp1: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp2: \u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp3: \u001b[32mG\u001b[0m\u001b[97m-\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp4: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[34mT\u001b[0m\u001b[34mT\u001b[0m\u001b[34mT\u001b[0m\n",
      "sp5: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp6: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\n",
      "\n",
      "\n",
      "Step 3.2.2 Delete columns of W (artifacts from internal missing data identification by GB2MSA)\n",
      "sp1: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp2: \u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp3: \u001b[32mG\u001b[0m\u001b[97m-\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp4: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[34mT\u001b[0m\u001b[34mT\u001b[0m\u001b[34mT\u001b[0m\n",
      "sp5: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp6: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\n",
      "\n",
      "\n",
      "Step 3.2.3 Replace IUPAC N with question marks)\n",
      "sp1: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp2: \u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[97mN\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp3: \u001b[32mG\u001b[0m\u001b[97m-\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[31mA\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp4: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[35m?\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[34mT\u001b[0m\u001b[34mT\u001b[0m\u001b[34mT\u001b[0m\n",
      "sp5: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[97m-\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[32mG\u001b[0m\n",
      "sp6: \u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[97m-\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[30m#\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[33mC\u001b[0m\u001b[32mG\u001b[0m\u001b[33mC\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[30m#\u001b[0m\u001b[31mA\u001b[0m\u001b[33mC\u001b[0m\u001b[31mA\u001b[0m\u001b[32mG\u001b[0m\u001b[34mT\u001b[0m\u001b[31mA\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Comment the alignment below if you want to use the MAFFT alignment above\n",
    "alignment = {\n",
    "    'sp1': \"TGCACCGTCGCCAACAGTAGTCCTCCACCGTCGCCACCGTCNCCAACAG??\",\n",
    "    'sp2': \"T-CACCGTCGCCAACAGTAGTCCTCCACCGTCGCCNNNNNNTCCAACA???\",\n",
    "    'sp3': \"TG-ACCGTCGCCAACAGTAGTCCTCCACCGTCGCCACCGTCGCCAA-----\",\n",
    "    'sp4': \"TGC-CCGTCGCCNNNNNNNNNNNNNNNNNNTCGCNACCGTCGCCAACATTT\",\n",
    "    'sp5': \"TGCA-CGTCGCCAACAGTAGTCCTCCACCGTCGCCACCGTCGCCAACAGTG\",\n",
    "    'sp6': \"TGCAC-GTCGCCAACAGTAGTCCTCCACCGTCGCCACCGTCGCCAACAGTA\",\n",
    "} \n",
    "\n",
    "# Print MAFFT alignment\n",
    "print(\"Step 0. Align sequences statically\")\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "###########################\n",
    "# Step 1: Data collection #\n",
    "###########################\n",
    "\n",
    "def GB2MSA_1(input_file, output_prefix, delimiter=',', write_names=True):\n",
    "    \"\"\"\n",
    "    Downloads GenBank sequences based on accession numbers in a CSV/TSV file and aligns them by gene using \n",
    "    MAFFT. If two fragments of the same locus are concatenated with no overlap between them, the space\n",
    "    between them will be treaed as missing data (15 Ws will flank these blocks of missing data, which will\n",
    "    be used to track these regions and be replaced with question marks in GB2MSA_2).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_file : str\n",
    "        Path to the CSV or TSV input file. The first column should contain sequence names (sample identifiers).\n",
    "        The first row should contain gene names starting from the second column. Cells contain GenBank \n",
    "        accession numbers (one or more separated by slashes). \"NA\", empty cells, or dashes are ignored.\n",
    "\n",
    "    output_prefix : str\n",
    "        Prefix used for naming intermediate FASTA files and final aligned output files.\n",
    "\n",
    "    delimiter : str, optional (default=',')\n",
    "        Delimiter used in the input file (e.g., ',' for CSV or '\\t' for TSV).\n",
    "\n",
    "    write_names : bool, optional (default=True)\n",
    "        If True, writes a TXT file listing all sequence names (from the first column).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    aligned_files : list of str\n",
    "        List of file paths to the MAFFT-aligned FASTA files for each gene.\n",
    "    \"\"\"\n",
    "    # Open the input CSV/TSV file\n",
    "    with open(input_file, newline='') as file:\n",
    "        reader = csv.reader(file, delimiter=delimiter)\n",
    "        rows = list(reader)\n",
    "\n",
    "    # Replace spaces in sequence names with underscores\n",
    "    sequence_names = [row[0].replace(\" \", \"_\") for row in rows[1:]]\n",
    "    \n",
    "    # Extract gene names from the header row (excluding first column)\n",
    "    gene_names = rows[0][1:]\n",
    "    \n",
    "    # Extract gene accession data for each sequence (excluding first column)\n",
    "    gene_columns = [row[1:] for row in rows[1:]]\n",
    "\n",
    "    # If requested, write the sequence names to a text file\n",
    "    if write_names:\n",
    "        names_file = f\"{output_prefix}_sequence_names.txt\"\n",
    "        with open(names_file, 'w') as nf:\n",
    "            for name in sequence_names:\n",
    "                nf.write(f\"{name}\\n\")\n",
    "\n",
    "    aligned_files = []  # List to store paths of aligned output files\n",
    "\n",
    "    # Iterate over each gene (i.e., each column after the first)\n",
    "    for gene_idx, gene_name in enumerate(gene_names):\n",
    "        fasta_file = f\"{output_prefix}_{gene_name}.fasta\"  # Name of temporary FASTA file\n",
    "        \n",
    "        with open(fasta_file, 'w') as fasta_out:\n",
    "            # Iterate through each row (sample/sequence)\n",
    "            for i, seq_name in enumerate(sequence_names):\n",
    "                cell = gene_columns[i][gene_idx].strip()\n",
    "                \n",
    "                # Skip cells with missing data (\"NA\", empty, or dash)\n",
    "                if cell.upper() == \"NA\" or not cell or cell == \"-\":\n",
    "                    continue\n",
    "\n",
    "                # Split accession numbers by '/' and filter out invalid entries\n",
    "                accessions = [acc for acc in cell.split('/') if acc.upper() != \"NA\" and acc != \"\" and acc != \"-\"]\n",
    "                sequences = []  # To hold the sequences retrieved from GenBank\n",
    "\n",
    "                # Fetch each sequence from GenBank\n",
    "                for acc in accessions:\n",
    "                    try:\n",
    "                        handle = Entrez.efetch(db=\"nucleotide\", id=acc, rettype=\"fasta\", retmode=\"text\")\n",
    "                        seq_record = SeqIO.read(handle, \"fasta\")\n",
    "                        handle.close()\n",
    "                        sequences.append(str(seq_record.seq))  # Store the sequence string\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error fetching {acc}: {e}\")\n",
    "\n",
    "                # Combine multiple sequences with 'W' delimiters to mark junctions (to be handled later)\n",
    "                combined_seq = \"WWWWWWWWWWWWWWW\".join(sequences)\n",
    "                \n",
    "                # Write the sequence to the FASTA file with its name as header\n",
    "                fasta_out.write(f\">{seq_name}\\n{combined_seq}\\n\")\n",
    "\n",
    "        # Define the name for the output alignment file\n",
    "        aligned_file = f\"{output_prefix}_{gene_name}_aligned.fasta\"\n",
    "\n",
    "        # Run MAFFT on the generated FASTA file and save the alignment\n",
    "        with open(aligned_file, 'w') as aligned_out:\n",
    "            subprocess.run([\"mafft\", \"--auto\", fasta_file], stdout=aligned_out)\n",
    "\n",
    "        # Append the aligned file path to the result list\n",
    "        aligned_files.append(aligned_file)\n",
    "\n",
    "    return aligned_files  # Return list of aligned output file paths\n",
    "\n",
    "def GB2MSA_2(alignment_file):\n",
    "    \"\"\"\n",
    "    If internal missing data were identified by GB2MSA_1, 15 Ws flank the blocks of missing data. \n",
    "    For each sequence in the alignment, GB2MSA_2:\n",
    "    - Replaces internal blocks of 15 'w' or spaced 'w' (e.g., w-w-w) with question marks.\n",
    "    - Removes columns with only '?' or '-' in all rows.\n",
    "    - Replaces dash blocks flanked by a nucleotide and a question mark (e.g., A??--AAC) with \n",
    "    question marks. These dashes can be artifacts from spaced 'w' and are missing data.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    alignment_file : str\n",
    "        Path to the MAFFT-aligned FASTA file to be processed.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Path to the cleaned alignment file.\n",
    "    \"\"\"\n",
    "    alignment = list(SeqIO.parse(alignment_file, \"fasta\"))\n",
    "    updated_records = []\n",
    "\n",
    "    # Step 1: Replace exact block of 15 'w' or 'W' with 15 '?'\n",
    "    for record in alignment:\n",
    "        seq = str(record.seq)\n",
    "        seq_cleaned = seq.replace(\"w\" * 15, \"?\" * 15).replace(\"W\" * 15, \"?\" * 15)\n",
    "        record.seq = Seq(seq_cleaned)\n",
    "        updated_records.append(record)\n",
    "\n",
    "    # Step 2: Replace non-contiguous 'w' blocks (e.g., w-w-w) with '?'\n",
    "    for record in updated_records:\n",
    "        chars = list(str(record.seq))\n",
    "        i = 0\n",
    "        while i < len(chars):\n",
    "            if chars[i].lower() == 'w':\n",
    "                count = 1\n",
    "                indices = [i]\n",
    "                j = i + 1\n",
    "                while j < len(chars) and count < 15:\n",
    "                    if chars[j] == '-':\n",
    "                        indices.append(j)\n",
    "                    elif chars[j].lower() == 'w':\n",
    "                        indices.append(j)\n",
    "                        count += 1\n",
    "                    else:\n",
    "                        break\n",
    "                    j += 1\n",
    "                if count == 15:\n",
    "                    for idx in indices:\n",
    "                        chars[idx] = '?'\n",
    "                i = j\n",
    "            else:\n",
    "                i += 1\n",
    "        record.seq = Seq(''.join(chars))\n",
    "\n",
    "    # Step 3: Remove columns with only '?' or '-' in all rows\n",
    "    sequences = [list(str(record.seq)) for record in updated_records]\n",
    "    if len(set(len(seq) for seq in sequences)) > 1:\n",
    "        raise ValueError(\"Sequences are not of the same length!\")\n",
    "\n",
    "    valid_columns = []\n",
    "    for i in range(len(sequences[0])):\n",
    "        column = [seq[i] for seq in sequences]\n",
    "        if any(base not in ['?', '-'] for base in column):\n",
    "            valid_columns.append(i)\n",
    "\n",
    "    # Rebuild records with cleaned sequences\n",
    "    cleaned_records = []\n",
    "    for record in updated_records:\n",
    "        cleaned_seq = ''.join(str(record.seq)[i] for i in valid_columns)\n",
    "        record.seq = Seq(cleaned_seq)\n",
    "        cleaned_records.append(record)\n",
    "\n",
    "    # Step 4: Replace dash blocks flanked by nucleotide and '?' with '?'\n",
    "    for record in cleaned_records:\n",
    "        chars = list(str(record.seq))\n",
    "        i = 0\n",
    "        while i < len(chars):\n",
    "            if chars[i] == '-':\n",
    "                start = i\n",
    "                while i < len(chars) and chars[i] == '-':\n",
    "                    i += 1\n",
    "                end = i - 1\n",
    "\n",
    "                # Check flanking characters safely\n",
    "                left = chars[start - 1] if start > 0 else ''\n",
    "                right = chars[end + 1] if end + 1 < len(chars) else ''\n",
    "\n",
    "                if ((left in 'ACGTacgt' and right in '?Nn') or \n",
    "                    (left in '?Nn' and right in 'ACGTacgt')):\n",
    "                    for j in range(start, end + 1):\n",
    "                        chars[j] = '?'\n",
    "            else:\n",
    "                i += 1\n",
    "        record.seq = Seq(''.join(chars))\n",
    "\n",
    "    # Step 5: Write to cleaned output\n",
    "    cleaned_file = alignment_file.replace(\".fasta\", \"_GB2MSA.fasta\")\n",
    "    with open(cleaned_file, \"w\") as out_handle:\n",
    "        SeqIO.write(cleaned_records, out_handle, \"fasta\")\n",
    "\n",
    "    return cleaned_file\n",
    "\n",
    "def GB2MSA_3(alignment_dict, orphan_threshold=6, log=False):\n",
    "    \"\"\"\n",
    "    Replaces specific blocks in a DNA alignment with '?':\n",
    "    - Contiguous gap blocks (length >= orphan_threshold) that are adjacent\n",
    "      to contiguous nucleotide blocks (length < orphan_threshold), where the\n",
    "      other side of that nucleotide block touches a '?'\n",
    "\n",
    "    Parameters:\n",
    "    alignment_dict (dict): {sequence_name: aligned_sequence}\n",
    "    orphan_threshold (int): Minimum size to define a valid gap block\n",
    "    log (bool): If True, print the start and end positions of replaced blocks\n",
    "\n",
    "    Returns:\n",
    "    dict: Cleaned alignment with selective replacements\n",
    "    \"\"\"\n",
    "    cleaned_alignment = {}\n",
    "\n",
    "    for seq_name, original_seq in alignment_dict.items():\n",
    "        sequence = list(original_seq)\n",
    "        seq_len = len(sequence)\n",
    "        updated_seq = ''.join(sequence)\n",
    "\n",
    "        gap_matches = list(re.finditer(r'-+', updated_seq))\n",
    "        valid_gap_blocks = [(m.start(), m.end() - 1) for m in gap_matches\n",
    "                            if (m.end() - m.start()) >= orphan_threshold]\n",
    "\n",
    "        to_replace = set()\n",
    "\n",
    "        if log:\n",
    "            print(f\"Sequence: {seq_name}\")\n",
    "\n",
    "        for gap_start, gap_end in valid_gap_blocks:\n",
    "            replaced = False\n",
    "\n",
    "            # Check left nucleotide orphan\n",
    "            left_end = gap_start - 1\n",
    "            i = left_end\n",
    "            while i >= 0 and updated_seq[i] not in '-?':\n",
    "                i -= 1\n",
    "            left_start = i + 1\n",
    "            left_len = left_end - left_start + 1\n",
    "\n",
    "            if left_len > 0 and left_len < orphan_threshold and i >= 0 and updated_seq[i] == '?':\n",
    "                to_replace.update(range(left_start, left_end + 1))\n",
    "                to_replace.update(range(gap_start, gap_end + 1))\n",
    "                replaced = True\n",
    "                if log:\n",
    "                    print(f\"  Gap block: {gap_start}-{gap_end}\")\n",
    "                    print(f\"  Left orphan nucleotide block: {left_start}-{left_end}\")\n",
    "\n",
    "            if not replaced:\n",
    "                # Check right nucleotide orphan\n",
    "                right_start = gap_end + 1\n",
    "                i = right_start\n",
    "                while i < seq_len and updated_seq[i] not in '-?':\n",
    "                    i += 1\n",
    "                right_end = i - 1\n",
    "                right_len = right_end - right_start + 1\n",
    "\n",
    "                if right_len > 0 and right_len < orphan_threshold and i < seq_len and updated_seq[i] == '?':\n",
    "                    to_replace.update(range(right_start, right_end + 1))\n",
    "                    to_replace.update(range(gap_start, gap_end + 1))\n",
    "                    if log:\n",
    "                        print(f\"  Gap block: {gap_start}-{gap_end}\")\n",
    "                        print(f\"  Right orphan nucleotide block: {right_start}-{right_end}\")\n",
    "\n",
    "        # Replace in sequence\n",
    "        for i in to_replace:\n",
    "            sequence[i] = '?'\n",
    "\n",
    "        cleaned_alignment[seq_name] = ''.join(sequence)\n",
    "\n",
    "    return cleaned_alignment\n",
    "\n",
    "def GB2MSA_4(alignment_dict):\n",
    "    cleaned_alignment = {}\n",
    "\n",
    "    for seq_name, seq in alignment_dict.items():\n",
    "        sequence = list(seq)\n",
    "\n",
    "        # Use regex to find blocks with at least 15 characters of w/W/- combined\n",
    "        # but must contain at least 15 w or W\n",
    "        pattern = re.finditer(r'([wW\\-]{15,})', ''.join(sequence))\n",
    "\n",
    "        for match in pattern:\n",
    "            block = match.group()\n",
    "            start, end = match.start(), match.end()\n",
    "\n",
    "            # Count number of w/W in the block\n",
    "            w_count = sum(1 for c in block if c in 'wW')\n",
    "            if w_count >= 15:\n",
    "                for i in range(start, end):\n",
    "                    sequence[i] = '?'\n",
    "\n",
    "        cleaned_alignment[seq_name] = ''.join(sequence)\n",
    "\n",
    "    return cleaned_alignment\n",
    "\n",
    "def GB2MSA(input_file, \n",
    "           output_prefix, \n",
    "           delimiter=',', \n",
    "           write_names=True, \n",
    "           log=False, \n",
    "           orphan_threshold=6):\n",
    "    \"\"\"\n",
    "    Complete GenBank-to-MSA pipeline:\n",
    "    1. Downloads sequences from GenBank and aligns them by gene using MAFFT.\n",
    "    2. Cleans the alignments by replacing internal missing data and removing empty columns.\n",
    "    3. Applies GB2MSA_3 to replace selected gap and orphan nucleotide blocks with '?'.\n",
    "    4. Applies GB2MSA_4 to replace blocks of 15 or more w/W (with or without interspersed gaps) with '?'.\n",
    "    5. Replaces terminal '?' in sequences with '-'.\n",
    "    6. Deletes intermediate files ending with '_aligned.fasta'.\n",
    "    7. Optionally logs wall clock and CPU time to a log file named '<output_prefix>_log.txt'.\n",
    "    \"\"\"\n",
    "    start_wall = time.time()\n",
    "    start_cpu = time.process_time()\n",
    "\n",
    "    # Step 1: Generate aligned FASTA files\n",
    "    aligned_files = GB2MSA_1(input_file, output_prefix, delimiter=delimiter, write_names=write_names)\n",
    "\n",
    "    # Step 2: Clean each aligned FASTA file\n",
    "    cleaned_files = []\n",
    "    for aligned_file in aligned_files:\n",
    "        cleaned_file = GB2MSA_2(aligned_file)\n",
    "        cleaned_files.append(cleaned_file)\n",
    "\n",
    "    # Step 3: Apply GB2MSA_3 to clean orphan gap/nucleotide blocks\n",
    "    for cleaned_file in cleaned_files:\n",
    "        records = list(SeqIO.parse(cleaned_file, \"fasta\"))\n",
    "        alignment_dict = {record.id: str(record.seq) for record in records}\n",
    "        updated_dict = GB2MSA_3(alignment_dict, orphan_threshold=orphan_threshold, log=log)\n",
    "        \n",
    "        # Step 4: Apply GB2MSA_4 to handle w/W blocks\n",
    "        updated_dict = GB2MSA_4(updated_dict)\n",
    "\n",
    "        updated_records = []\n",
    "        for record in records:\n",
    "            record.seq = Seq(updated_dict[record.id])\n",
    "            updated_records.append(record)\n",
    "        with open(cleaned_file, \"w\") as out_handle:\n",
    "            SeqIO.write(updated_records, out_handle, \"fasta\")\n",
    "\n",
    "    # Step 5: Replace terminal '?' with '-' in each sequence\n",
    "    for cleaned_file in cleaned_files:\n",
    "        records = list(SeqIO.parse(cleaned_file, \"fasta\"))\n",
    "        updated_records = []\n",
    "        for record in records:\n",
    "            seq = str(record.seq)\n",
    "            left = len(seq) - len(seq.lstrip('?'))\n",
    "            right = len(seq) - len(seq.rstrip('?'))\n",
    "            new_seq = '-' * left + seq[left:len(seq)-right] + '-' * right if right > 0 else '-' * left + seq[left:]\n",
    "            record.seq = Seq(new_seq)\n",
    "            updated_records.append(record)\n",
    "        with open(cleaned_file, \"w\") as out_handle:\n",
    "            SeqIO.write(updated_records, out_handle, \"fasta\")\n",
    "\n",
    "    # Step 6: Delete intermediate *_aligned.fasta files\n",
    "    for aligned_file in aligned_files:\n",
    "        if aligned_file.endswith(\"_aligned.fasta\") and os.path.exists(aligned_file):\n",
    "            os.remove(aligned_file)\n",
    "\n",
    "    # Step 7: Log timing if requested\n",
    "    if log:\n",
    "        end_wall = time.time()\n",
    "        end_cpu = time.process_time()\n",
    "        wall_time = end_wall - start_wall\n",
    "        cpu_time = end_cpu - start_cpu\n",
    "\n",
    "        log_file = f\"{output_prefix}_log.txt\"\n",
    "        with open(log_file, \"a\") as lf:\n",
    "            lf.write(f\"--- GB2MSA run for '{output_prefix}' ---\\n\")\n",
    "            lf.write(f\"Wall clock time: {wall_time:.2f} seconds\\n\")\n",
    "            lf.write(f\"CPU time: {cpu_time:.2f} seconds\\n\\n\")\n",
    "\n",
    "    return cleaned_files\n",
    "\n",
    "def addSeq(\n",
    "    alignment,\n",
    "    new_seqs,\n",
    "    output,\n",
    "    write_names=True,\n",
    "    orphan_threshold=0,\n",
    "    log=False,\n",
    "    n2question=None,\n",
    "    gaps2question=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Add new sequences to an existing alignment using MAFFT, clean and standardize the result.\n",
    "\n",
    "    Steps performed:\n",
    "    1. Remove '#' columns from original alignment.\n",
    "    2. Align new sequences with MAFFT using --add.\n",
    "    3. Trim orphan nucleotide blocks from new sequences.\n",
    "    4. Remove short DNA blocks near # in first and last partitions.\n",
    "    5. Replace terminal '-' with '?'.\n",
    "    6. Optionally replace all N/n with '?' in selected sequences.\n",
    "    7. Optionally replace long gap blocks with '?'.\n",
    "    8. Reinsert '#' columns.\n",
    "    9. Replace all-'?' blocks between '#' with '-'.\n",
    "    10. Write the result and an optional log file.\n",
    "\n",
    "    Parameters:\n",
    "        alignment (str or dict): Existing alignment file path (FASTA) or dictionary {id: sequence}.\n",
    "        new_seqs (str or dict): New sequences to add (FASTA path or dict {id: sequence}).\n",
    "        output (str): Path to write the updated alignment in FASTA format.\n",
    "        write_names (bool): Whether to write a _terminal_names.txt file listing sequence IDs.\n",
    "        orphan_threshold (int): Threshold to detect and remove orphan DNA blocks.\n",
    "        log (bool): If True, writes a log file with trimming and runtime information.\n",
    "        n2question (str, list or None): Replace 'N/n' with '?' in specific sequences:\n",
    "            - 'all': apply to all sequences\n",
    "            - str: apply to a single sequence ID\n",
    "            - list: apply to listed sequence IDs\n",
    "        gaps2question (int or None): Replace contiguous gap blocks larger than this threshold with '?'. Only applied to added sequences.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Example usage:\n",
    "        addSeq(\"alignment.fasta\", \"new.fasta\", \"updated.fasta\", n2question=\"seq123\", log=True)\n",
    "        addSeq(alignment_dict, new_dict, \"out.fas\", n2question='all')\n",
    "    \"\"\"\n",
    "\n",
    "    # Start timing the execution\n",
    "    start_time = time.time()\n",
    "    temp_files_to_remove = []  # Temporary files to be removed after execution\n",
    "    log_lines = [] \n",
    "\n",
    "    # Log the function call and parameters for reproducibility\n",
    "    if log:\n",
    "        cmd_used = f\"addSeq(alignment=..., new_seqs=..., output='{output}', write_names={write_names}, orphan_threshold={orphan_threshold}, log={log}, n2question={n2question}, gaps2question={gaps2question})\"\n",
    "        log_lines.append(f\"Command used: {cmd_used}\")\n",
    "        log_lines.append(\"\")\n",
    "\n",
    "    # === Step 1: Load and clean the alignment ===\n",
    "    def write_dict_to_temp_fasta(seq_dict):\n",
    "        records = [SeqRecord(Seq(seq), id=str(seq_id), description=\"\") for seq_id, seq in seq_dict.items()]\n",
    "        tmp = tempfile.NamedTemporaryFile(\"w+\", delete=False)\n",
    "        SeqIO.write(records, tmp, \"fasta\")\n",
    "        tmp.close()\n",
    "        return tmp.name\n",
    "\n",
    "    if isinstance(alignment, dict):\n",
    "        alignment_path = write_dict_to_temp_fasta(alignment)\n",
    "        temp_files_to_remove.append(alignment_path)\n",
    "    elif isinstance(alignment, str):\n",
    "        alignment_path = alignment\n",
    "    else:\n",
    "        raise ValueError(\"alignment must be a FASTA file path or a dictionary\")\n",
    "\n",
    "    records = list(SeqIO.parse(alignment_path, \"fasta\"))\n",
    "    if not records:\n",
    "        raise ValueError(\"Input alignment is empty or not found\")\n",
    "\n",
    "    aln_len = len(records[0].seq)\n",
    "    # Identify columns that are '#' characters to temporarily remove them for alignment\n",
    "    pound_cols = [i for i in range(aln_len) if any(rec.seq[i] == '#' for rec in records)]\n",
    "\n",
    "    # Log input alignment info\n",
    "    if log:\n",
    "        log_lines.append(f\"Input alignment: {len(records)} sequences\")\n",
    "        log_lines.append(f\"Input alignment: {len(pound_cols)} # columns\")\n",
    "\n",
    "    def remove_cols(seq, cols):\n",
    "        return ''.join(seq[i] for i in range(len(seq)) if i not in cols)\n",
    "\n",
    "    # Remove '#' columns\n",
    "    aln_no_pound = [\n",
    "        SeqRecord(Seq(remove_cols(str(rec.seq), pound_cols)), id=rec.id, description=\"\")\n",
    "        for rec in records\n",
    "    ]\n",
    "    # Write cleaned alignment to a temporary file\n",
    "    with tempfile.NamedTemporaryFile(\"w+\", delete=False) as aln_tmp:\n",
    "        SeqIO.write(aln_no_pound, aln_tmp, \"fasta\")\n",
    "        aln_path = aln_tmp.name\n",
    "        temp_files_to_remove.append(aln_path)\n",
    "\n",
    "    # === Step 2: Load new sequences ===\n",
    "    if isinstance(new_seqs, dict):\n",
    "        new_seqs_path = write_dict_to_temp_fasta(new_seqs)\n",
    "        new_seq_count = len(new_seqs)\n",
    "        temp_files_to_remove.append(new_seqs_path)\n",
    "    elif isinstance(new_seqs, str):\n",
    "        new_seq_count = sum(1 for _ in SeqIO.parse(new_seqs, \"fasta\"))\n",
    "        new_seqs_path = new_seqs\n",
    "    else:\n",
    "        raise ValueError(\"new_seqs must be a FASTA file path or a dictionary\")\n",
    "    # Log new sequence info\n",
    "    if log:\n",
    "        log_lines.append(f\"Input new sequences: {new_seq_count} sequences\")\n",
    "\n",
    "    # === Step 3: Align new sequences with MAFFT ===\n",
    "    with tempfile.NamedTemporaryFile(\"w+\", delete=False) as out_tmp:\n",
    "        out_path = out_tmp.name\n",
    "        temp_files_to_remove.append(out_path)\n",
    "\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            ['mafft', '--add', new_seqs_path, '--keeplength', '--preservecase', aln_path],\n",
    "            check=True,\n",
    "            stdout=open(out_path, 'w'),\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "        )\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        for file in temp_files_to_remove:\n",
    "            try:\n",
    "                os.remove(file)\n",
    "            except Exception:\n",
    "                pass\n",
    "        raise RuntimeError(f\"MAFFT failed!\\nCommand: {e.cmd}\\nExit status: {e.returncode}\\nMAFFT error output:\\n{e.stderr}\")\n",
    "\n",
    "    mafft_aligned_records = list(SeqIO.parse(out_path, \"fasta\"))\n",
    "    original_ids = {rec.id for rec in records}\n",
    "    new_records = [rec for rec in mafft_aligned_records if rec.id not in original_ids]\n",
    "\n",
    "\n",
    "    def replace_gap_blocks(seq, threshold, seq_id=None):\n",
    "        seq_list = list(seq)\n",
    "        replaced_log = []\n",
    "        i = 0\n",
    "        while i < len(seq_list):\n",
    "            if seq_list[i] == '-':\n",
    "                start = i\n",
    "                while i < len(seq_list) and seq_list[i] == '-':\n",
    "                    i += 1\n",
    "                if (i - start) > threshold:\n",
    "                    for j in range(start, i):\n",
    "                        seq_list[j] = '?'\n",
    "                    if seq_id:\n",
    "                        replaced_log.append(f\"{seq_id}: {i - start} contiguous '-' replaced with '?' at {start}â€“{i}\")\n",
    "            else:\n",
    "                i += 1\n",
    "        return ''.join(seq_list), replaced_log\n",
    "\n",
    "    def find_dna_blocks(seq_list, start, end):\n",
    "        blocks = []\n",
    "        i = start\n",
    "        while i < end:\n",
    "            if seq_list[i] not in \"-?#\":\n",
    "                s = i\n",
    "                while i < end and seq_list[i] not in \"-?#\":\n",
    "                    i += 1\n",
    "                e = i\n",
    "                blocks.append((s, e))\n",
    "            else:\n",
    "                i += 1\n",
    "        return blocks\n",
    "\n",
    "    def trim_orphan_blocks(seq, threshold, seq_id=None):\n",
    "        seq_list = list(seq)\n",
    "        trimmed_log = []\n",
    "\n",
    "        def find_blocks(seq_list):\n",
    "            blocks = []\n",
    "            i = 0\n",
    "            while i < len(seq_list):\n",
    "                if seq_list[i] not in \"-?#\":\n",
    "                    start = i\n",
    "                    while i < len(seq_list) and seq_list[i] not in \"-?#\":\n",
    "                        i += 1\n",
    "                    end = i\n",
    "                    blocks.append((start, end))\n",
    "                else:\n",
    "                    i += 1\n",
    "            return blocks\n",
    "\n",
    "        changed = True\n",
    "        while changed:\n",
    "            changed = False\n",
    "\n",
    "            while True:\n",
    "                blocks = find_blocks(seq_list)\n",
    "                if len(blocks) < 2:\n",
    "                    break\n",
    "                first_start, first_end = blocks[0]\n",
    "                next_start = blocks[1][0]\n",
    "                gap_count = seq_list[first_end:next_start].count('-') + seq_list[first_end:next_start].count('?')\n",
    "                size = first_end - first_start\n",
    "                if size < threshold and gap_count > threshold:\n",
    "                    deleted = ''.join(seq_list[first_start:first_end])\n",
    "                    seq_list[first_start:first_end] = ['-'] * size\n",
    "                    if seq_id:\n",
    "                        trimmed_log.append(f\"{seq_id}: Left {first_start}â€“{first_end} (size={size}, '{deleted}')\")\n",
    "                    changed = True\n",
    "                    continue\n",
    "                break\n",
    "\n",
    "            while True:\n",
    "                blocks = find_blocks(seq_list)\n",
    "                if len(blocks) < 2:\n",
    "                    break\n",
    "                last_start, last_end = blocks[-1]\n",
    "                prev_end = blocks[-2][1]\n",
    "                gap_count = seq_list[prev_end:last_start].count('-') + seq_list[prev_end:last_start].count('?')\n",
    "                size = last_end - last_start\n",
    "                if size < threshold and gap_count > threshold:\n",
    "                    deleted = ''.join(seq_list[last_start:last_end])\n",
    "                    seq_list[last_start:last_end] = ['-'] * size\n",
    "                    if seq_id:\n",
    "                        trimmed_log.append(f\"{seq_id}: Right {last_start}â€“{last_end} (size={size}, '{deleted}')\")\n",
    "                    changed = True\n",
    "                    continue\n",
    "                break\n",
    "\n",
    "        if pound_cols:\n",
    "            first_hash = min(pound_cols)\n",
    "            last_hash = max(pound_cols)\n",
    "            blocks = find_dna_blocks(seq_list, 0, first_hash)\n",
    "            if len(blocks) == 1:\n",
    "                s, e = blocks[0]\n",
    "                if e == first_hash and (e - s) < threshold:\n",
    "                    deleted = ''.join(seq_list[s:e])\n",
    "                    seq_list[s:e] = ['-'] * (e - s)\n",
    "                    if seq_id:\n",
    "                        trimmed_log.append(f\"{seq_id}: Left {s}â€“{e} (size={e - s}, '{deleted}')\")\n",
    "            blocks = find_dna_blocks(seq_list, last_hash + 1, len(seq_list))\n",
    "            if len(blocks) == 1:\n",
    "                s, e = blocks[0]\n",
    "                if s == last_hash + 1 and (e - s) < threshold:\n",
    "                    deleted = ''.join(seq_list[s:e])\n",
    "                    seq_list[s:e] = ['-'] * (e - s)\n",
    "                    if seq_id:\n",
    "                        trimmed_log.append(f\"{seq_id}: Right {s}â€“{e} (size={e - s}, '{deleted}')\")\n",
    "\n",
    "        return ''.join(seq_list), trimmed_log\n",
    "\n",
    "    trimmed_new_records = []\n",
    "    all_trim_logs = []\n",
    "    gaps2q_log = []  # This will no longer be used for logging replaced gaps\n",
    "\n",
    "    for rec in new_records:\n",
    "        trimmed_seq, seq_log = trim_orphan_blocks(str(rec.seq), orphan_threshold, seq_id=rec.id)\n",
    "        # Remove gaps2question here to not log replaced gaps multiple times\n",
    "        trimmed_new_records.append(SeqRecord(Seq(trimmed_seq), id=rec.id, description=\"\"))\n",
    "        all_trim_logs.extend(seq_log)\n",
    "\n",
    "    processed_records = [rec for rec in mafft_aligned_records if rec.id in original_ids] + trimmed_new_records\n",
    "\n",
    "    updated_records = []\n",
    "    for rec in processed_records:\n",
    "        seq_chars = list(str(rec.seq))\n",
    "        for i in range(len(seq_chars)):\n",
    "            if seq_chars[i] == '-':\n",
    "                seq_chars[i] = '?'\n",
    "            else:\n",
    "                break\n",
    "        for i in range(len(seq_chars) - 1, -1, -1):\n",
    "            if seq_chars[i] == '-':\n",
    "                seq_chars[i] = '?'\n",
    "            else:\n",
    "                break\n",
    "        updated_records.append(SeqRecord(Seq(''.join(seq_chars)), id=rec.id, description=\"\"))\n",
    "\n",
    "    n2q_log = []\n",
    "    if n2question:\n",
    "        if isinstance(n2question, str) and n2question != 'all':\n",
    "            target_ids = {n2question}\n",
    "        elif isinstance(n2question, list):\n",
    "            target_ids = set(n2question)\n",
    "        elif n2question == 'all':\n",
    "            target_ids = {rec.id for rec in updated_records}\n",
    "        else:\n",
    "            target_ids = set()\n",
    "\n",
    "        for rec in updated_records:\n",
    "            if rec.id in target_ids:\n",
    "                seq_str = str(rec.seq)\n",
    "                count_n = seq_str.count('N') + seq_str.count('n')\n",
    "                if count_n > 0:\n",
    "                    rec.seq = Seq(seq_str.replace('N', '?').replace('n', '?'))\n",
    "                    n2q_log.append(f\"{rec.id}: {count_n} N/n replaced with ?\")\n",
    "\n",
    "    # === Now apply gaps2question as the very last step on added sequences ===\n",
    "    def replace_gap_blocks(seq, threshold, seq_id=None):\n",
    "        seq_list = list(seq)\n",
    "        replaced_log = []\n",
    "        i = 0\n",
    "        while i < len(seq_list):\n",
    "            if seq_list[i] == '-':\n",
    "                start = i\n",
    "                while i < len(seq_list) and seq_list[i] == '-':\n",
    "                    i += 1\n",
    "                if (i - start) > threshold:\n",
    "                    for j in range(start, i):\n",
    "                        seq_list[j] = '?'\n",
    "                    if seq_id:\n",
    "                        replaced_log.append(f\"{seq_id}: {i - start} contiguous '-' replaced with '?' at {start}â€“{i}\")\n",
    "            else:\n",
    "                i += 1\n",
    "        return ''.join(seq_list), replaced_log\n",
    "\n",
    "    # Apply gaps2question only if specified\n",
    "    if gaps2question:\n",
    "        updated_records_dict = {rec.id: rec for rec in updated_records}\n",
    "        for rec in trimmed_new_records:\n",
    "            seq = str(updated_records_dict[rec.id].seq)\n",
    "            new_seq, _ = replace_gap_blocks(seq, gaps2question, seq_id=rec.id)\n",
    "            updated_records_dict[rec.id].seq = Seq(new_seq)\n",
    "        updated_records = list(updated_records_dict.values())\n",
    "\n",
    "    final_records = []\n",
    "    for rec in updated_records:\n",
    "        seq_list = list(str(rec.seq))\n",
    "        for col in sorted(pound_cols):\n",
    "            seq_list.insert(col, '#')\n",
    "        final_records.append(SeqRecord(Seq(''.join(seq_list)), id=rec.id, description=\"\"))\n",
    "\n",
    "    def process_blocks(seq):\n",
    "        seq_chars = list(seq)\n",
    "        blocks = []\n",
    "        start = 0\n",
    "        for i, c in enumerate(seq_chars):\n",
    "            if c == '#':\n",
    "                blocks.append((start, i))\n",
    "                start = i + 1\n",
    "        blocks.append((start, len(seq_chars)))\n",
    "        for (start, end) in blocks:\n",
    "            if all(seq_chars[i] == '?' for i in range(start, end)):\n",
    "                for i in range(start, end):\n",
    "                    seq_chars[i] = '-'\n",
    "        return ''.join(seq_chars)\n",
    "\n",
    "    final_output = [\n",
    "        SeqRecord(Seq(process_blocks(str(rec.seq))), id=rec.id, description=\"\")\n",
    "        for rec in final_records\n",
    "    ]\n",
    "\n",
    "    # --- New function to find contiguous '?' blocks ---\n",
    "    def find_question_blocks(seq):\n",
    "        blocks = []\n",
    "        seq_len = len(seq)\n",
    "        i = 0\n",
    "        while i < seq_len:\n",
    "            if seq[i] == '?':\n",
    "                start = i\n",
    "                while i < seq_len and seq[i] == '?':\n",
    "                    i += 1\n",
    "                end = i\n",
    "                blocks.append((start, end))\n",
    "            else:\n",
    "                i += 1\n",
    "        return blocks\n",
    "\n",
    "    # Identify gap ('?') blocks in added sequences for final logging\n",
    "    gap_question_blocks_log = []\n",
    "    added_ids = {rec.id for rec in trimmed_new_records}  # IDs of added sequences\n",
    "\n",
    "    for rec in final_output:\n",
    "        if rec.id in added_ids:\n",
    "            q_blocks = find_question_blocks(str(rec.seq))\n",
    "            for (start, end) in q_blocks:\n",
    "                length = end - start\n",
    "                gap_question_blocks_log.append(f\"{rec.id}: ? block at positions {start}-{end} (length={length})\")\n",
    "\n",
    "    SeqIO.write(final_output, output, \"fasta\")\n",
    "    if write_names:\n",
    "        with open(output + \"_terminal_names.txt\", \"w\") as f:\n",
    "            for rec in final_output:\n",
    "                f.write(rec.id + \"\\n\")\n",
    "\n",
    "    if log:\n",
    "        elapsed = time.time() - start_time\n",
    "        log_lines.append(f\"Final alignment: {len(final_output)} sequences\")\n",
    "        log_lines.append(f\"Final alignment: {len(final_output[0].seq)} columns\")\n",
    "        log_lines.append(f\"Final alignment: {sum(1 for i in range(len(final_output[0].seq)) if any(rec.seq[i] == '#' for rec in final_output))} # columns\")\n",
    "        log_lines.append(\"\")\n",
    "        log_lines.append(\"Trimmed orphan blocks from new sequences:\")\n",
    "        log_lines.extend(all_trim_logs or [\"None\"])\n",
    "        if gap_question_blocks_log:\n",
    "            log_lines.append(\"\")\n",
    "            log_lines.append(\"Gap block replacements:\")\n",
    "            log_lines.extend(gap_question_blocks_log)\n",
    "        if n2q_log:\n",
    "            log_lines.append(\"\")\n",
    "            log_lines.append(\"N/n to ? replacements:\")\n",
    "            log_lines.extend(n2q_log)\n",
    "        log_lines.append(\"\")\n",
    "        log_lines.append(f\"Runtime: {elapsed:.2f} seconds\")\n",
    "        with open(output + \".log\", \"w\") as log_file:\n",
    "            log_file.write(\"\\n\".join(log_lines))\n",
    "\n",
    "    for file in temp_files_to_remove:\n",
    "        try:\n",
    "            os.remove(file)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Step 1.1: Delete gap artifacts from Mafft #\n",
    "#############################################\n",
    "\n",
    "def remove_all_gap_columns(alignment):\n",
    "    \"\"\"\n",
    "    Remove columns from the alignment where all terminal sequences have a gap ('-').\n",
    "    \n",
    "    Parameters:\n",
    "        alignment (dict): A dictionary where keys are sequence names and values are sequence strings.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Updated alignment with columns removed where all terminal sequences have a gap.\n",
    "    \"\"\"\n",
    "    # Convert the alignment to a list of sequences\n",
    "    sequences = list(alignment.values())\n",
    "    num_sequences = len(sequences)\n",
    "    seq_length = len(sequences[0])  # Assuming all sequences are the same length\n",
    "\n",
    "    # Identify columns that need to be removed (where all terminal sequences have a gap)\n",
    "    columns_to_remove = []\n",
    "    for col in range(seq_length):\n",
    "        # Check if all terminal sequences (sp1, sp2, ...) have a gap ('-') in this column\n",
    "        if all(seq[col] == '-' for seq in sequences):\n",
    "            columns_to_remove.append(col)\n",
    "    \n",
    "    # Remove the identified columns from each sequence\n",
    "    for seq_name, seq in alignment.items():\n",
    "        # Create a new sequence with the columns removed\n",
    "        new_seq = ''.join(seq[col] for col in range(seq_length) if col not in columns_to_remove)\n",
    "        alignment[seq_name] = new_seq\n",
    "    \n",
    "    return alignment\n",
    "\n",
    "# Print\n",
    "remove_all_gap_columns(alignment)\n",
    "print(\"\\n\") # Jump a line\n",
    "print(\"Step 1.1 Delete columns presenting gaps in all taxa (artifacts from MAFFT)\")\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "###################################################\n",
    "# Step 1.2: Delete orphan nucleotides iteratively #\n",
    "###################################################\n",
    "\n",
    "def calculate_orphan_threshold_from_percentile(alignment, percentile=25, log=False, terminal_only=True):\n",
    "    \"\"\"\n",
    "    Calculate orphan threshold based on a specific percentile of gap lengths in the alignment.\n",
    "    \n",
    "    Args:\n",
    "        alignment (dict): A dictionary with sequence IDs as keys and sequences as values.\n",
    "        percentile (float): The percentile to use for setting the orphan threshold (e.g., 75 for the 75th percentile).\n",
    "        log (bool): If True, print the list of gap lengths and the computed orphan threshold.\n",
    "        terminal_only (bool): If True, only consider gap blocks at the terminal positions (start and end of sequences).\n",
    "        \n",
    "    Returns:\n",
    "        int: Calculated orphan threshold based on the specified percentile.\n",
    "    \"\"\"\n",
    "    gap_lengths = []\n",
    "\n",
    "    # Loop through each sequence to calculate gap lengths\n",
    "    for sequence in alignment.values():\n",
    "        sequence_length = len(sequence)\n",
    "        gap_count = 0\n",
    "        in_gap = False\n",
    "\n",
    "        # If terminal_only is True, we'll check only the first and last blocks of gaps\n",
    "        if terminal_only:\n",
    "            # Check for gaps starting at the beginning of the sequence\n",
    "            if sequence[0] == '-':\n",
    "                gap_count = 1\n",
    "                in_gap = True\n",
    "                for nucleotide in sequence[1:]:\n",
    "                    if nucleotide == '-':\n",
    "                        gap_count += 1\n",
    "                    else:\n",
    "                        break  # End of the first terminal gap block\n",
    "                gap_lengths.append(gap_count)  # Add the terminal gap block at the start\n",
    "                \n",
    "            # Check for gaps starting at the end of the sequence\n",
    "            gap_count = 0\n",
    "            in_gap = False\n",
    "            if sequence[-1] == '-':\n",
    "                gap_count = 1\n",
    "                in_gap = True\n",
    "                for nucleotide in reversed(sequence[:-1]):\n",
    "                    if nucleotide == '-':\n",
    "                        gap_count += 1\n",
    "                    else:\n",
    "                        break  # End of the last terminal gap block\n",
    "                gap_lengths.append(gap_count)  # Add the terminal gap block at the end\n",
    "        else:\n",
    "            # Loop through the sequence to find contiguous blocks of gaps (not restricted to terminals)\n",
    "            for nucleotide in sequence:\n",
    "                if nucleotide == '-':\n",
    "                    if not in_gap:\n",
    "                        in_gap = True  # Start of a new gap block\n",
    "                        gap_count = 1  # Start counting the length of the new gap block\n",
    "                    else:\n",
    "                        gap_count += 1  # Continue counting the gap block length\n",
    "                else:\n",
    "                    if in_gap:\n",
    "                        gap_lengths.append(gap_count)  # End of a gap block, save its length\n",
    "                        in_gap = False\n",
    "                        gap_count = 0  # Reset the gap count for the next block\n",
    "\n",
    "            # If the sequence ends with a gap block, make sure to add the last block's length\n",
    "            if in_gap:\n",
    "                gap_lengths.append(gap_count)\n",
    "        \n",
    "    # Calculate the specified percentile of gap lengths\n",
    "    orphan_threshold = int(np.percentile(gap_lengths, percentile))  # Use the given percentile (e.g., 75th percentile)\n",
    "    \n",
    "    # Print the list of gap lengths\n",
    "    if log:\n",
    "        print(\"List of gap lengths:\", gap_lengths)\n",
    "        print(\"Orphan threshold:\", orphan_threshold)\n",
    "        \n",
    "    return orphan_threshold\n",
    "    \n",
    "def delete_orphan_nucleotides2(alignment, orphan_threshold, log_changes=False):\n",
    "    \"\"\"\n",
    "    Iteratively eliminates orphan nucleotide blocks from the start and end of each sequence.\n",
    "    An orphan block is a short contiguous run of nucleotides near the terminal ends separated by many gaps.\n",
    "\n",
    "    Args:\n",
    "        alignment (dict): {sequence_id: sequence_string}\n",
    "        orphan_threshold (int): Max block length and max gap tolerance.\n",
    "        log_changes (bool): Whether to return a log of changes made.\n",
    "\n",
    "    Returns:\n",
    "        dict: Cleaned alignment.\n",
    "        str (optional): Log of changes made.\n",
    "    \"\"\"\n",
    "    change_log = []\n",
    "\n",
    "    def find_blocks(seq):\n",
    "        \"\"\"Find contiguous non-gap blocks as (start, end) tuples.\"\"\"\n",
    "        blocks = []\n",
    "        i = 0\n",
    "        while i < len(seq):\n",
    "            if seq[i] != '-':\n",
    "                start = i\n",
    "                while i < len(seq) and seq[i] != '-':\n",
    "                    i += 1\n",
    "                end = i\n",
    "                blocks.append((start, end))\n",
    "            else:\n",
    "                i += 1\n",
    "        return blocks\n",
    "\n",
    "    alignment_changed = True\n",
    "    while alignment_changed:\n",
    "        alignment_changed = False\n",
    "\n",
    "        for seq_id, sequence in alignment.items():\n",
    "            seq_list = list(sequence)\n",
    "            changed = False\n",
    "\n",
    "            # Iteratively check from left side\n",
    "            while True:\n",
    "                blocks = find_blocks(seq_list)\n",
    "                if len(blocks) < 2:\n",
    "                    break\n",
    "                first_start, first_end = blocks[0]\n",
    "                next_start = blocks[1][0]\n",
    "                gap_count = seq_list[first_end:next_start].count('-')\n",
    "\n",
    "                if (first_end - first_start < orphan_threshold) and (gap_count > orphan_threshold):\n",
    "                    deleted = ''.join(seq_list[first_start:first_end])\n",
    "                    seq_list[first_start:first_end] = ['-'] * (first_end - first_start)\n",
    "                    changed = True\n",
    "                    if log_changes:\n",
    "                        change_log.append(\n",
    "                            f\"{seq_id}: Left block {first_start}-{first_end} deleted ('{deleted}')\"\n",
    "                        )\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            # Iteratively check from right side\n",
    "            while True:\n",
    "                blocks = find_blocks(seq_list)\n",
    "                if len(blocks) < 2:\n",
    "                    break\n",
    "                last_start, last_end = blocks[-1]\n",
    "                prev_end = blocks[-2][1]\n",
    "                gap_count = seq_list[prev_end:last_start].count('-')\n",
    "\n",
    "                if (last_end - last_start < orphan_threshold) and (gap_count > orphan_threshold):\n",
    "                    deleted = ''.join(seq_list[last_start:last_end])\n",
    "                    seq_list[last_start:last_end] = ['-'] * (last_end - last_start)\n",
    "                    changed = True\n",
    "                    if log_changes:\n",
    "                        change_log.append(\n",
    "                            f\"{seq_id}: Right block {last_start}-{last_end} deleted ('{deleted}')\"\n",
    "                        )\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            if changed:\n",
    "                alignment_changed = True\n",
    "                alignment[seq_id] = ''.join(seq_list)\n",
    "\n",
    "    if log_changes:\n",
    "        return alignment, \"\\n\".join(change_log)\n",
    "    else:\n",
    "        return alignment\n",
    "\n",
    "print('\\n')\n",
    "print(\"Step 1.2. Delete terminal orphan nucleotides (given a threshold)\")\n",
    "print_colored_alignment(delete_orphan_nucleotides2(alignment, orphan_threshold=5))\n",
    "\n",
    "#########################################################\n",
    "# Step 2.1: Classify terminal (?) and internal gaps (-) #\n",
    "#########################################################\n",
    "\n",
    "def replace_terminal_gaps_dict(alignment):\n",
    "    \"\"\"\n",
    "    Replaces terminal gaps ('-') with '?' in a sequence alignment stored as a dictionary,\n",
    "    while keeping internal gaps as '-'.\n",
    "\n",
    "    Parameters:\n",
    "        alignment (dict): Dictionary with sequence names as keys and sequences as values.\n",
    "\n",
    "    Returns:\n",
    "        dict: Modified alignment with terminal gaps replaced by '?'.\n",
    "    \"\"\"\n",
    "    # Rename 'modified_alignment' to 'alignment'\n",
    "    for name, sequence in alignment.items():\n",
    "        # Find the first and last non-gap characters\n",
    "        first_non_gap = next((i for i, char in enumerate(sequence) if char != \"-\"), None)\n",
    "        last_non_gap = next((i for i, char in enumerate(reversed(sequence), 1) if char != \"-\"), None)\n",
    "        \n",
    "        if first_non_gap is not None and last_non_gap is not None:\n",
    "            last_non_gap = len(sequence) - last_non_gap  # Adjust reversed index\n",
    "            \n",
    "            # Replace terminal gaps with '?' and keep internal gaps as '-'\n",
    "            new_sequence = (\n",
    "                \"?\" * first_non_gap +\n",
    "                sequence[first_non_gap:last_non_gap + 1] +\n",
    "                \"?\" * (len(sequence) - last_non_gap - 1)\n",
    "            )\n",
    "            alignment[name] = new_sequence\n",
    "        else:\n",
    "            # Handle sequences with only gaps\n",
    "            alignment[name] = \"?\" * len(sequence)\n",
    "    \n",
    "    return alignment\n",
    "\n",
    "replace_terminal_gaps_dict(alignment)\n",
    "print(\"\\n\") # Jump a line\n",
    "print(\"Step 2.1 Classify terminal (?) and internal gaps (-)\")\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "##################################################\n",
    "# Step 1.3: Trim terminal invariants iteratively #\n",
    "##################################################\n",
    "def is_parsimony_non_informative(column):\n",
    "    \"\"\"\n",
    "    Check if a column is invariant.\n",
    "    A column is non-informative if:\n",
    "    - All characters are the same.\n",
    "    - The characters include '?' and only one other character (e.g., 'A' and '?').\n",
    "\n",
    "    Parameters:\n",
    "        column (list): A list of characters in a column (e.g., ['A', 'A', 'A', '?']).\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the column is non-informative, False if informative.\n",
    "    \"\"\"\n",
    "    unique_characters = set(column)\n",
    "        \n",
    "    # Check if the column has only one unique character (parsimony non-informative)\n",
    "    if len(unique_characters) == 1:\n",
    "        return True\n",
    "    \n",
    "    # Check if the column contains '?' and exactly one other unique character\n",
    "    if '?' in unique_characters and len(unique_characters) == 2:\n",
    "        return True\n",
    "    \n",
    "    # Otherwise, the column is informative (multiple unique characters without '?')\n",
    "    return False\n",
    "\n",
    "def remove_non_informative_positions(alignment, removed_indices=None):\n",
    "    \"\"\"\n",
    "    Remove parsimony non-informative positions from both the start and the end of the alignment.\n",
    "    \n",
    "    Parameters:\n",
    "        alignment (dict): Dictionary where keys are sequence names and values are sequences (strings).\n",
    "        removed_indices (list, optional): If provided, the function will append the indices of removed columns.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Updated alignment with non-informative positions removed.\n",
    "    \"\"\"\n",
    "    # Convert the alignment to a list of sequences\n",
    "    sequences = list(alignment.values())\n",
    "    seq_length = len(sequences[0])\n",
    "\n",
    "    # Remove non-informative positions from the start\n",
    "    first_position = 0\n",
    "    while first_position < seq_length and is_parsimony_non_informative([seq[first_position] for seq in sequences]):\n",
    "        first_position += 1\n",
    "\n",
    "    # Remove non-informative positions from the end\n",
    "    last_position = seq_length - 1\n",
    "    while last_position >= first_position and is_parsimony_non_informative([seq[last_position] for seq in sequences]):\n",
    "        last_position -= 1\n",
    "\n",
    "    # If logging, record the indices of removed columns\n",
    "    if removed_indices is not None:\n",
    "        removed_start = list(range(0, first_position))\n",
    "        removed_end = list(range(last_position + 1, seq_length))\n",
    "        removed_indices.extend(removed_start + removed_end)\n",
    "\n",
    "    # Build the updated alignment\n",
    "    for seq_name, seq in alignment.items():\n",
    "        alignment[seq_name] = seq[first_position:last_position + 1]\n",
    "\n",
    "    return alignment\n",
    "\n",
    "# Print\n",
    "remove_non_informative_positions(alignment)\n",
    "print(\"\\n\") # Jump a line\n",
    "print(\"Step 1.3. Trim parsimony non-informative characters in terminal position\")\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "##################################\n",
    "# Step 2.2 Internal missing data #\n",
    "##################################\n",
    "\n",
    "def replace_dashes_with_question_marks(alignment, internal_column_ranges=None, internal_leaves=\"all\", internal_method=\"manual\", internal_threshold=None):\n",
    "    \"\"\"\n",
    "    Replace dashes with question marks in the specified column ranges for each sequence in the alignment.\n",
    "    \n",
    "    Args:\n",
    "        alignment (dict): A dictionary with sequence IDs as keys and sequences as values.\n",
    "        internal_column_ranges (list of tuples, optional): A list of tuples where each tuple defines a range of columns \n",
    "                                                          (inclusive) to check for dashes. E.g., [(5, 10), (50, 60)].\n",
    "        internal_leaves (str or list, optional): If \"all\", replace dashes in all sequences. If a list of sequence IDs \n",
    "                                                   is provided, replace dashes in those sequences only.\n",
    "        internal_method (str, optional): Defines how to replace dashes.\n",
    "            - \"manual\": Specify column ranges and terminal sequences to replace dashes.\n",
    "            - \"semi\": Replace internal blocks of contiguous dashes larger than the threshold with question marks.\n",
    "        internal_threshold (int, optional): The threshold for \"semi\" method. Only internal blocks of contiguous gaps \n",
    "                                             larger than this threshold are replaced with question marks.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Updated alignment with dashes replaced by question marks in the specified columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If internal_leaves is a list, only consider those sequences\n",
    "    if internal_leaves != \"all\":\n",
    "        sequences_to_process = set(internal_leaves)\n",
    "    else:\n",
    "        sequences_to_process = set(alignment.keys())\n",
    "    \n",
    "    # Convert the alignment into a list of sequences for easier indexing\n",
    "    alignment = {seq_id: list(seq) for seq_id, seq in alignment.items()}  # Convert sequences to lists for mutability\n",
    "\n",
    "    if internal_method == \"manual\":\n",
    "        # Replace dashes with question marks in the specified column ranges\n",
    "        for seq_id, seq in alignment.items():\n",
    "            if seq_id not in sequences_to_process:\n",
    "                continue  # Skip sequences not in the internal_leavesinternal_terminals list\n",
    "            \n",
    "            for start, end in internal_column_ranges:\n",
    "                # Ensure the range is within the bounds of the sequence length\n",
    "                start = max(0, start)\n",
    "                end = min(len(seq), end)\n",
    "\n",
    "                # Replace dashes with question marks within the specified range\n",
    "                for i in range(start, end + 1):  # +1 because the end is inclusive\n",
    "                    if seq[i] == '-':\n",
    "                        seq[i] = '?'\n",
    "        \n",
    "    elif internal_method == \"semi\" and internal_threshold is not None:\n",
    "        # Replace internal blocks of contiguous dashes larger than the internal_threshold with question marks\n",
    "        for seq_id, seq in alignment.items():\n",
    "            if seq_id not in sequences_to_process:\n",
    "                continue  # Skip sequences not in the internal_leavesinternal_terminals list\n",
    "\n",
    "            # Identify contiguous blocks of gaps (internal and terminal)\n",
    "            gap_block_start = None\n",
    "            for i in range(len(seq)):\n",
    "                if seq[i] == '-':\n",
    "                    if gap_block_start is None:\n",
    "                        gap_block_start = i  # Start of a new gap block\n",
    "                else:\n",
    "                    if gap_block_start is not None:\n",
    "                        # End of a gap block\n",
    "                        gap_length = i - gap_block_start\n",
    "                        if gap_length > internal_threshold and gap_block_start != 0 and gap_block_start != len(seq) - gap_length:\n",
    "                            # It's an internal block larger than threshold, replace with '?'\n",
    "                            for j in range(gap_block_start, i):\n",
    "                                seq[j] = '?'\n",
    "                        gap_block_start = None  # Reset for the next block\n",
    "            # Check for a gap block at the end of the sequence\n",
    "            if gap_block_start is not None:\n",
    "                gap_length = len(seq) - gap_block_start\n",
    "                if gap_length > internal_threshold and gap_block_start != 0:\n",
    "                    for j in range(gap_block_start, len(seq)):\n",
    "                        seq[j] = '?'\n",
    "        \n",
    "    # Convert the list back to a string\n",
    "    alignment = {seq_id: ''.join(seq) for seq_id, seq in alignment.items()}\n",
    "\n",
    "    return alignment\n",
    "\n",
    "# Print\n",
    "#alignment = replace_dashes_with_question_marks(alignment, internal_column_ranges=[(1,30),(33,40)], internal_leaves=[\"sp2\"], internal_method=\"manual\")\n",
    "alignment = replace_dashes_with_question_marks(alignment, internal_leaves=[\"sp2\"], internal_method=\"semi\", internal_threshold=2)\n",
    "print(\"\\n\") # Jump a line\n",
    "print(\"Step 1.3. Treat internal missing data\")\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "#########################\n",
    "# Step 3.1 Partitioning #\n",
    "#########################\n",
    "\n",
    "def add_breaks_terminal(alignment):\n",
    "    \"\"\"\n",
    "    Add # in all instances of terminal gap opening/closure (indicated by ?).\n",
    "       \n",
    "    Parameters:\n",
    "        alignment (dict): Dictionary where keys are sequence names and values are sequences.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Updated alignment with '#' before terminal gap opening and after terminal gap closure.\n",
    "    \"\"\"\n",
    "    # Determine the length of the sequences\n",
    "    seq_length = len(next(iter(alignment.values())))\n",
    "\n",
    "    # Initialize a list to keep track of positions that need '#' in all sequences\n",
    "    hash_positions = [False] * seq_length\n",
    "\n",
    "    # Iterate through each sequence to find gap regions\n",
    "    for seq in alignment.values():\n",
    "        i = 0\n",
    "        while i < seq_length:\n",
    "            if seq[i] == '?':\n",
    "                # Found the start of a gap region\n",
    "                start = i\n",
    "                while i < seq_length and seq[i] == '?':\n",
    "                    i += 1\n",
    "                end = i\n",
    "                # Mark the positions for this gap region (avoid marking the first column)\n",
    "                if start > 0:\n",
    "                    hash_positions[start] = True\n",
    "                if end < seq_length:\n",
    "                    hash_positions[end] = True\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    # Update each sequence with '#' at the identified positions\n",
    "    for key in alignment:\n",
    "        new_seq = []\n",
    "        for i in range(seq_length):\n",
    "            if hash_positions[i]:\n",
    "                new_seq.append('#')\n",
    "            new_seq.append(alignment[key][i])\n",
    "        # Handle the case where the last position is a gap\n",
    "        if hash_positions[-1]:\n",
    "            new_seq.append('#')\n",
    "        alignment[key] = ''.join(new_seq)\n",
    "        \n",
    "def classify_and_insert_hashtags(alignment, \n",
    "                                 partitioning_round=1, \n",
    "                                 log_csv_output=False, \n",
    "                                 csv_file_path=\"contiguous_invariant_blocks.csv\"):\n",
    "    # Step 1: Classify columns as invariant or variant\n",
    "    num_sequences = len(alignment)\n",
    "    num_columns = len(next(iter(alignment.values())))  # Get the number of columns from one sequence\n",
    "\n",
    "    column_types = []  # To store the type of each column (invariant/variant)\n",
    "    contiguous_invariant_blocks = []  # To store lengths and positions of invariant blocks\n",
    "\n",
    "    for col_idx in range(num_columns):\n",
    "        column = [seq[col_idx] for seq in alignment.values()]\n",
    "        unique_values = set(column)\n",
    "        if len(unique_values - {'?'}) == 1:\n",
    "            column_types.append('invariant')\n",
    "        else:\n",
    "            column_types.append('variant')\n",
    "\n",
    "    # Step 2: Identify contiguous invariant columns and their lengths\n",
    "    current_invariant_block = None\n",
    "    for col_idx in range(num_columns):\n",
    "        if column_types[col_idx] == 'invariant':\n",
    "            if current_invariant_block is None:\n",
    "                current_invariant_block = {'start': col_idx, 'length': 1}\n",
    "            else:\n",
    "                current_invariant_block['length'] += 1\n",
    "        else:\n",
    "            if current_invariant_block:\n",
    "                contiguous_invariant_blocks.append(current_invariant_block)\n",
    "                current_invariant_block = None\n",
    "    if current_invariant_block:\n",
    "        contiguous_invariant_blocks.append(current_invariant_block)\n",
    "\n",
    "    # Step 3: Log and optionally process blocks\n",
    "    contiguous_invariant_blocks.sort(key=lambda x: x['length'], reverse=True)\n",
    "    block_lengths = {}\n",
    "    for block in contiguous_invariant_blocks:\n",
    "        block_lengths.setdefault(block['length'], []).append(block)\n",
    "\n",
    "    if partitioning_round == \"max\":\n",
    "        add_breaks_terminal(alignment)  # Call the other function instead of inserting hashtags\n",
    "    else:\n",
    "        # Step 4: Track the positions for inserting hashtags\n",
    "        hashtag_positions = []\n",
    "        block_lengths_sorted = sorted(block_lengths.keys(), reverse=True)\n",
    "        for block_length in block_lengths_sorted[:partitioning_round]:\n",
    "            blocks = block_lengths[block_length]\n",
    "            for block in blocks:\n",
    "                start_idx = block['start']\n",
    "                end_idx = start_idx + block['length'] - 1\n",
    "                middle_idx = (start_idx + end_idx) // 2\n",
    "                hashtag_positions.append(middle_idx)\n",
    "\n",
    "        # Step 5: Insert hashtags\n",
    "        for seq_id, seq in alignment.items():\n",
    "            sorted_positions = sorted(hashtag_positions)\n",
    "            shift = 0\n",
    "            for middle_idx in sorted_positions:\n",
    "                adjusted_idx = middle_idx + shift\n",
    "                seq = seq[:adjusted_idx + 1] + '#' + seq[adjusted_idx + 1:]\n",
    "                shift += 1\n",
    "            alignment[seq_id] = seq\n",
    "\n",
    "    # Step 6: Optionally log to CSV\n",
    "    if log_csv_output:\n",
    "        file_dir = os.path.dirname(csv_file_path)\n",
    "        if not os.path.exists(file_dir) and file_dir:\n",
    "            os.makedirs(file_dir)\n",
    "\n",
    "        with open(csv_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Start', 'End', 'Length'])\n",
    "            for block in contiguous_invariant_blocks:\n",
    "                start_idx = block['start']\n",
    "                end_idx = start_idx + block['length'] - 1\n",
    "                writer.writerow([start_idx, end_idx, block['length']])\n",
    "        print(f\"Log of contiguous invariant blocks written to {csv_file_path}\")\n",
    "\n",
    "    return alignment, contiguous_invariant_blocks\n",
    "\n",
    "\n",
    "# Print\n",
    "classify_and_insert_hashtags(alignment,partitioning_round=1, log_csv_output=False)\n",
    "print(\"\\n\") # Jump a line\n",
    "print(\"Step 3.1. Insert pound sign in the n-largest block of contiguous invariants\")\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "########################\n",
    "# Step 3.2: Refinement #\n",
    "########################\n",
    "\n",
    "def refinement_question2hyphen(alignment):\n",
    "    \"\"\"\n",
    "    Replaces contiguous '?' characters flanked by '#' with '-' in the sequence alignment.\n",
    "    This includes blocks surrounded by '#' as well as the first and last blocks that are flanked only on one side.\n",
    "\n",
    "    Parameters:\n",
    "        alignment (dict): Dictionary with sequence names as keys and sequences as values.\n",
    "\n",
    "    Returns:\n",
    "        dict: Modified alignment with '?' replaced by '-' where flanked by '#'.\n",
    "    \"\"\"\n",
    "    # Iterate through each sequence in the alignment\n",
    "    for name, sequence in alignment.items():\n",
    "        # Replace '?' surrounded by '#' on both sides (internal blocks)\n",
    "        modified_sequence = re.sub(r'(?<=#)\\?+(?=#)', lambda m: '-' * len(m.group(0)), sequence)\n",
    "        \n",
    "        # Replace '?' in the first block (flanked by # on the right)\n",
    "        modified_sequence = re.sub(r'^(\\?+)(?=#)', lambda m: '-' * len(m.group(0)), modified_sequence)\n",
    "        \n",
    "        # Replace '?' in the last block (flanked by # on the left)\n",
    "        modified_sequence = re.sub(r'(?<=#)(\\?+)$', lambda m: '-' * len(m.group(0)), modified_sequence)\n",
    "\n",
    "        # Update the alignment dictionary with the modified sequence\n",
    "        alignment[name] = modified_sequence\n",
    "\n",
    "    return alignment\n",
    "\n",
    "# Print\n",
    "refinement_question2hyphen(alignment) # Replaces blocks of missing data (consecutive '?' characters flanked by '#') with '-'.\n",
    "print(\"\\n\") # Jump a line\n",
    "print(\"Step 3.2.1 Replace ? flanked by # with -\")\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "def remove_columns_with_W(alignment: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Remove problematic columns in a DNA alignment:\n",
    "    \n",
    "    1. For each sequence, identify 15 contiguous 'W' or 'w' characters. \n",
    "       Mark those columns for deletion.\n",
    "    2. Remove columns where all values are:\n",
    "       - only 'W'\n",
    "       - only 'W' and '?'\n",
    "       - only 'W', '?', and '-'\n",
    "    \n",
    "    Args:\n",
    "        alignment (dict): Dictionary of {sequence_id: sequence_string}\n",
    "\n",
    "    Returns:\n",
    "        dict: Cleaned alignment with columns removed\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # Convert alignment to matrix\n",
    "    seq_ids = list(alignment.keys())\n",
    "    seqs = list(alignment.values())\n",
    "    alignment_array = np.array([list(seq) for seq in seqs])\n",
    "    n_rows, n_cols = alignment_array.shape\n",
    "\n",
    "    columns_to_remove = set()\n",
    "\n",
    "    # Step 1: Find 15 contiguous W/w in any sequence\n",
    "    for row in alignment_array:\n",
    "        upper_row = [char.upper() for char in row]\n",
    "        for i in range(n_cols - 14):\n",
    "            if all(base == 'W' for base in upper_row[i:i+15]):\n",
    "                columns_to_remove.update(range(i, i + 15))\n",
    "\n",
    "    # Step 2: Remove columns with only W / W+? / W+?+-\n",
    "    for col_idx in range(n_cols):\n",
    "        col_bases = set(alignment_array[:, col_idx].astype(str).flatten().tolist())\n",
    "        col_bases_upper = {b.upper() for b in col_bases}\n",
    "        if col_bases_upper.issubset({'W'}) or \\\n",
    "           col_bases_upper.issubset({'W', '?'}) or \\\n",
    "           col_bases_upper.issubset({'W', '?', '-'}):\n",
    "            columns_to_remove.add(col_idx)\n",
    "\n",
    "    # Remove columns\n",
    "    columns_to_keep = sorted(set(range(n_cols)) - columns_to_remove)\n",
    "    cleaned_array = alignment_array[:, columns_to_keep]\n",
    "\n",
    "    # Convert back to dictionary\n",
    "    cleaned_alignment = {\n",
    "        seq_id: ''.join(cleaned_array[i]) for i, seq_id in enumerate(seq_ids)\n",
    "    }\n",
    "\n",
    "    return cleaned_alignment\n",
    "\n",
    "# Print\n",
    "remove_columns_with_W(alignment) \n",
    "print(\"\\n\") # Jump a line\n",
    "print(\"Step 3.2.2 Delete columns of W (artifacts from internal missing data identification by GB2MSA)\")\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "def n2question_func(alignment: dict, leaves='all', log=False):\n",
    "    \"\"\"\n",
    "    Replace all ambiguous nucleotides 'N' or 'n' with '?' in selected sequences.\n",
    "\n",
    "    Parameters:\n",
    "    - alignment (dict): Dictionary where keys are sequence names and values are DNA sequences (str).\n",
    "    - leaves (str or list): Sequence name(s) to apply the replacement. Use 'all' to apply to all sequences.\n",
    "    - log (bool): If True, also return a log of replaced blocks with their positions.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Modified alignment with 'N'/'n' replaced with '?'.\n",
    "    - list (optional): List of tuples (seq_name, start, end) for each replaced block.\n",
    "    \"\"\"\n",
    "    if leaves == 'all':\n",
    "        leaves_to_process = alignment.keys()\n",
    "    elif isinstance(leaves, str):\n",
    "        leaves_to_process = [leaves]\n",
    "    else:\n",
    "        leaves_to_process = leaves\n",
    "\n",
    "    updated_alignment = {}\n",
    "    replacement_log = []\n",
    "\n",
    "    for name, seq in alignment.items():\n",
    "        if name in leaves_to_process:\n",
    "            new_seq = []\n",
    "            i = 0\n",
    "            while i < len(seq):\n",
    "                if seq[i] in ('N', 'n'):\n",
    "                    start = i\n",
    "                    while i < len(seq) and seq[i] in ('N', 'n'):\n",
    "                        i += 1\n",
    "                    end = i - 1\n",
    "                    replacement_log.append((name, start, end))\n",
    "                    new_seq.extend(['?'] * (end - start + 1))\n",
    "                else:\n",
    "                    new_seq.append(seq[i])\n",
    "                    i += 1\n",
    "            updated_alignment[name] = ''.join(new_seq)\n",
    "        else:\n",
    "            updated_alignment[name] = seq\n",
    "\n",
    "    if log:\n",
    "        return updated_alignment, replacement_log\n",
    "    return updated_alignment\n",
    "\n",
    "\n",
    "# Print\n",
    "alignment = n2question_func(alignment, leaves='sp4') \n",
    "print(\"\\n\") # Jump a line\n",
    "print(\"Step 3.2.3 Replace IUPAC N with question marks)\")\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "###########################\n",
    "# ADDITIONAL LOG FUNCTION #\n",
    "###########################\n",
    "\n",
    "def compute_summary_after(alignment):\n",
    "    num_seqs = len(alignment)\n",
    "\n",
    "    # Transpose to columns\n",
    "    columns = list(zip(*alignment.values()))\n",
    "    \n",
    "    # Count columns that contain pound signs\n",
    "    total_pound = sum('#' in col for col in columns)\n",
    "    \n",
    "    # Alignment length excluding columns of only pound signs\n",
    "    aln_length = sum(1 for col in columns if set(col) != {'#'})\n",
    "\n",
    "    # Count nucleotide and gap characters\n",
    "    total_nt = sum(c in \"ACGTacgt\" for seq in alignment.values() for c in seq)\n",
    "    total_gaps = sum(seq.count(\"-\") for seq in alignment.values())\n",
    "    total_ns = sum(c in \"Nn\" for seq in alignment.values() for c in seq)\n",
    "    total_qm = sum(seq.count(\"?\") for seq in alignment.values())  # <- includes everything now\n",
    "\n",
    "    # Count additional missing data as gap-only blocks between #\n",
    "    missing_by_partition = 0\n",
    "    for seq in alignment.values():\n",
    "        parts = seq.split(\"#\")\n",
    "        for part in parts:\n",
    "            if all(c == '-' for c in part):\n",
    "                missing_by_partition += len(part)\n",
    "\n",
    "    total_missing = total_qm + missing_by_partition\n",
    "\n",
    "    return {\n",
    "        \"num_seqs\": num_seqs,\n",
    "        \"aln_length\": aln_length,\n",
    "        \"total_nt\": total_nt,\n",
    "        \"total_gaps\": total_gaps,\n",
    "        \"total_ns\": total_ns,\n",
    "        \"total_qm\": total_qm,\n",
    "        \"total_pound\": total_pound,\n",
    "        \"missing_by_partition\": missing_by_partition,\n",
    "        \"total_missing\": total_missing\n",
    "    }\n",
    "\n",
    "def detect_fully_missing_partitions(alignment):\n",
    "    \"\"\"\n",
    "    Logs all `?` and `-` from fully missing partitions.\n",
    "    A fully missing partition is a region (between #) in which a sequence has only dashes.\n",
    "    \"\"\"\n",
    "    log_entries = []\n",
    "    total_question_marks = 0\n",
    "    total_dash_from_missing_partitions = 0\n",
    "\n",
    "    for seq_id, seq in alignment.items():\n",
    "        total_question_marks += seq.count(\"?\")\n",
    "\n",
    "        parts = seq.split(\"#\")\n",
    "        col_index = 0  # absolute position tracker\n",
    "        for i, part in enumerate(parts):\n",
    "            if all(c == '-' for c in part):\n",
    "                dash_count = len(part)\n",
    "                total_dash_from_missing_partitions += dash_count\n",
    "                start = col_index\n",
    "                end = col_index + dash_count - 1\n",
    "                log_entries.append(f\"{seq_id}: partition {i} ({start}â€“{end}, length {dash_count}) fully missing (all '-')\")\n",
    "            col_index += len(part) + 1  # +1 for the '#' removed in split\n",
    "\n",
    "    summary = (\n",
    "        f\"Total '?' characters: {total_question_marks}\\n\"\n",
    "        f\"Total '-' characters in fully missing partitions: {total_dash_from_missing_partitions}\\n\"\n",
    "        f\"Combined total: {total_question_marks + total_dash_from_missing_partitions}\\n\"\n",
    "    )\n",
    "\n",
    "    return summary + \"\\n\" + \"\\n\".join(log_entries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5d3e9c",
   "metadata": {},
   "source": [
    "## Command-line functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25404b0",
   "metadata": {},
   "source": [
    "Functions using *argparse* to run in the command line. They were tested in Mac OS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de218d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sp1': 'G#CACCGTCGCCAACAGTAGTCCT#CCACCGTCGCCACCGTC?CCA#ACAG',\n",
       " 'sp2': 'G#CACCGTCGCCAACAGTAGTCCT#CCACCGTCGCC??????TCCA#ACA?',\n",
       " 'sp3': 'G#-ACCGTCGCCAACAGTAGTCCT#CCACCGTCGCCACCGT?????#----',\n",
       " 'sp4': '-#C-CCGTCGCC????????????#??????TCGC?ACCGTCGCCA#ACAT',\n",
       " 'sp5': 'A#CA-CGTCGCCAACAGTAGTCCT#CCACCGTCGCCACCGTCGCCA#ACAG',\n",
       " 'sp6': '-#CAC-GTCGCCAACAGT??????#???CCGTCGCCACCGTCGCCA#----'}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the integrated function\n",
    "def prepDyn(input_file=None, \n",
    "            GB_input=None,\n",
    "            input_format=\"fasta\",\n",
    "            MSA=False,\n",
    "            output_file=None,\n",
    "            output_format=\"fasta\",\n",
    "            log=False,\n",
    "            # Trimming parameters\n",
    "            orphan_method=None,\n",
    "            orphan_threshold=10,\n",
    "            percentile=25,\n",
    "            del_inv=True,\n",
    "            # Missing data parameters\n",
    "            internal_method=None,\n",
    "            internal_column_ranges=None,\n",
    "            internal_leaves=\"all\",\n",
    "            internal_threshold=None,\n",
    "            n2question=None,\n",
    "            # Partitioning parameters\n",
    "            partitioning_round=0\n",
    "            ):\n",
    "    \"\"\"\n",
    "    Preprocess missing data for dynamic homology in PhyG. First, columns containing \n",
    "    only gaps, orphan nucleotides, and invariant columns can be trimmed. Second, \n",
    "    missing data is coded with question marks. Third, partitions are delimited in \n",
    "    highly conserved regions.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to the input alignment file or directory. Ignored if GB_input is provided.\n",
    "        GB_input (str): Path to a CSV/TSV file containing GenBank accession numbers. If provided,\n",
    "                        sequences will be downloaded from GenBank and aligned before preprocessing.\n",
    "        input_format (str): Format of the input alignment. Options: 'fasta' (default), \n",
    "                            'clustal', 'phylip', or any format accepted by Biopython. \n",
    "        MSA (bool): Whether to perform MSA if input sequences specified in input_file are unaligned\n",
    "        orphan_method (str): The trimming method. By default, trimming orphan nucleotides\n",
    "                             is not performed. Options:\n",
    "                            - 'auto': trim using the 25th percentile;\n",
    "                            - 'semi': trim with a manual threshold.\n",
    "        orphan_threshold (int): Threshold used to trim orphan nucleotides if orphan_method = 'semi'.\n",
    "        percentile (float): Used with orphan_method = 'auto' to define trimming threshold.\n",
    "        del_inv (bool): Whether to trim invariant terminal columns. Default is True.\n",
    "        internal_method (str): Defines how to identify internal missing data. Automatic identificaton\n",
    "                               of missing data is made if GB_input is provided. Otherwise, naive \n",
    "                               options to identify internal missing data are:\n",
    "                               - \"manual\": Use column ranges;\n",
    "                               - \"semi\": Use a threshold for gaps.\n",
    "        internal_column_ranges (list): Column ranges (inclusive) if internal_method = 'manual'.\n",
    "        internal_leaves (str or list): Sequences to apply internal missing data replacement \n",
    "                                       if internal_method is not \"None\".\n",
    "        internal_threshold (int): Used with internal_method = 'semi' to define gap threshold.\n",
    "                                  Contiguous '-' larger than the threshold are replaced with '?'.\n",
    "        partitioning_round (int): Number of partitioning round. Invariant regions are sorted by length\n",
    "                                  in descendant order and the n-largest block(s) partitioned using '#'.\n",
    "                                  If \"max\" is specified, pound signs are inserted arund all blocks of \n",
    "                                  missing data. \n",
    "        output_file (str): Custom prefix for output files. If None, base_name from input_file is used.\n",
    "        output_format (str): Output format. Default is 'fasta'.\n",
    "        log (bool): Whether to write a log with wall-clock time. Default is False.\n",
    "        n2question (str or list): If specified, replaces ambiguous nucleotide 'N' or 'n' with '?'. If None (default), n2question is not performed. If 'all', apply to all sequences. If you want to apply to only one sequence, write the name of this sequence. If you want to apply to multiple sequences but no all, wrie the list of sequences.\n",
    "                                  \n",
    "    Returns:\n",
    "        dict: The preprocessed unaligned sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    # Start timers if logging is enabled\n",
    "    if log:\n",
    "        start_wall_time = time.time()\n",
    "        start_cpu_time = time.process_time()\n",
    "\n",
    "    # Step 1: Run GB2MSA if GenBank input is provided\n",
    "    if GB_input is not None:\n",
    "        print(\"Running GB2MSA on GenBank input...\")\n",
    "        gb_output_prefix = output_file if output_file else \"output\"\n",
    "        cleaned_files = GB2MSA(GB_input, output_prefix=gb_output_prefix, log=False)\n",
    "\n",
    "        for file in cleaned_files:\n",
    "            gene_name = os.path.splitext(os.path.basename(file))[0].replace(\"output_\", \"\")\n",
    "            if output_file:\n",
    "                specific_output_prefix = f\"{output_file}_{gene_name}\"\n",
    "            else:\n",
    "                specific_output_prefix = f\"{gene_name}\"\n",
    "\n",
    "            alignment = AlignIO.read(file, \"fasta\")\n",
    "            alignment_dict = {record.id: str(record.seq) for record in alignment}\n",
    "            \n",
    "            prepDyn(input_file=alignment_dict,\n",
    "                    input_format=\"dict\",\n",
    "                    orphan_method=orphan_method,\n",
    "                    orphan_threshold=orphan_threshold,\n",
    "                    percentile=percentile,\n",
    "                    del_inv=del_inv,\n",
    "                    internal_method=internal_method,\n",
    "                    internal_column_ranges=internal_column_ranges,\n",
    "                    internal_leaves=internal_leaves,\n",
    "                    internal_threshold=internal_threshold,\n",
    "                    n2question=n2question,\n",
    "                    partitioning_round=partitioning_round,\n",
    "                    output_format=output_format,\n",
    "                    log=log,\n",
    "                    output_file=specific_output_prefix)\n",
    "\n",
    "        return\n",
    "\n",
    "    # Step 2: If a folder is provided, process each alignment inside\n",
    "    if isinstance(input_file, str) and os.path.isdir(input_file):\n",
    "        for file_name in os.listdir(input_file):\n",
    "            if file_name.endswith(f\".{input_format}\"):\n",
    "                file_path = os.path.join(input_file, file_name)\n",
    "                prepDyn(file_path,\n",
    "                      input_format=input_format,\n",
    "                      MSA=MSA,\n",
    "                      orphan_method=orphan_method,\n",
    "                      orphan_threshold=orphan_threshold,\n",
    "                      percentile=percentile,\n",
    "                      del_inv=del_inv,\n",
    "                      internal_method=internal_method,\n",
    "                      internal_column_ranges=internal_column_ranges,\n",
    "                      internal_leaves=internal_leaves,\n",
    "                      internal_threshold=internal_threshold,\n",
    "                      n2question=n2question,\n",
    "                      output_format=output_format)\n",
    "        return\n",
    "\n",
    "    # Step 3: Read and process alignment\n",
    "    if isinstance(input_file, dict):\n",
    "        alignment = input_file\n",
    "    else:\n",
    "        alignment = AlignIO.read(input_file, input_format)\n",
    "        alignment = {record.id: str(record.seq) for record in alignment}\n",
    "\n",
    "    # Optional MAFFT alignment\n",
    "    if MSA:\n",
    "        with tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as tmp_in:\n",
    "            for k, v in alignment.items():\n",
    "                tmp_in.write(f\">{k}\\n{v}\\n\")\n",
    "            tmp_in_path = tmp_in.name\n",
    "\n",
    "        tmp_out_path = tmp_in_path + \"_aligned.fasta\"\n",
    "        try:\n",
    "            subprocess.run([\"mafft\", \"--auto\", tmp_in_path], stdout=open(tmp_out_path, \"w\"), stderr=subprocess.DEVNULL, check=True)\n",
    "        except subprocess.CalledProcessError:\n",
    "            raise RuntimeError(\"MAFFT alignment failed.\")\n",
    "\n",
    "        alignment = AlignIO.read(tmp_out_path, \"fasta\")\n",
    "        alignment = {record.id: str(record.seq) for record in alignment}\n",
    "\n",
    "        os.remove(tmp_in_path)\n",
    "        os.remove(tmp_out_path)\n",
    "\n",
    "    # Summary of characteristics before preprocessing\n",
    "    if log:\n",
    "        num_seqs = len(alignment)\n",
    "        aln_length = len(next(iter(alignment.values())))\n",
    "        total_nt = sum(c.upper() in \"ACGT\" for seq in alignment.values() for c in seq)\n",
    "        total_gaps = sum(seq.count(\"-\") for seq in alignment.values())\n",
    "        total_ns = sum(c in \"Nn\" for seq in alignment.values() for c in seq)\n",
    "\n",
    "    # 3.1 Remove columns with gaps in all leaves\n",
    "    remove_all_gap_columns(alignment)\n",
    "    \n",
    "    # 3.2 Trim orphan nucleotides\n",
    "    orphan_log = None\n",
    "    if orphan_method == \"percentile\":\n",
    "        orphan_threshold = calculate_orphan_threshold_from_percentile(alignment, percentile, terminal_only=True)\n",
    "        if log:\n",
    "            alignment, orphan_log = delete_orphan_nucleotides2(alignment, orphan_threshold, log_changes=True)\n",
    "        else:\n",
    "            alignment = delete_orphan_nucleotides2(alignment, orphan_threshold)\n",
    "    elif orphan_method == \"semi\":\n",
    "        if log:\n",
    "            alignment, orphan_log = delete_orphan_nucleotides2(alignment, orphan_threshold, log_changes=True)\n",
    "        else:\n",
    "            alignment = delete_orphan_nucleotides2(alignment, orphan_threshold)\n",
    "\n",
    "\n",
    "    # 3.3 Replace terminal gaps with ?\n",
    "    alignment = replace_terminal_gaps_dict(alignment)\n",
    "\n",
    "    # 3.4 Trim invariant columns\n",
    "    removed_cols = []\n",
    "    if del_inv:\n",
    "        alignment = remove_non_informative_positions(alignment, removed_indices=removed_cols)\n",
    "\n",
    "    alignment = replace_terminal_gaps_dict(alignment)\n",
    "\n",
    "    # 3.5 Replace internal gaps with ?\n",
    "    if internal_method == \"manual\":\n",
    "        alignment = replace_dashes_with_question_marks(alignment=alignment, \n",
    "                                                       internal_column_ranges=internal_column_ranges, \n",
    "                                                       internal_leaves=internal_leaves, \n",
    "                                                       internal_method=\"manual\")\n",
    "    elif internal_method == \"semi\":\n",
    "        alignment = replace_dashes_with_question_marks(alignment=alignment, \n",
    "                                                       internal_leaves=internal_leaves, \n",
    "                                                       internal_method=\"semi\", \n",
    "                                                       internal_threshold=internal_threshold)\n",
    "\n",
    "    # 3.6 Replace ambiguous nucleotides N/n with ?\n",
    "    n_blocks = []\n",
    "    if n2question is not None:\n",
    "        alignment, n_blocks = n2question_func(alignment, leaves=n2question, log=True)\n",
    "\n",
    "\n",
    "    # 3.7 Successive partition\n",
    "    classify_and_insert_hashtags(alignment, \n",
    "                                 partitioning_round=partitioning_round)\n",
    "    refinement_question2hyphen(alignment)\n",
    "    alignment = remove_columns_with_W(alignment)\n",
    "\n",
    "    # Step 4: Write output file\n",
    "    records = [SeqRecord(Seq(seq), id=key, description=\"\") for key, seq in alignment.items()]\n",
    "    base_name = os.path.splitext(os.path.basename(input_file))[0] if isinstance(input_file, str) else \"alignment\"\n",
    "\n",
    "    if output_file:\n",
    "        output_prefix = f\"{output_file}\"\n",
    "    else:\n",
    "        output_prefix = base_name\n",
    "\n",
    "    output_path = f\"{output_prefix}_preprocessed.{output_format}\"\n",
    "    with open(output_path, \"w\") as output_handle:\n",
    "        SeqIO.write(records, output_handle, output_format)\n",
    "\n",
    "    # Step 5: Write log\n",
    "    if log:\n",
    "        end_wall_time = time.time()\n",
    "        end_cpu_time = time.process_time()\n",
    "        wall_time = end_wall_time - start_wall_time\n",
    "        cpu_time = end_cpu_time - start_cpu_time\n",
    "        \n",
    "        # Build the command string with all parameters and their current values\n",
    "        cmd_parts = [\"prepDyn(\"]\n",
    "        params = {\n",
    "            \"input_file\": input_file,\n",
    "            \"GB_input\": GB_input,\n",
    "            \"input_format\": input_format,\n",
    "            \"MSA\": MSA,\n",
    "            \"output_file\": output_file,\n",
    "            \"output_format\": output_format,\n",
    "            \"log\": log,\n",
    "            \"orphan_method\": orphan_method,\n",
    "            \"orphan_threshold\": orphan_threshold,\n",
    "            \"percentile\": percentile,\n",
    "            \"del_inv\": del_inv,\n",
    "            \"internal_method\": internal_method,\n",
    "            \"internal_column_ranges\": internal_column_ranges,\n",
    "            \"internal_leaves\": internal_leaves,\n",
    "            \"internal_threshold\": internal_threshold,\n",
    "            \"n2question\": n2question,\n",
    "            \"partitioning_round\": partitioning_round\n",
    "        }\n",
    "        param_strs = []\n",
    "        for k, v in params.items():\n",
    "            if v is None:\n",
    "                param_strs.append(f\"{k}=None\")\n",
    "            elif isinstance(v, str):\n",
    "                param_strs.append(f\"{k}='{v}'\")\n",
    "            else:\n",
    "                param_strs.append(f\"{k}={repr(v)}\")\n",
    "        cmd_parts.append(\", \".join(param_strs))\n",
    "        cmd_parts.append(\")\")\n",
    "        cmd_line = \"\".join(cmd_parts)\n",
    "        \n",
    "        log_path = f\"{output_prefix}_log.txt\"\n",
    "        with open(log_path, \"w\") as log_file:\n",
    "            log_file.write(\"--- Command used ---\\n\")\n",
    "            log_file.write(f\"{cmd_line}\\n\\n\")\n",
    "\n",
    "            log_file.write(\"--- Step 1: Summary before preprocessing ---\\n\")\n",
    "            log_file.write(f\"No. sequences: {num_seqs}\\n\")\n",
    "            log_file.write(f\"No. columns: {aln_length}\\n\")\n",
    "            log_file.write(f\"Total no. nucleotides (A/C/G/T only): {total_nt} bp\\n\")\n",
    "            log_file.write(f\"Total no. gaps (-): {total_gaps}\\n\")\n",
    "            log_file.write(f\"Total no. IUPAC N: {total_ns}\\n\\n\")\n",
    "\n",
    "            # log trimming: invariants\n",
    "            if del_inv:\n",
    "                log_file.write(\"--- Step 2: Trimming (invariant columns) ---\\n\")\n",
    "                log_file.write(f\"{removed_cols}\\n\\n\")\n",
    "            # log trimming: orphans\n",
    "            if orphan_log:\n",
    "                log_file.write(\"--- Step 2: Trimming (orphan nucleotides) ---\\n\")\n",
    "                log_file.write(f\"{orphan_log}\\n\\n\")\n",
    "            \n",
    "            # log missing: n2question\n",
    "            if n_blocks:\n",
    "                log_file.write(\"--- Step 3: Missing data identification (Ns replaced with '?') ---\\n\")\n",
    "                for seq_name, start, end in n_blocks:\n",
    "                    log_file.write(f\"{seq_name}: {start}â€“{end}\\n\")\n",
    "                log_file.write(\"\\n\")\n",
    "            \n",
    "            # log terminal ? detection\n",
    "            missing_partition_log = detect_fully_missing_partitions(alignment)\n",
    "            if missing_partition_log:\n",
    "                log_file.write(\"--- Step 3: Missing data identification ---\\n\")\n",
    "                log_file.write(f\"{missing_partition_log}\\n\\n\")\n",
    "\n",
    "\n",
    "            # log partitioning (#)\n",
    "            if partitioning_round > 0:\n",
    "                log_file.write(\"--- Step 4: Partitioning (columns with # inserted) ---\\n\")\n",
    "                # Transpose alignment to columns\n",
    "                columns = list(zip(*alignment.values()))\n",
    "                pound_indices = [i for i, col in enumerate(columns) if '#' in col]\n",
    "                log_file.write(f\"{pound_indices}\\n\\n\")\n",
    "            \n",
    "            # log preprocessed summary            \n",
    "            summary_post = compute_summary_after(alignment)\n",
    "            log_file.write(\"--- Summary after preprocessing ---\\n\")\n",
    "            log_file.write(f\"No. sequences: {summary_post['num_seqs']}\\n\")\n",
    "            log_file.write(f\"No. columns: {summary_post['aln_length']}\\n\")\n",
    "            log_file.write(f\"No. pound sign columns (#): {summary_post['total_pound']}\\n\")\n",
    "            log_file.write(f\"Total no. nucleotides (A/C/G/T): {summary_post['total_nt']} bp\\n\")\n",
    "            log_file.write(f\"Total no. gaps (-): {summary_post['total_gaps']}\\n\")\n",
    "            log_file.write(f\"Total no. IUPAC N: {summary_post['total_ns']}\\n\")\n",
    "            log_file.write(f\"Total no. missing values (?): {summary_post['total_missing']}\\n\\n\")\n",
    "            \n",
    "            # log run time\n",
    "            log_file.write(\"--- Run time ---\\n\")\n",
    "            log_file.write(f\"Wall-clock time: {wall_time:.8f} seconds\\n\")\n",
    "            log_file.write(f\"CPU time: {cpu_time:.8f} seconds\\n\")\n",
    "\n",
    "    return alignment\n",
    "\n",
    "\n",
    "###########\n",
    "# Example #\n",
    "###########\n",
    "alignment = {\n",
    "    'sp1': \"TG#CACCGTCGCCAACAGTAGTCCTCCACCGTCGCCACCGTCNCCAACAG?G\",\n",
    "    'sp2': \"TG#CACCGTCGCCAACAGTAGTCCTCCACCGTCGCCNNNNNNTCCAACA??G\",\n",
    "    'sp3': \"TG#-ACCGTCGCCAACAGTAGTCCTCCACCGTCGCCACCGT----AA----G\",\n",
    "    'sp4': \"--#C-CCGTCGCCNNNNNNNNNNNNNNNNNNTCGCNACCGTCGCCAACATTG\",\n",
    "    'sp5': \"TA#CA-CGTCGCCAACAGTAGTCCTCCACCGTCGCCACCGTCGCCAACAGTG\",\n",
    "    'sp6': \"--#CAC-GTCGCCAACAGT---------CCGTCGCCACCGTCGCCA----TG\",\n",
    "} \n",
    "\n",
    "# Convert to SeqRecord list\n",
    "records = [SeqRecord(Seq(seq), id=name, description=\"\") for name, seq in alignment.items()]\n",
    "# Write to FASTA\n",
    "with open(\"debug_data/test_n2question.fasta\", \"w\") as output_handle:\n",
    "    SeqIO.write(records, output_handle, \"fasta\")\n",
    "\n",
    "prepDyn(input_file= \"debug_data/test_n2question.fasta\",\n",
    "        output_file=\"debug_data/test_n2question\",\n",
    "        orphan_method=\"semi\", orphan_threshold=3,\n",
    "        internal_method=\"semi\", internal_threshold=8,\n",
    "        del_inv=True,\n",
    "        partitioning_round=2,\n",
    "        log=True, \n",
    "        n2question='all')\n",
    "\n",
    "# Command in bash to debug\n",
    "# python src/prepDyn.py --input_file jupyter/debug_data/test_n2question.fasta --output_file jupyter/debug_data/test_n2question --orphan_method semi --orphan_threshold 2 --del_inv --partitioning_round 2 --log --n2question all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "021aa84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/labanfibios/anaconda3/lib/python3.10/site-packages/Bio/Entrez/__init__.py:570: UserWarning: \n",
      "Email address is not specified.\n",
      "\n",
      "To make use of NCBI's E-utilities, NCBI requires you to specify your\n",
      "email address with each request.  As an example, if your email address\n",
      "is A.N.Other@example.com, you can specify it as follows:\n",
      "   from Bio import Entrez\n",
      "   Entrez.email = 'A.N.Other@example.com'\n",
      "In case of excessive usage of the E-utilities, NCBI will attempt to contact\n",
      "a user at the email address provided before blocking access to the\n",
      "E-utilities.\n",
      "  warnings.warn(\"\"\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[134], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Debug tests: GB2MSA\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mGB2MSA\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdebug_data/v5_GenBank_prepDyn.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m       \u001b[49m\u001b[43moutput_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdebug_data/v5_TEST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m       \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mwrite_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mlog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[123], line 336\u001b[0m, in \u001b[0;36mGB2MSA\u001b[0;34m(input_file, output_prefix, delimiter, write_names, log, orphan_threshold)\u001b[0m\n\u001b[1;32m    333\u001b[0m start_cpu \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mprocess_time()\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# Step 1: Generate aligned FASTA files\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m aligned_files \u001b[38;5;241m=\u001b[39m \u001b[43mGB2MSA_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# Step 2: Clean each aligned FASTA file\u001b[39;00m\n\u001b[1;32m    339\u001b[0m cleaned_files \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[123], line 90\u001b[0m, in \u001b[0;36mGB2MSA_1\u001b[0;34m(input_file, output_prefix, delimiter, write_names)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m acc \u001b[38;5;129;01min\u001b[39;00m accessions:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[43mEntrez\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mefetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnucleotide\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrettype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfasta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m         seq_record \u001b[38;5;241m=\u001b[39m SeqIO\u001b[38;5;241m.\u001b[39mread(handle, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfasta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m         handle\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/Bio/Entrez/__init__.py:184\u001b[0m, in \u001b[0;36mefetch\u001b[0;34m(db, **keywords)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ids\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;66;03m# NCBI prefers an HTTP POST instead of an HTTP GET if there are\u001b[39;00m\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;66;03m# more than about 200 IDs\u001b[39;00m\n\u001b[1;32m    183\u001b[0m         post \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcgi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/Bio/Entrez/__init__.py:543\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(cgi, params, post, ecitmatch)\u001b[0m\n\u001b[1;32m    541\u001b[0m         handle \u001b[38;5;241m=\u001b[39m _urlopen(cgi, data\u001b[38;5;241m=\u001b[39m_as_bytes(options))\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 543\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[43m_urlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcgi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _HTTPError \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/urllib/request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[1;32m    518\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[0;32m--> 519\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[1;32m    522\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/urllib/request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    535\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m--> 536\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m    537\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/urllib/request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[0;32m-> 1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/urllib/request.py:1352\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[0;32m-> 1352\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1354\u001b[0m     h\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/http/client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1373\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Debug tests: GB2MSA\n",
    "GB2MSA(input_file = \"debug_data/v5_GenBank_prepDyn.csv\", \n",
    "       output_prefix=\"debug_data/v5_TEST\",\n",
    "       delimiter=',',write_names=True,log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "880467d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug tests: addSeq\n",
    "\n",
    "alignment = {\n",
    "    'sp1': \"TGCACCGTCGCC#AACAGTAGTCCTCCACCG#AAGTTACCGTCNCCAACAG??\",\n",
    "    'sp2': \"T-CACCGTCGCC#AACAGTAGTCCTCCACCG#AAGTTNNNNNNTCCAACA???\",\n",
    "    'sp3': \"TG-ACCGTCGCC#AACAGTAGTCCTCCACCG#AAGTTACCGTCGCCAA-----\",\n",
    "    'sp4': \"TGC-CCGTCGCC#NNNNNNNNNNNNNNNNNN#AAGTTACCGTCGCCAACATTT\",\n",
    "    'sp5': \"TGCA-CGTCGCC#AACAGTAGTCCTCCACCG#AAGTTACCGTCGCCAACAGTG\",\n",
    "    'sp6': \"TGCAC-GTCGCC#AACAGTAGTCCTCCACCG#AAGTTACCGTCGCCAACAGTA\",\n",
    "} \n",
    "new_seqs = {\n",
    "    'sp7': \"TGCACCGTCGCCAACAGTAGTCCTCCACCGTCG\",\n",
    "    'sp8': \"TGCACCGTCGCCAACAGTAGTCCTCCACCGAAGCAACAGTA\"\n",
    "}\n",
    "\n",
    "#addSeq(alignment, new_seqs, \n",
    "#       output=\"debug_data/addSeq_debug3_output.fas\", \n",
    "#       orphan_threshold=0,\n",
    "#       write_names=False)\n",
    "\n",
    "addSeq(alignment=\"debug_data/ex4.3_aln.fas\",\n",
    "       new_seqs=\"debug_data/ex4.3_new_seqs.fas\",\n",
    "       output=\"debug_data/ex4.3_out.fas\",\n",
    "       #orphan_threshold=0, \n",
    "       #n2question=\"Thoropa_miliaris_CFBH10125\",\n",
    "       #gaps2question=25,\n",
    "       #write_names=True,\n",
    "       log=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f7512613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed column indices: [0, 52]\n"
     ]
    }
   ],
   "source": [
    "alignment = {\n",
    "    'sp1': \"TGCACCGTCGCC#AACAGTAGTCCTCCACCG#AAGTTACCGTCNCCAACAG?A\",\n",
    "    'sp2': \"T-CACCGTCGCC#AACAGTAGTCCTCCACCG#AAGTTNNNNNNTCCAACA??A\",\n",
    "    'sp3': \"TGCACCGTCGCC#AACAGTAGTCCTCCACCG#AAGTTACCGTCGCCAA----A\",\n",
    "    'sp4': \"TGC-CCGTCGCC#NNNNNNNNNNNNNNNNNN#AAGTTACCGTCGCCAACATTA\",\n",
    "    'sp5': \"TGCA-CGTCGCC#AACAGTAGTCCTCCACCG#AAGTTACCGTCGCCAACAGTA\",\n",
    "    'sp6': \"TGCAC-GTCGCC#AACAGTAGTCCTCCACCG#AAGTTACCGTCGCCAACAGTA\",\n",
    "} \n",
    "\n",
    "def remove_non_informative_positions(alignment, removed_indices=None):\n",
    "    \"\"\"\n",
    "    Remove parsimony non-informative positions from both the start and the end of the alignment.\n",
    "    \n",
    "    Parameters:\n",
    "        alignment (dict): Dictionary where keys are sequence names and values are sequences (strings).\n",
    "        removed_indices (list, optional): If provided, the function will append the indices of removed columns.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Updated alignment with non-informative positions removed.\n",
    "    \"\"\"\n",
    "    # Convert the alignment to a list of sequences\n",
    "    sequences = list(alignment.values())\n",
    "    seq_length = len(sequences[0])\n",
    "\n",
    "    # Remove non-informative positions from the start\n",
    "    first_position = 0\n",
    "    while first_position < seq_length and is_parsimony_non_informative([seq[first_position] for seq in sequences]):\n",
    "        first_position += 1\n",
    "\n",
    "    # Remove non-informative positions from the end\n",
    "    last_position = seq_length - 1\n",
    "    while last_position >= first_position and is_parsimony_non_informative([seq[last_position] for seq in sequences]):\n",
    "        last_position -= 1\n",
    "\n",
    "    # If logging, record the indices of removed columns\n",
    "    if removed_indices is not None:\n",
    "        removed_start = list(range(0, first_position))\n",
    "        removed_end = list(range(last_position + 1, seq_length))\n",
    "        removed_indices.extend(removed_start + removed_end)\n",
    "\n",
    "    # Build the updated alignment\n",
    "    for seq_name, seq in alignment.items():\n",
    "        alignment[seq_name] = seq[first_position:last_position + 1]\n",
    "\n",
    "    return alignment\n",
    "\n",
    "removed_cols = []\n",
    "alignment = remove_non_informative_positions(alignment, removed_indices=removed_cols)\n",
    "print(\"Removed column indices:\", removed_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331357c5",
   "metadata": {},
   "source": [
    "## Time complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc271c21",
   "metadata": {},
   "source": [
    "To understand time complexity, the following combination of parameters were used to simulate 192 MSAs: \n",
    "\n",
    "- Number of leaves = from 10 to 50\n",
    "- Number of characters = from 500 to to 50000\n",
    "- Percentage of terminal missing data = from 0.05 to 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac13aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98543a87",
   "metadata": {},
   "source": [
    "## Empirical example 1: Sanger sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c0449f",
   "metadata": {},
   "source": [
    "For Sanger sequences, we used the dataset of Hylodidae from de SÃ¡ et al. (2022), which comprises 75 leaves with ~4,291 bp (12S, tRNA-Val, 16S, COI, RAG1-a, RAG1-b). We compared tree costs and topologies from three preprocessing strategies:\n",
    "\n",
    "1. No preprocessing\n",
    "\n",
    "2. Pound signs flanking all instances of terminal \"gap\" opening/closure (false gaps, actually missing data)\n",
    "\n",
    "3. *prepDyn*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6a10d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. No preprocessing\n",
    "GB2MSA(input_file = \"../test_data/prepDyn_Empirical1_Hylodidae/1_noPreprocessing/Sa2022.csv\", \n",
    "       output_prefix=\"../test_data/prepDyn_Empirical1_Hylodidae/1_noPreprocessing/output\",\n",
    "       delimiter=',',write_names=True,log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17f72c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. prepDyn with \"max\"\n",
    "prepDyn(GB_input = \"Data/prepDyn_Empirical1_Hylodidae/2_max/Sa2022.csv\", \n",
    "         output_file=\"Data/prepDyn_Empirical1_Hylodidae/2_max/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_round=\"max\",\n",
    "         log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcac3e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3a. prepDyn with no partitions\n",
    "prepDyn(GB_input = \"Data/prepDyn_Empirical1_Hylodidae/3_prepDyn0/Sa2022.csv\", \n",
    "         output_file=\"Data/prepDyn_Empirical1_Hylodidae/3_prepDyn0/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_round=0,\n",
    "         log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feb32a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3b. prepDyn inserting # in the largest block of contiguous invariant(s)\n",
    "prepDyn(GB_input=\"Data/prepDyn_Empirical1_Hylodidae/3_prepDyn1/Sa2022.csv\", \n",
    "         output_file=\"Data/prepDyn_Empirical1_Hylodidae/3_prepDyn1/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_round=1,\n",
    "         log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617eff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3c. prepDyn inserting # in the 2-largest block of contiguous invariant(s)\n",
    "prepDyn(GB_input=\"Data/prepDyn_Empirical1_Hylodidae/3_prepDyn2/Sa2022.csv\", \n",
    "         output_file=\"Data/prepDyn_Empirical1_Hylodidae/3_prepDyn2/output\",\n",
    "         orphan_method=\"semi\", orphan_threshold=6,\n",
    "         partitioning_round=2,\n",
    "         log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161f2c96",
   "metadata": {},
   "source": [
    "## Empirical example 2: Phylogenomic sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e65d13",
   "metadata": {},
   "source": [
    "Empirical example using 352 loci sequeced with AHE from XX species of the Old World treefrogs Rhacophoridae. Note that only files with the extension \".fasta\" in the specified directory will be considered as input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ea26a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an alignment (FASTA, Clustal, Phylip, etc.)\n",
    "prepDyn(input_file = \"/Users/labanfibios/Desktop/Doutorado/Project/B3_PreprocessingPHYG/Example2_Rhacophoridae/\", \n",
    "      input_format=\"fasta\",\n",
    "      orphan_method=\"semi\",\n",
    "      orphan_threshold=10, \n",
    "      internal_method=\"semi\",\n",
    "      internal_threshold=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944dde12",
   "metadata": {},
   "source": [
    "## Empirical example 3: Ancient DNA sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e32c565",
   "metadata": {},
   "source": [
    "## TRASH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7643a37",
   "metadata": {},
   "source": [
    "Other functions used in previous versions of this code (now discarded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3810619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0. Align sequences statically\n",
      "sp1: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp2: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[97mw\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp3: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp4: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp5: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp6: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n",
      "\n",
      "\n",
      "preprocessed\n",
      "sp1: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp2: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[35m?\u001b[0m\n",
      "sp3: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[35m?\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp4: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp5: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp6: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def replace_w_blocks_with_question(alignment_dict):\n",
    "    cleaned_alignment = {}\n",
    "\n",
    "    for seq_name, seq in alignment_dict.items():\n",
    "        sequence = list(seq)\n",
    "\n",
    "        # Use regex to find blocks with at least 15 characters of w/W/- combined\n",
    "        # but must contain at least 15 w or W\n",
    "        pattern = re.finditer(r'([wW\\-]{15,})', ''.join(sequence))\n",
    "\n",
    "        for match in pattern:\n",
    "            block = match.group()\n",
    "            start, end = match.start(), match.end()\n",
    "\n",
    "            # Count number of w/W in the block\n",
    "            w_count = sum(1 for c in block if c in 'wW')\n",
    "            if w_count >= 15:\n",
    "                for i in range(start, end):\n",
    "                    sequence[i] = '?'\n",
    "\n",
    "        cleaned_alignment[seq_name] = ''.join(sequence)\n",
    "\n",
    "    return cleaned_alignment\n",
    "\n",
    "\n",
    "# Input data\n",
    "alignment = {\n",
    "    'sp1': \"ttcatcgtcgacgtaatagt-ctccaccg-cgccac??????-------a\",\n",
    "    'sp2': \"ttcwwwwww------wwwwww--wwwccg-cgccaccgtcgc-------?\",\n",
    "    'sp3': \"ttcatcgtcgacgtaatagt-ctccaccg-cgccaccgt???-------a\",\n",
    "    'sp4': \"ttcatcgtcgacgtaatagt-ctccaccg-cgccaccgtcgc-------a\",\n",
    "    'sp5': \"ttcatcgtcgacgtaatagt-ctccaccg-cgccaccgtcgc-------a\",\n",
    "    'sp6': \"ttcatcgtcgacgtaatagt-ctccaccg-cgccaccgtcgc-------a\"\n",
    "} \n",
    "\n",
    "print(\"Step 0. Align sequences statically\")\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "print('\\n')\n",
    "print(\"preprocessed\")\n",
    "print_colored_alignment(replace_w_blocks_with_question(alignment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ab3c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Step 1.2. Delete terminal orphan nucleotides (given a threshold)\n",
      "sp1: \u001b[32mg\u001b[0m\u001b[30m#\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[30m#\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\n",
      "sp2: \u001b[97m-\u001b[0m\u001b[30m#\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[30m#\u001b[0m\u001b[34mt\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[32mg\u001b[0m\u001b[30m#\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97mn\u001b[0m\u001b[97m-\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\n",
      "sp3: \u001b[33mc\u001b[0m\u001b[30m#\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[30m#\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp4: \u001b[97m-\u001b[0m\u001b[30m#\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[30m#\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\n",
      "sp5: \u001b[33mc\u001b[0m\u001b[30m#\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[30m#\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp6: \u001b[97m-\u001b[0m\u001b[30m#\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[30m#\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\n",
      "\n",
      "\n",
      "Step 2.2 Insert pound signs in all cases of TERMINAL gap opening/closure\n",
      "sp1: \u001b[32mg\u001b[0m\u001b[30m#\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[30m#\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\n",
      "sp2: \u001b[97m-\u001b[0m\u001b[30m#\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[30m#\u001b[0m\u001b[34mt\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[32mg\u001b[0m\u001b[30m#\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97mn\u001b[0m\u001b[97m-\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\n",
      "sp3: \u001b[33mc\u001b[0m\u001b[30m#\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[30m#\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp4: \u001b[97m-\u001b[0m\u001b[30m#\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[30m#\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\n",
      "sp5: \u001b[33mc\u001b[0m\u001b[30m#\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[30m#\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp6: \u001b[97m-\u001b[0m\u001b[30m#\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[30m#\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\n",
      "\n",
      "\n",
      "Step 2.3 Insert pound signs if INTERNAL block length > X (e.g. X = 1)\n",
      "sp1: \u001b[32mg\u001b[0m\u001b[30m#\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[30m#\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[30m#\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\n",
      "sp2: \u001b[97m-\u001b[0m\u001b[30m#\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[30m#\u001b[0m\u001b[34mt\u001b[0m\u001b[32mg\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[32mg\u001b[0m\u001b[30m#\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[30m#\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97mn\u001b[0m\u001b[97m-\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\n",
      "sp3: \u001b[33mc\u001b[0m\u001b[30m#\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[30m#\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[30m#\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp4: \u001b[97m-\u001b[0m\u001b[30m#\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[30m#\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[30m#\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\n",
      "sp5: \u001b[33mc\u001b[0m\u001b[30m#\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[30m#\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[30m#\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp6: \u001b[97m-\u001b[0m\u001b[30m#\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[30m#\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[30m#\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[30m#\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "# Step 1.2: Delete orphan nucleotides in the first and last positions #\n",
    "#######################################################################\n",
    "\n",
    "def delete_orphan_nucleotides(alignment, orphan_threshold=10):\n",
    "    \"\"\"\n",
    "    Orphan nucleotides are artifacts from static alignment, in which one or a few nucleotides\n",
    "    are accidentally not aligned in the beginning or end of the sequences. Manual deletion or\n",
    "    local alignment should be performed, but here I provide a heuristic, optional function to\n",
    "    tentatively identify and delete orphan nucleotides in terminal positions.\n",
    "    \n",
    "    For each sequence in the alignment, replaces orphan nucleotides at the \n",
    "    beginning or end of the sequence with a hyphen if they are followed or preceded \n",
    "    by a number of gaps equal to or greater than the orphan_threshold, but only if \n",
    "    the number of orphan nucleotides are fewer than or equal to the orphan_threshold in length.\n",
    "    \n",
    "    Parameters:\n",
    "        alignment (dict): Dictionary with sequence names as keys and sequences as values.\n",
    "        orphan_threshold (int): The number of consecutive gaps after the orphan nucleotide\n",
    "                                 required for it to be considered an orphan.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Modified alignment with orphan nucleotides replaced by '-'.\n",
    "    \"\"\"\n",
    "    \n",
    "    for seq_name, seq in alignment.items():\n",
    "        # Check the first position for orphan nucleotides (beginning of sequence)\n",
    "        if seq[0] != \"-\":\n",
    "            i = 0\n",
    "            # Find contiguous orphan nucleotides at the beginning of the sequence\n",
    "            while i < len(seq) and seq[i] != \"-\" and seq[i] != \"#\":\n",
    "                i += 1\n",
    "            \n",
    "            # If the contiguous orphan nucleotides are followed by enough gaps\n",
    "            if i > 0 and seq[i:i+orphan_threshold] == \"-\" * orphan_threshold:\n",
    "                # Only replace if the length of contiguous orphan nucleotides is <= orphan_threshold\n",
    "                if i <= orphan_threshold:\n",
    "                    seq = \"-\" * i + seq[i:]\n",
    "        \n",
    "        # Check the last position for orphan nucleotides (end of sequence)\n",
    "        if seq[-1] != \"-\":\n",
    "            i = len(seq) - 1\n",
    "            # Find contiguous orphan nucleotides at the end of the sequence\n",
    "            while i >= 0 and seq[i] != \"-\" and seq[i] != \"#\":\n",
    "                i -= 1\n",
    "            \n",
    "            # If the contiguous orphan nucleotides are preceded by enough gaps\n",
    "            if i < len(seq) - 1 and seq[i - orphan_threshold:i] == \"-\" * orphan_threshold:\n",
    "                # Only replace if the length of contiguous orphan nucleotides is <= orphan_threshold\n",
    "                if len(seq) - i - 1 <= orphan_threshold:\n",
    "                    seq = seq[:i+1] + \"-\" * (len(seq) - i - 1)\n",
    "        \n",
    "        # Update the sequence in the alignment dictionary\n",
    "        alignment[seq_name] = seq\n",
    "    \n",
    "    return alignment\n",
    "\n",
    "print('\\n')\n",
    "print(\"Step 1.2. Delete terminal orphan nucleotides (given a threshold)\")\n",
    "print_colored_alignment(delete_orphan_nucleotides(alignment, orphan_threshold=6))\n",
    "        \n",
    "###############################################################################\n",
    "# Step 2.3: Insert breaks in internal gap opening/closure if block length > X #\n",
    "###############################################################################\n",
    "\n",
    "def add_breaks_internal(alignment, X=1, Y=None):\n",
    "    \"\"\"\n",
    "    Add '#' flanking internal gap blocks if the gap block size exceeds X.\n",
    "    \n",
    "    Parameters:\n",
    "        alignment (dict): Dictionary where keys are sequence names and values are sequences.\n",
    "        X (int): Threshold for the minimum gap size to trigger '#' insertion (default: X = 1).\n",
    "        Y (list of ranges): Optional list of ranges (start, end) defining blocks of gaps that must be flanked by '#'. Indexing of columns in the matrix starts with zero (not one). In internal gap blocks, Y should be specified if the minimum size of known missing data blocks (e.g. incomplete sequences due to primers) is not larger than the maximum size of gap blocks.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated alignment with '#' flanking internal gap blocks.\n",
    "    \"\"\"\n",
    "    # Determine the length of the sequences\n",
    "    seq_length = len(next(iter(alignment.values())))\n",
    "\n",
    "    # Initialize a list to keep track of positions that need '#' in all sequences\n",
    "    hash_positions = [False] * seq_length\n",
    "\n",
    "    # Iterate through each sequence to find internal gap block\n",
    "    for seq in alignment.values():\n",
    "        i = 0  # Start from position 0\n",
    "        while i < seq_length:\n",
    "            if seq[i] == '-':\n",
    "                # Find the start of a gap block\n",
    "                start = i\n",
    "                while i < seq_length and seq[i] == '-':\n",
    "                    i += 1\n",
    "                end = i\n",
    "                # Mark the position for this gap opening/closure ONLY if the gap block is larger than X\n",
    "                if (end - start) > X:\n",
    "                    hash_positions[start] = True  # Mark the start of gap block\n",
    "                    if end < seq_length:\n",
    "                        hash_positions[end] = True  # Mark the end of gap block\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    # Add '#' around the ranges specified in Y\n",
    "    if Y:\n",
    "        for range_start, range_end in Y:\n",
    "            if range_start >= 0 and range_end < seq_length:\n",
    "                hash_positions[range_start] = True\n",
    "                if range_end + 1 < seq_length:\n",
    "                    hash_positions[range_end + 1] = True\n",
    "                    \n",
    "    # Add '#' at the identified positions in each sequence\n",
    "    for key in alignment:\n",
    "        new_seq = []\n",
    "        for i in range(seq_length):\n",
    "            if i > 0 and hash_positions[i]:  # Avoid position 1 and single-gap blocks\n",
    "                new_seq.append('#')\n",
    "            new_seq.append(alignment[key][i])\n",
    "        if hash_positions[-1]:\n",
    "            new_seq.append('#')  # Ensure we append '#' at the end if necessary\n",
    "        \n",
    "        # After modifying the sequence, remove consecutive '#' characters\n",
    "        modified_seq = ''.join(new_seq)\n",
    "        modified_seq = re.sub(r'#+', '#', modified_seq)  # Replace multiple '#' with a single '#'\n",
    "        \n",
    "        alignment[key] = modified_seq\n",
    "    \n",
    "    return alignment  # Return the modified alignment\n",
    "\n",
    "\n",
    "# Print\n",
    "add_breaks_internal(alignment, X=2) # insert '#' flanking internal gap blocks larger than 2\n",
    "#add_breaks_internal(alignment, X=2, Y=[(37, 42)]) # insert '#' in columns 37 and 42\n",
    "print(\"\\n\") # Jump a line\n",
    "print(\"Step 2.3 Insert pound signs if INTERNAL block length > X (e.g. X = 1)\")\n",
    "print_colored_alignment(alignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57ea93e",
   "metadata": {},
   "source": [
    "Some functions to use the trimal method. However, even when we use the \"terminal_only\" option, internal columns are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9975b90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0. Align sequences statically\n",
      "sp1: \u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp2: \u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[32mg\u001b[0m\u001b[97mn\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp3: \u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[34mt\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "sp4: \u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp5: \u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[31ma\u001b[0m\n",
      "sp6: \u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[31ma\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[33mc\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[97m-\u001b[0m\u001b[34mt\u001b[0m\u001b[33mc\u001b[0m\u001b[32mg\u001b[0m\u001b[33mc\u001b[0m\u001b[31ma\u001b[0m\u001b[31ma\u001b[0m\u001b[97m-\u001b[0m\u001b[33mc\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\u001b[97m-\u001b[0m\n",
      "\n",
      "\n",
      "Step 1.2. Trim problematic positions using trimai\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Alignment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m alignment \u001b[38;5;241m=\u001b[39m dict_to_multiple_seq_alignment(alignment)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Convert Biopython to pytrimal objects\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m alignment \u001b[38;5;241m=\u001b[39m \u001b[43mAlignment\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_biopython(alignment)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Trim using trimal\u001b[39;00m\n\u001b[1;32m     26\u001b[0m trimmer \u001b[38;5;241m=\u001b[39m AutomaticTrimmer(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgappyout\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'Alignment' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Input data\n",
    "alignment = {\n",
    "    'sp1': \"tt-------------agtagt-ctccaccg-cgccaccgtcgc-------a\",\n",
    "    'sp2': \"-ccaccgtc------gngtag-ctccaccg-cgc-------gccaacagta\",\n",
    "    'sp3': \"-ccaccgtcgccaacagtagt------ccg--------tt-----------\",\n",
    "    'sp4': \"------ct-----------gt-ctccaccg-cgccacc-tcgcca-aagta\",\n",
    "    'sp5': \"--caccgtcgccaacagtagt-ctccaccg-cgccaccgtcgcca-cag-a\",\n",
    "    'sp6': \"--caccgtc-ccaacagtagt-ctccaccgtcgcaccg-tcgcaa-c----\"\n",
    "} \n",
    "\n",
    "print(\"Step 0. Align sequences statically\")\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "#print('\\n')\n",
    "#print(\"Step 1.1 Delete columns presenting gaps in all taxa\")\n",
    "#print_colored_alignment(remove_all_gap_columns(alignment))\n",
    "\n",
    "print('\\n')\n",
    "print(\"Step 1.2. Trim problematic positions using trimai\")\n",
    "\n",
    "# Convert dictionary to Biopython objects\n",
    "alignment = dict_to_multiple_seq_alignment(alignment)\n",
    "# Convert Biopython to pytrimal objects\n",
    "alignment = Alignment.from_biopython(alignment)\n",
    "# Trim using trimal\n",
    "trimmer = AutomaticTrimmer(method=\"gappyout\") \n",
    "#trimmer = AutomaticTrimmer(method=\"automated1\")\n",
    "alignment = trimmer.trim(alignment)\n",
    "alignment = pytrimal.TrimmedAlignment.terminal_only(alignment)\n",
    "\n",
    "\n",
    "# Convert the trimal to Biopython\n",
    "alignment = Alignment.to_biopython(alignment)\n",
    "# Convert MultipleSeqAlignment to dictionary\n",
    "alignment = {record.id: str(record.seq) for record in alignment}\n",
    "print_colored_alignment(alignment)\n",
    "\n",
    "# Print\n",
    "#show_alignment(alignment)\n",
    "\n",
    "# Write in fasta format\n",
    "#Alignment.dump(alignment, file=\"Example_trimai.fasta\", format=\"fasta\")\n",
    "\n",
    "def calculate_statistics(values):\n",
    "    mean = sum(values) / len(values)  # Mean = Sum of values / Number of values\n",
    "    min_value = min(values)           # Minimum value\n",
    "    max_value = max(values)           # Maximum value\n",
    "    \n",
    "    return mean, min_value, max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb5c2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OLDaddSeq(\n",
    "    alignment,\n",
    "    new_seqs,\n",
    "    output,\n",
    "    write_names=True,\n",
    "    orphan_threshold=10,\n",
    "    log=False,\n",
    "    n2question=None,\n",
    "    gaps2question=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Add new sequences to an existing alignment using MAFFT, clean and standardize the result.\n",
    "\n",
    "    Steps performed:\n",
    "    1. Remove '#' columns from original alignment.\n",
    "    2. Align new sequences with MAFFT using --add.\n",
    "    3. Trim orphan nucleotide blocks from new sequences.\n",
    "    4. Remove short DNA blocks near # in first and last partitions.\n",
    "    5. Replace terminal '-' with '?'.\n",
    "    6. Optionally replace all N/n with '?' in selected sequences.\n",
    "    7. Optionally replace long gap blocks with '?'.\n",
    "    8. Reinsert '#' columns.\n",
    "    9. Replace all-'?' blocks between '#' with '-'.\n",
    "    10. Write the result and an optional log file.\n",
    "\n",
    "    Parameters:\n",
    "        alignment (str or dict): Existing alignment file path (FASTA) or dictionary {id: sequence}.\n",
    "        new_seqs (str or dict): New sequences to add (FASTA path or dict {id: sequence}).\n",
    "        output (str): Path to write the updated alignment in FASTA format.\n",
    "        write_names (bool): Whether to write a _terminal_names.txt file listing sequence IDs.\n",
    "        orphan_threshold (int): Threshold to detect and remove orphan DNA blocks.\n",
    "        log (bool): If True, writes a log file with trimming and runtime information.\n",
    "        n2question (str, list or None): Replace 'N/n' with '?' in specific sequences:\n",
    "            - 'all': apply to all sequences\n",
    "            - str: apply to a single sequence ID\n",
    "            - list: apply to listed sequence IDs\n",
    "        gaps2question (int or None): Replace contiguous gap blocks larger than this threshold with '?'. Only applied to added sequences.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Example usage:\n",
    "        addSeq(\"alignment.fasta\", \"new.fasta\", \"updated.fasta\", n2question=\"seq123\", log=True)\n",
    "        addSeq(alignment_dict, new_dict, \"out.fas\", n2question='all')\n",
    "    \"\"\"\n",
    "\n",
    "    # Start timing the execution\n",
    "    start_time = time.time()\n",
    "    temp_files_to_remove = []  # Temporary files to be removed after execution\n",
    "    log_lines = [] \n",
    "\n",
    "    # Log the function call and parameters for reproducibility\n",
    "    if log:\n",
    "        cmd_used = f\"addSeq(alignment=..., new_seqs=..., output='{output}', write_names={write_names}, orphan_threshold={orphan_threshold}, log={log}, n2question={n2question}, gaps2question={gaps2question})\"\n",
    "        log_lines.append(f\"Command used: {cmd_used}\")\n",
    "        log_lines.append(\"\")\n",
    "\n",
    "    # === Step 1: Load and clean the alignment ===\n",
    "    def write_dict_to_temp_fasta(seq_dict):\n",
    "        records = [SeqRecord(Seq(seq), id=str(seq_id), description=\"\") for seq_id, seq in seq_dict.items()]\n",
    "        tmp = tempfile.NamedTemporaryFile(\"w+\", delete=False)\n",
    "        SeqIO.write(records, tmp, \"fasta\")\n",
    "        tmp.close()\n",
    "        return tmp.name\n",
    "\n",
    "    if isinstance(alignment, dict):\n",
    "        alignment_path = write_dict_to_temp_fasta(alignment)\n",
    "        temp_files_to_remove.append(alignment_path)\n",
    "    elif isinstance(alignment, str):\n",
    "        alignment_path = alignment\n",
    "    else:\n",
    "        raise ValueError(\"alignment must be a FASTA file path or a dictionary\")\n",
    "\n",
    "    records = list(SeqIO.parse(alignment_path, \"fasta\"))\n",
    "    if not records:\n",
    "        raise ValueError(\"Input alignment is empty or not found\")\n",
    "\n",
    "    aln_len = len(records[0].seq)\n",
    "    # Identify columns that are '#' characters to temporarily remove them for alignment\n",
    "    pound_cols = [i for i in range(aln_len) if any(rec.seq[i] == '#' for rec in records)]\n",
    "\n",
    "    # Log input alignment info\n",
    "    if log:\n",
    "        log_lines.append(f\"Input alignment: {len(records)} sequences\")\n",
    "        log_lines.append(f\"Input alignment: {len(pound_cols)} # columns\")\n",
    "\n",
    "    def remove_cols(seq, cols):\n",
    "        return ''.join(seq[i] for i in range(len(seq)) if i not in cols)\n",
    "\n",
    "    # Remove '#' columns\n",
    "    aln_no_pound = [\n",
    "        SeqRecord(Seq(remove_cols(str(rec.seq), pound_cols)), id=rec.id, description=\"\")\n",
    "        for rec in records\n",
    "    ]\n",
    "    # Write cleaned alignment to a temporary file\n",
    "    with tempfile.NamedTemporaryFile(\"w+\", delete=False) as aln_tmp:\n",
    "        SeqIO.write(aln_no_pound, aln_tmp, \"fasta\")\n",
    "        aln_path = aln_tmp.name\n",
    "        temp_files_to_remove.append(aln_path)\n",
    "\n",
    "    # === Step 2: Load new sequences ===\n",
    "    if isinstance(new_seqs, dict):\n",
    "        new_seqs_path = write_dict_to_temp_fasta(new_seqs)\n",
    "        new_seq_count = len(new_seqs)\n",
    "        temp_files_to_remove.append(new_seqs_path)\n",
    "    elif isinstance(new_seqs, str):\n",
    "        new_seq_count = sum(1 for _ in SeqIO.parse(new_seqs, \"fasta\"))\n",
    "        new_seqs_path = new_seqs\n",
    "    else:\n",
    "        raise ValueError(\"new_seqs must be a FASTA file path or a dictionary\")\n",
    "    # Log new sequence info\n",
    "    if log:\n",
    "        log_lines.append(f\"Input new sequences: {new_seq_count} sequences\")\n",
    "\n",
    "    # === Step 3: Align new sequences with MAFFT ===\n",
    "    with tempfile.NamedTemporaryFile(\"w+\", delete=False) as out_tmp:\n",
    "        out_path = out_tmp.name\n",
    "        temp_files_to_remove.append(out_path)\n",
    "\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            ['mafft', '--add', new_seqs_path, '--keeplength', '--preservecase', aln_path],\n",
    "            check=True,\n",
    "            stdout=open(out_path, 'w'),\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "        )\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        for file in temp_files_to_remove:\n",
    "            try:\n",
    "                os.remove(file)\n",
    "            except Exception:\n",
    "                pass\n",
    "        raise RuntimeError(f\"MAFFT failed!\\nCommand: {e.cmd}\\nExit status: {e.returncode}\\nMAFFT error output:\\n{e.stderr}\")\n",
    "\n",
    "    mafft_aligned_records = list(SeqIO.parse(out_path, \"fasta\"))\n",
    "    original_ids = {rec.id for rec in records}\n",
    "    new_records = [rec for rec in mafft_aligned_records if rec.id not in original_ids]\n",
    "\n",
    "\n",
    "    def replace_gap_blocks(seq, threshold, seq_id=None):\n",
    "        seq_list = list(seq)\n",
    "        replaced_log = []\n",
    "        i = 0\n",
    "        while i < len(seq_list):\n",
    "            if seq_list[i] == '-':\n",
    "                start = i\n",
    "                while i < len(seq_list) and seq_list[i] == '-':\n",
    "                    i += 1\n",
    "                if (i - start) > threshold:\n",
    "                    for j in range(start, i):\n",
    "                        seq_list[j] = '?'\n",
    "                    if seq_id:\n",
    "                        replaced_log.append(f\"{seq_id}: {i - start} contiguous '-' replaced with '?' at {start}â€“{i}\")\n",
    "            else:\n",
    "                i += 1\n",
    "        return ''.join(seq_list), replaced_log\n",
    "\n",
    "    def find_dna_blocks(seq_list, start, end):\n",
    "        blocks = []\n",
    "        i = start\n",
    "        while i < end:\n",
    "            if seq_list[i] not in \"-?#\":\n",
    "                s = i\n",
    "                while i < end and seq_list[i] not in \"-?#\":\n",
    "                    i += 1\n",
    "                e = i\n",
    "                blocks.append((s, e))\n",
    "            else:\n",
    "                i += 1\n",
    "        return blocks\n",
    "\n",
    "    def trim_orphan_blocks(seq, threshold, seq_id=None):\n",
    "        seq_list = list(seq)\n",
    "        trimmed_log = []\n",
    "\n",
    "        def find_blocks(seq_list):\n",
    "            blocks = []\n",
    "            i = 0\n",
    "            while i < len(seq_list):\n",
    "                if seq_list[i] not in \"-?#\":\n",
    "                    start = i\n",
    "                    while i < len(seq_list) and seq_list[i] not in \"-?#\":\n",
    "                        i += 1\n",
    "                    end = i\n",
    "                    blocks.append((start, end))\n",
    "                else:\n",
    "                    i += 1\n",
    "            return blocks\n",
    "\n",
    "        changed = True\n",
    "        while changed:\n",
    "            changed = False\n",
    "\n",
    "            while True:\n",
    "                blocks = find_blocks(seq_list)\n",
    "                if len(blocks) < 2:\n",
    "                    break\n",
    "                first_start, first_end = blocks[0]\n",
    "                next_start = blocks[1][0]\n",
    "                gap_count = seq_list[first_end:next_start].count('-') + seq_list[first_end:next_start].count('?')\n",
    "                size = first_end - first_start\n",
    "                if size < threshold and gap_count > threshold:\n",
    "                    deleted = ''.join(seq_list[first_start:first_end])\n",
    "                    seq_list[first_start:first_end] = ['-'] * size\n",
    "                    if seq_id:\n",
    "                        trimmed_log.append(f\"{seq_id}: Left {first_start}â€“{first_end} (size={size}, '{deleted}')\")\n",
    "                    changed = True\n",
    "                    continue\n",
    "                break\n",
    "\n",
    "            while True:\n",
    "                blocks = find_blocks(seq_list)\n",
    "                if len(blocks) < 2:\n",
    "                    break\n",
    "                last_start, last_end = blocks[-1]\n",
    "                prev_end = blocks[-2][1]\n",
    "                gap_count = seq_list[prev_end:last_start].count('-') + seq_list[prev_end:last_start].count('?')\n",
    "                size = last_end - last_start\n",
    "                if size < threshold and gap_count > threshold:\n",
    "                    deleted = ''.join(seq_list[last_start:last_end])\n",
    "                    seq_list[last_start:last_end] = ['-'] * size\n",
    "                    if seq_id:\n",
    "                        trimmed_log.append(f\"{seq_id}: Right {last_start}â€“{last_end} (size={size}, '{deleted}')\")\n",
    "                    changed = True\n",
    "                    continue\n",
    "                break\n",
    "\n",
    "        if pound_cols:\n",
    "            first_hash = min(pound_cols)\n",
    "            last_hash = max(pound_cols)\n",
    "            blocks = find_dna_blocks(seq_list, 0, first_hash)\n",
    "            if len(blocks) == 1:\n",
    "                s, e = blocks[0]\n",
    "                if e == first_hash and (e - s) < threshold:\n",
    "                    deleted = ''.join(seq_list[s:e])\n",
    "                    seq_list[s:e] = ['-'] * (e - s)\n",
    "                    if seq_id:\n",
    "                        trimmed_log.append(f\"{seq_id}: Left {s}â€“{e} (size={e - s}, '{deleted}')\")\n",
    "            blocks = find_dna_blocks(seq_list, last_hash + 1, len(seq_list))\n",
    "            if len(blocks) == 1:\n",
    "                s, e = blocks[0]\n",
    "                if s == last_hash + 1 and (e - s) < threshold:\n",
    "                    deleted = ''.join(seq_list[s:e])\n",
    "                    seq_list[s:e] = ['-'] * (e - s)\n",
    "                    if seq_id:\n",
    "                        trimmed_log.append(f\"{seq_id}: Right {s}â€“{e} (size={e - s}, '{deleted}')\")\n",
    "\n",
    "        return ''.join(seq_list), trimmed_log\n",
    "\n",
    "    trimmed_new_records = []\n",
    "    all_trim_logs = []\n",
    "    gaps2q_log = []\n",
    "    for rec in new_records:\n",
    "        trimmed_seq, seq_log = trim_orphan_blocks(str(rec.seq), orphan_threshold, seq_id=rec.id)\n",
    "        if gaps2question:\n",
    "            trimmed_seq, gap_log = replace_gap_blocks(trimmed_seq, gaps2question, seq_id=rec.id)\n",
    "            gaps2q_log.extend(gap_log)\n",
    "        trimmed_new_records.append(SeqRecord(Seq(trimmed_seq), id=rec.id, description=\"\"))\n",
    "        all_trim_logs.extend(seq_log)\n",
    "\n",
    "    processed_records = [rec for rec in mafft_aligned_records if rec.id in original_ids] + trimmed_new_records\n",
    "\n",
    "    updated_records = []\n",
    "    for rec in processed_records:\n",
    "        seq_chars = list(str(rec.seq))\n",
    "        for i in range(len(seq_chars)):\n",
    "            if seq_chars[i] == '-':\n",
    "                seq_chars[i] = '?'\n",
    "            else:\n",
    "                break\n",
    "        for i in range(len(seq_chars) - 1, -1, -1):\n",
    "            if seq_chars[i] == '-':\n",
    "                seq_chars[i] = '?'\n",
    "            else:\n",
    "                break\n",
    "        updated_records.append(SeqRecord(Seq(''.join(seq_chars)), id=rec.id, description=\"\"))\n",
    "\n",
    "    n2q_log = []\n",
    "    if n2question:\n",
    "        if isinstance(n2question, str) and n2question != 'all':\n",
    "            target_ids = {n2question}\n",
    "        elif isinstance(n2question, list):\n",
    "            target_ids = set(n2question)\n",
    "        elif n2question == 'all':\n",
    "            target_ids = {rec.id for rec in updated_records}\n",
    "        else:\n",
    "            target_ids = set()\n",
    "\n",
    "        for rec in updated_records:\n",
    "            if rec.id in target_ids:\n",
    "                seq_str = str(rec.seq)\n",
    "                count_n = seq_str.count('N') + seq_str.count('n')\n",
    "                if count_n > 0:\n",
    "                    rec.seq = Seq(seq_str.replace('N', '?').replace('n', '?'))\n",
    "                    n2q_log.append(f\"{rec.id}: {count_n} N/n replaced with ?\")\n",
    "\n",
    "    final_records = []\n",
    "    for rec in updated_records:\n",
    "        seq_list = list(str(rec.seq))\n",
    "        for col in sorted(pound_cols):\n",
    "            seq_list.insert(col, '#')\n",
    "        final_records.append(SeqRecord(Seq(''.join(seq_list)), id=rec.id, description=\"\"))\n",
    "\n",
    "    def process_blocks(seq):\n",
    "        seq_chars = list(seq)\n",
    "        blocks = []\n",
    "        start = 0\n",
    "        for i, c in enumerate(seq_chars):\n",
    "            if c == '#':\n",
    "                blocks.append((start, i))\n",
    "                start = i + 1\n",
    "        blocks.append((start, len(seq_chars)))\n",
    "        for (start, end) in blocks:\n",
    "            if all(seq_chars[i] == '?' for i in range(start, end)):\n",
    "                for i in range(start, end):\n",
    "                    seq_chars[i] = '-'\n",
    "        return ''.join(seq_chars)\n",
    "\n",
    "    final_output = [\n",
    "        SeqRecord(Seq(process_blocks(str(rec.seq))), id=rec.id, description=\"\")\n",
    "        for rec in final_records\n",
    "    ]\n",
    "\n",
    "    SeqIO.write(final_output, output, \"fasta\")\n",
    "    if write_names:\n",
    "        with open(output + \"_terminal_names.txt\", \"w\") as f:\n",
    "            for rec in final_output:\n",
    "                f.write(rec.id + \"\\n\")\n",
    "\n",
    "    if log:\n",
    "        elapsed = time.time() - start_time\n",
    "        log_lines.append(f\"Final alignment: {len(final_output)} sequences\")\n",
    "        log_lines.append(f\"Final alignment: {len(final_output[0].seq)} columns\")\n",
    "        log_lines.append(f\"Final alignment: {sum(1 for i in range(len(final_output[0].seq)) if any(rec.seq[i] == '#' for rec in final_output))} # columns\")\n",
    "        log_lines.append(\"\")\n",
    "        log_lines.append(\"Trimmed orphan blocks from new sequences:\")\n",
    "        log_lines.extend(all_trim_logs or [\"None\"])\n",
    "        if gaps2q_log:\n",
    "            log_lines.append(\"\")\n",
    "            log_lines.append(\"Gap block replacements:\")\n",
    "            log_lines.extend(gaps2q_log)\n",
    "        if n2q_log:\n",
    "            log_lines.append(\"\")\n",
    "            log_lines.append(\"N/n to ? replacements:\")\n",
    "            log_lines.extend(n2q_log)\n",
    "        log_lines.append(\"\")\n",
    "        log_lines.append(f\"Runtime: {elapsed:.2f} seconds\")\n",
    "        with open(output + \".log\", \"w\") as log_file:\n",
    "            log_file.write(\"\\n\".join(log_lines))\n",
    "\n",
    "    for file in temp_files_to_remove:\n",
    "        try:\n",
    "            os.remove(file)\n",
    "        except Exception:\n",
    "            pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
